<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url('https://themes.googleusercontent.com/fonts/css?kit=4mNYFHt_IKFsPe52toizH4jB0lxTqxuZ9MrNDVFkHCs');.lst-kix_ne2kodfi1kix-8>li:before{content:"\0025a0  "}.lst-kix_um8gcdc8t9m9-2>li:before{content:"-  "}.lst-kix_um8gcdc8t9m9-3>li:before{content:"-  "}.lst-kix_um8gcdc8t9m9-4>li:before{content:"-  "}.lst-kix_um8gcdc8t9m9-6>li:before{content:"-  "}ul.lst-kix_8d6owvwxykff-3{list-style-type:none}ul.lst-kix_8d6owvwxykff-2{list-style-type:none}.lst-kix_um8gcdc8t9m9-5>li:before{content:"-  "}.lst-kix_um8gcdc8t9m9-7>li:before{content:"-  "}ul.lst-kix_8d6owvwxykff-5{list-style-type:none}ul.lst-kix_8d6owvwxykff-4{list-style-type:none}ul.lst-kix_8d6owvwxykff-7{list-style-type:none}.lst-kix_dnm4ke3nhj1e-8>li:before{content:"\0025a0  "}ul.lst-kix_8d6owvwxykff-6{list-style-type:none}ul.lst-kix_8d6owvwxykff-8{list-style-type:none}.lst-kix_um8gcdc8t9m9-8>li:before{content:"-  "}ul.lst-kix_8d6owvwxykff-1{list-style-type:none}ul.lst-kix_8d6owvwxykff-0{list-style-type:none}ul.lst-kix_dnm4ke3nhj1e-5{list-style-type:none}ul.lst-kix_dnm4ke3nhj1e-6{list-style-type:none}ul.lst-kix_dnm4ke3nhj1e-7{list-style-type:none}ul.lst-kix_dnm4ke3nhj1e-8{list-style-type:none}ul.lst-kix_dnm4ke3nhj1e-0{list-style-type:none}ul.lst-kix_dnm4ke3nhj1e-1{list-style-type:none}ul.lst-kix_dnm4ke3nhj1e-2{list-style-type:none}ul.lst-kix_dnm4ke3nhj1e-3{list-style-type:none}ul.lst-kix_dnm4ke3nhj1e-4{list-style-type:none}ol.lst-kix_8dw3ycrgt362-3.start{counter-reset:lst-ctn-kix_8dw3ycrgt362-3 0}ul.lst-kix_p3wa2k83s8cn-1{list-style-type:none}ul.lst-kix_p3wa2k83s8cn-0{list-style-type:none}ul.lst-kix_p3wa2k83s8cn-3{list-style-type:none}.lst-kix_3z0hr7pzpec4-3>li:before{content:"\0025cf  "}.lst-kix_3z0hr7pzpec4-5>li:before{content:"\0025a0  "}ul.lst-kix_p3wa2k83s8cn-2{list-style-type:none}ul.lst-kix_p3wa2k83s8cn-5{list-style-type:none}ul.lst-kix_p3wa2k83s8cn-4{list-style-type:none}ul.lst-kix_p3wa2k83s8cn-7{list-style-type:none}ul.lst-kix_p3wa2k83s8cn-6{list-style-type:none}.lst-kix_ao97xhpq4hr9-7>li:before{content:"\0025cb  "}ul.lst-kix_p3wa2k83s8cn-8{list-style-type:none}.lst-kix_ao97xhpq4hr9-5>li:before{content:"\0025a0  "}ul.lst-kix_hm0ybxxbxp2l-5{list-style-type:none}ul.lst-kix_hm0ybxxbxp2l-6{list-style-type:none}ul.lst-kix_hm0ybxxbxp2l-7{list-style-type:none}ul.lst-kix_hm0ybxxbxp2l-8{list-style-type:none}ul.lst-kix_hm0ybxxbxp2l-1{list-style-type:none}ul.lst-kix_hm0ybxxbxp2l-2{list-style-type:none}ul.lst-kix_hm0ybxxbxp2l-3{list-style-type:none}ul.lst-kix_hm0ybxxbxp2l-4{list-style-type:none}.lst-kix_3z0hr7pzpec4-7>li:before{content:"\0025cb  "}.lst-kix_mfwegjdddisb-3>li:before{content:"\0025cf  "}.lst-kix_mfwegjdddisb-5>li:before{content:"\0025a0  "}.lst-kix_7n1btf6e2iyv-7>li:before{content:"\0025cb  "}.lst-kix_mfwegjdddisb-1>li:before{content:"\0025cb  "}.lst-kix_mfwegjdddisb-7>li:before{content:"\0025cb  "}ul.lst-kix_hm0ybxxbxp2l-0{list-style-type:none}.lst-kix_7n1btf6e2iyv-1>li:before{content:"\0025cb  "}.lst-kix_ao97xhpq4hr9-3>li:before{content:"\0025cf  "}.lst-kix_7n1btf6e2iyv-3>li:before{content:"\0025cf  "}ol.lst-kix_8dw3ycrgt362-8.start{counter-reset:lst-ctn-kix_8dw3ycrgt362-8 0}.lst-kix_ao97xhpq4hr9-1>li:before{content:"\0025cb  "}.lst-kix_7n1btf6e2iyv-5>li:before{content:"\0025a0  "}.lst-kix_7pcs9kbg0qhi-4>li{counter-increment:lst-ctn-kix_7pcs9kbg0qhi-4}.lst-kix_8dw3ycrgt362-0>li{counter-increment:lst-ctn-kix_8dw3ycrgt362-0}.lst-kix_dnm4ke3nhj1e-1>li:before{content:"\0025cb  "}.lst-kix_dnm4ke3nhj1e-5>li:before{content:"\0025a0  "}ol.lst-kix_7pcs9kbg0qhi-2.start{counter-reset:lst-ctn-kix_7pcs9kbg0qhi-2 0}.lst-kix_dnm4ke3nhj1e-7>li:before{content:"\0025cb  "}.lst-kix_7pcs9kbg0qhi-5>li{counter-increment:lst-ctn-kix_7pcs9kbg0qhi-5}.lst-kix_um8gcdc8t9m9-1>li:before{content:"-  "}.lst-kix_8dw3ycrgt362-7>li:before{content:"" counter(lst-ctn-kix_8dw3ycrgt362-7,lower-latin) ". "}.lst-kix_dnm4ke3nhj1e-3>li:before{content:"\0025cf  "}ul.lst-kix_3z0hr7pzpec4-5{list-style-type:none}ul.lst-kix_3z0hr7pzpec4-6{list-style-type:none}ul.lst-kix_3z0hr7pzpec4-7{list-style-type:none}.lst-kix_8dw3ycrgt362-1>li:before{content:"" counter(lst-ctn-kix_8dw3ycrgt362-1,lower-latin) ". "}.lst-kix_8dw3ycrgt362-3>li:before{content:"" counter(lst-ctn-kix_8dw3ycrgt362-3,decimal) ". "}ul.lst-kix_3z0hr7pzpec4-8{list-style-type:none}.lst-kix_5we8o0btcr67-7>li:before{content:"\0025cb  "}.lst-kix_8dw3ycrgt362-5>li:before{content:"" counter(lst-ctn-kix_8dw3ycrgt362-5,lower-roman) ". "}ul.lst-kix_3z0hr7pzpec4-0{list-style-type:none}ul.lst-kix_3z0hr7pzpec4-1{list-style-type:none}ul.lst-kix_3z0hr7pzpec4-2{list-style-type:none}ul.lst-kix_3z0hr7pzpec4-3{list-style-type:none}.lst-kix_ne2kodfi1kix-1>li:before{content:"\0025cb  "}ul.lst-kix_3z0hr7pzpec4-4{list-style-type:none}.lst-kix_5s83gilt2o7g-1>li:before{content:"\0025cb  "}.lst-kix_ne2kodfi1kix-3>li:before{content:"\0025cf  "}.lst-kix_ne2kodfi1kix-7>li:before{content:"\0025cb  "}.lst-kix_5s83gilt2o7g-3>li:before{content:"\0025cf  "}.lst-kix_5s83gilt2o7g-5>li:before{content:"\0025a0  "}.lst-kix_3z0hr7pzpec4-1>li:before{content:"\0025cb  "}.lst-kix_ne2kodfi1kix-5>li:before{content:"\0025a0  "}ul.lst-kix_5iyzxwlmxp1f-6{list-style-type:none}ul.lst-kix_5iyzxwlmxp1f-5{list-style-type:none}ul.lst-kix_5iyzxwlmxp1f-4{list-style-type:none}ul.lst-kix_5iyzxwlmxp1f-3{list-style-type:none}.lst-kix_5s83gilt2o7g-6>li:before{content:"\0025cf  "}ul.lst-kix_5iyzxwlmxp1f-2{list-style-type:none}ul.lst-kix_5iyzxwlmxp1f-1{list-style-type:none}ul.lst-kix_5iyzxwlmxp1f-0{list-style-type:none}ul.lst-kix_5iyzxwlmxp1f-8{list-style-type:none}ul.lst-kix_5iyzxwlmxp1f-7{list-style-type:none}ol.lst-kix_8dw3ycrgt362-7.start{counter-reset:lst-ctn-kix_8dw3ycrgt362-7 0}.lst-kix_5we8o0btcr67-3>li:before{content:"\0025cf  "}.lst-kix_hm0ybxxbxp2l-7>li:before{content:"\0025cb  "}.lst-kix_hm0ybxxbxp2l-6>li:before{content:"\0025cf  "}.lst-kix_8d6owvwxykff-0>li:before{content:"\0025cf  "}.lst-kix_5we8o0btcr67-2>li:before{content:"\0025a0  "}ul.lst-kix_msukp0gxz1c7-6{list-style-type:none}ol.lst-kix_7pcs9kbg0qhi-6.start{counter-reset:lst-ctn-kix_7pcs9kbg0qhi-6 0}ul.lst-kix_msukp0gxz1c7-5{list-style-type:none}ul.lst-kix_msukp0gxz1c7-8{list-style-type:none}.lst-kix_mvnr2hoa94kd-2>li:before{content:"\0025a0  "}ul.lst-kix_msukp0gxz1c7-7{list-style-type:none}ul.lst-kix_msukp0gxz1c7-2{list-style-type:none}.lst-kix_hm0ybxxbxp2l-2>li:before{content:"\0025a0  "}ul.lst-kix_msukp0gxz1c7-1{list-style-type:none}ul.lst-kix_msukp0gxz1c7-4{list-style-type:none}ul.lst-kix_msukp0gxz1c7-3{list-style-type:none}.lst-kix_9uzslqp6q29w-3>li:before{content:"\0025cf  "}ul.lst-kix_msukp0gxz1c7-0{list-style-type:none}.lst-kix_hm0ybxxbxp2l-3>li:before{content:"\0025cf  "}.lst-kix_9uzslqp6q29w-4>li:before{content:"\0025cb  "}.lst-kix_mvnr2hoa94kd-1>li:before{content:"\0025cb  "}.lst-kix_9uzslqp6q29w-0>li:before{content:"\0025cf  "}ul.lst-kix_hait203o5dqo-1{list-style-type:none}ul.lst-kix_hait203o5dqo-0{list-style-type:none}ul.lst-kix_hait203o5dqo-3{list-style-type:none}ul.lst-kix_hait203o5dqo-2{list-style-type:none}ul.lst-kix_hait203o5dqo-5{list-style-type:none}ul.lst-kix_hait203o5dqo-4{list-style-type:none}ul.lst-kix_hait203o5dqo-7{list-style-type:none}.lst-kix_8dw3ycrgt362-1>li{counter-increment:lst-ctn-kix_8dw3ycrgt362-1}ul.lst-kix_hait203o5dqo-6{list-style-type:none}ul.lst-kix_hait203o5dqo-8{list-style-type:none}.lst-kix_8d6owvwxykff-5>li:before{content:"\0025a0  "}.lst-kix_8d6owvwxykff-4>li:before{content:"\0025cb  "}.lst-kix_8d6owvwxykff-1>li:before{content:"\0025cb  "}.lst-kix_7pcs9kbg0qhi-3>li{counter-increment:lst-ctn-kix_7pcs9kbg0qhi-3}.lst-kix_msukp0gxz1c7-7>li:before{content:"\0025cb  "}.lst-kix_msukp0gxz1c7-6>li:before{content:"\0025cf  "}.lst-kix_msukp0gxz1c7-3>li:before{content:"\0025cf  "}.lst-kix_8d6owvwxykff-8>li:before{content:"\0025a0  "}.lst-kix_msukp0gxz1c7-2>li:before{content:"\0025a0  "}.lst-kix_6wlb53t5sfma-3>li:before{content:"\0025cf  "}.lst-kix_3z0hr7pzpec4-2>li:before{content:"\0025a0  "}.lst-kix_3z0hr7pzpec4-6>li:before{content:"\0025cf  "}.lst-kix_6wlb53t5sfma-7>li:before{content:"\0025cb  "}ol.lst-kix_7pcs9kbg0qhi-7.start{counter-reset:lst-ctn-kix_7pcs9kbg0qhi-7 0}.lst-kix_8dw3ycrgt362-8>li{counter-increment:lst-ctn-kix_8dw3ycrgt362-8}.lst-kix_ao97xhpq4hr9-8>li:before{content:"\0025a0  "}.lst-kix_7n1btf6e2iyv-2>li:before{content:"\0025a0  "}.lst-kix_mfwegjdddisb-6>li:before{content:"\0025cf  "}.lst-kix_e8kaqrotva4-6>li:before{content:"\0025cf  "}.lst-kix_ao97xhpq4hr9-4>li:before{content:"\0025cb  "}.lst-kix_ao97xhpq4hr9-0>li:before{content:"\0025cf  "}.lst-kix_7n1btf6e2iyv-6>li:before{content:"\0025cf  "}.lst-kix_mfwegjdddisb-2>li:before{content:"\0025a0  "}.lst-kix_w93gux8gcekz-1>li:before{content:"-  "}.lst-kix_dnm4ke3nhj1e-2>li:before{content:"\0025a0  "}.lst-kix_dnm4ke3nhj1e-6>li:before{content:"\0025cf  "}ul.lst-kix_38adif7m6rbx-0{list-style-type:none}ul.lst-kix_38adif7m6rbx-1{list-style-type:none}ul.lst-kix_38adif7m6rbx-2{list-style-type:none}ul.lst-kix_38adif7m6rbx-3{list-style-type:none}ul.lst-kix_owgllipbp8j5-0{list-style-type:none}.lst-kix_r60cdwx1dlic-2>li:before{content:"\0025a0  "}ul.lst-kix_owgllipbp8j5-1{list-style-type:none}ul.lst-kix_38adif7m6rbx-8{list-style-type:none}ul.lst-kix_owgllipbp8j5-2{list-style-type:none}.lst-kix_9uzslqp6q29w-7>li:before{content:"\0025cb  "}ul.lst-kix_owgllipbp8j5-3{list-style-type:none}ul.lst-kix_owgllipbp8j5-4{list-style-type:none}ul.lst-kix_owgllipbp8j5-5{list-style-type:none}ul.lst-kix_38adif7m6rbx-4{list-style-type:none}ul.lst-kix_owgllipbp8j5-6{list-style-type:none}ul.lst-kix_38adif7m6rbx-5{list-style-type:none}ul.lst-kix_owgllipbp8j5-7{list-style-type:none}ul.lst-kix_38adif7m6rbx-6{list-style-type:none}ul.lst-kix_owgllipbp8j5-8{list-style-type:none}ul.lst-kix_38adif7m6rbx-7{list-style-type:none}.lst-kix_w93gux8gcekz-5>li:before{content:"-  "}.lst-kix_um8gcdc8t9m9-0>li:before{content:"-  "}.lst-kix_e8kaqrotva4-2>li:before{content:"\0025a0  "}.lst-kix_8dw3ycrgt362-8>li:before{content:"" counter(lst-ctn-kix_8dw3ycrgt362-8,lower-roman) ". "}.lst-kix_mvnr2hoa94kd-5>li:before{content:"\0025a0  "}.lst-kix_5we8o0btcr67-6>li:before{content:"\0025cf  "}.lst-kix_ne2kodfi1kix-2>li:before{content:"\0025a0  "}.lst-kix_8dw3ycrgt362-4>li:before{content:"" counter(lst-ctn-kix_8dw3ycrgt362-4,lower-latin) ". "}.lst-kix_5s83gilt2o7g-2>li:before{content:"\0025a0  "}.lst-kix_ne2kodfi1kix-6>li:before{content:"\0025cf  "}.lst-kix_8dw3ycrgt362-0>li:before{content:"" counter(lst-ctn-kix_8dw3ycrgt362-0,decimal) ". "}.lst-kix_r60cdwx1dlic-6>li:before{content:"\0025cf  "}.lst-kix_7pcs9kbg0qhi-6>li:before{content:"" counter(lst-ctn-kix_7pcs9kbg0qhi-6,decimal) ". "}.lst-kix_7pcs9kbg0qhi-1>li:before{content:"" counter(lst-ctn-kix_7pcs9kbg0qhi-1,lower-latin) ". "}.lst-kix_7pcs9kbg0qhi-3>li:before{content:"" counter(lst-ctn-kix_7pcs9kbg0qhi-3,decimal) ". "}ul.lst-kix_rsr7bmzen4a7-0{list-style-type:none}ul.lst-kix_s4rut6kfjze3-2{list-style-type:none}ul.lst-kix_rsr7bmzen4a7-3{list-style-type:none}ul.lst-kix_s4rut6kfjze3-3{list-style-type:none}ul.lst-kix_rsr7bmzen4a7-4{list-style-type:none}ul.lst-kix_s4rut6kfjze3-4{list-style-type:none}ul.lst-kix_rsr7bmzen4a7-1{list-style-type:none}ul.lst-kix_s4rut6kfjze3-5{list-style-type:none}ul.lst-kix_rsr7bmzen4a7-2{list-style-type:none}ul.lst-kix_s4rut6kfjze3-6{list-style-type:none}ul.lst-kix_rsr7bmzen4a7-7{list-style-type:none}.lst-kix_7pcs9kbg0qhi-4>li:before{content:"" counter(lst-ctn-kix_7pcs9kbg0qhi-4,lower-latin) ". "}ul.lst-kix_s4rut6kfjze3-7{list-style-type:none}ul.lst-kix_rsr7bmzen4a7-8{list-style-type:none}ul.lst-kix_s4rut6kfjze3-8{list-style-type:none}ul.lst-kix_rsr7bmzen4a7-5{list-style-type:none}ul.lst-kix_rsr7bmzen4a7-6{list-style-type:none}ul.lst-kix_9sdxipg1h5wr-5{list-style-type:none}ul.lst-kix_9sdxipg1h5wr-4{list-style-type:none}ul.lst-kix_9sdxipg1h5wr-3{list-style-type:none}ul.lst-kix_9sdxipg1h5wr-2{list-style-type:none}ul.lst-kix_9sdxipg1h5wr-8{list-style-type:none}ul.lst-kix_s4rut6kfjze3-0{list-style-type:none}ul.lst-kix_9sdxipg1h5wr-7{list-style-type:none}ul.lst-kix_s4rut6kfjze3-1{list-style-type:none}ul.lst-kix_9sdxipg1h5wr-6{list-style-type:none}.lst-kix_9sdxipg1h5wr-8>li:before{content:"\0025a0  "}.lst-kix_8dw3ycrgt362-2>li{counter-increment:lst-ctn-kix_8dw3ycrgt362-2}.lst-kix_9sdxipg1h5wr-7>li:before{content:"\0025cb  "}ul.lst-kix_9sdxipg1h5wr-1{list-style-type:none}ul.lst-kix_9sdxipg1h5wr-0{list-style-type:none}.lst-kix_9sdxipg1h5wr-5>li:before{content:"\0025a0  "}.lst-kix_9sdxipg1h5wr-0>li:before{content:"\0025cf  "}.lst-kix_9sdxipg1h5wr-2>li:before{content:"\0025a0  "}.lst-kix_3bekfbfd6o8j-7>li:before{content:"\0025cb  "}.lst-kix_hait203o5dqo-4>li:before{content:"-  "}.lst-kix_hait203o5dqo-2>li:before{content:"-  "}.lst-kix_hait203o5dqo-1>li:before{content:"-  "}.lst-kix_2aliey961vrg-0>li:before{content:"\0025cf  "}.lst-kix_6wlb53t5sfma-2>li:before{content:"\0025a0  "}.lst-kix_2aliey961vrg-5>li:before{content:"\0025a0  "}.lst-kix_lqb82f5hzmh3-3>li:before{content:"\0025cf  "}.lst-kix_2aliey961vrg-3>li:before{content:"\0025cf  "}.lst-kix_6wlb53t5sfma-0>li:before{content:"\0025cf  "}.lst-kix_6wlb53t5sfma-8>li:before{content:"\0025a0  "}.lst-kix_38adif7m6rbx-6>li:before{content:"\0025cf  "}.lst-kix_lqb82f5hzmh3-1>li:before{content:"\0025cb  "}.lst-kix_38adif7m6rbx-8>li:before{content:"\0025a0  "}ul.lst-kix_w93gux8gcekz-8{list-style-type:none}.lst-kix_e8kaqrotva4-7>li:before{content:"\0025cb  "}.lst-kix_c4453ihcjob0-1>li:before{content:"\0025cb  "}.lst-kix_3bekfbfd6o8j-1>li:before{content:"\0025cb  "}.lst-kix_8dw3ycrgt362-6>li{counter-increment:lst-ctn-kix_8dw3ycrgt362-6}.lst-kix_r60cdwx1dlic-3>li:before{content:"\0025cf  "}.lst-kix_w93gux8gcekz-0>li:before{content:"-  "}.lst-kix_w93gux8gcekz-8>li:before{content:"-  "}.lst-kix_e8kaqrotva4-1>li:before{content:"\0025cb  "}.lst-kix_w93gux8gcekz-6>li:before{content:"-  "}ul.lst-kix_w93gux8gcekz-7{list-style-type:none}ul.lst-kix_w93gux8gcekz-6{list-style-type:none}.lst-kix_9i19e8txouig-4>li:before{content:"\0025cb  "}ul.lst-kix_w93gux8gcekz-5{list-style-type:none}ul.lst-kix_w93gux8gcekz-4{list-style-type:none}ul.lst-kix_w93gux8gcekz-3{list-style-type:none}.lst-kix_9uzslqp6q29w-8>li:before{content:"\0025a0  "}ul.lst-kix_w93gux8gcekz-2{list-style-type:none}.lst-kix_9i19e8txouig-2>li:before{content:"\0025a0  "}.lst-kix_c4453ihcjob0-7>li:before{content:"\0025cb  "}ul.lst-kix_w93gux8gcekz-1{list-style-type:none}ul.lst-kix_w93gux8gcekz-0{list-style-type:none}.lst-kix_6shy0i3drli2-6>li:before{content:"-  "}.lst-kix_38adif7m6rbx-0>li:before{content:"\0025cf  "}.lst-kix_r60cdwx1dlic-5>li:before{content:"\0025a0  "}.lst-kix_6shy0i3drli2-4>li:before{content:"-  "}.lst-kix_owgllipbp8j5-4>li:before{content:"\0025cb  "}.lst-kix_5s83gilt2o7g-7>li:before{content:"\0025cb  "}.lst-kix_g3mw20emt8ab-8>li:before{content:"\0025a0  "}.lst-kix_5we8o0btcr67-5>li:before{content:"\0025a0  "}ul.lst-kix_mfwegjdddisb-6{list-style-type:none}.lst-kix_5we8o0btcr67-0>li:before{content:"\0025cf  "}ul.lst-kix_mfwegjdddisb-5{list-style-type:none}ul.lst-kix_mfwegjdddisb-4{list-style-type:none}ul.lst-kix_mfwegjdddisb-3{list-style-type:none}.lst-kix_g3mw20emt8ab-5>li:before{content:"\0025a0  "}ul.lst-kix_mfwegjdddisb-8{list-style-type:none}ul.lst-kix_mfwegjdddisb-7{list-style-type:none}.lst-kix_hm0ybxxbxp2l-1>li:before{content:"\0025cb  "}.lst-kix_mvnr2hoa94kd-4>li:before{content:"\0025cb  "}.lst-kix_hm0ybxxbxp2l-4>li:before{content:"\0025cb  "}.lst-kix_9uzslqp6q29w-2>li:before{content:"\0025a0  "}ul.lst-kix_lqb82f5hzmh3-0{list-style-type:none}ul.lst-kix_5gii2tr4ae90-1{list-style-type:none}ul.lst-kix_5gii2tr4ae90-0{list-style-type:none}ul.lst-kix_5gii2tr4ae90-3{list-style-type:none}ul.lst-kix_5gii2tr4ae90-2{list-style-type:none}ul.lst-kix_5gii2tr4ae90-5{list-style-type:none}ul.lst-kix_5gii2tr4ae90-4{list-style-type:none}ul.lst-kix_5gii2tr4ae90-7{list-style-type:none}ul.lst-kix_5gii2tr4ae90-6{list-style-type:none}ul.lst-kix_5gii2tr4ae90-8{list-style-type:none}.lst-kix_7pcs9kbg0qhi-7>li{counter-increment:lst-ctn-kix_7pcs9kbg0qhi-7}ul.lst-kix_ne2kodfi1kix-3{list-style-type:none}ul.lst-kix_ne2kodfi1kix-2{list-style-type:none}.lst-kix_p3wa2k83s8cn-2>li:before{content:"\0025a0  "}ul.lst-kix_ne2kodfi1kix-1{list-style-type:none}ul.lst-kix_ne2kodfi1kix-0{list-style-type:none}ul.lst-kix_ne2kodfi1kix-7{list-style-type:none}ul.lst-kix_ne2kodfi1kix-6{list-style-type:none}ul.lst-kix_ne2kodfi1kix-5{list-style-type:none}ul.lst-kix_ne2kodfi1kix-4{list-style-type:none}.lst-kix_rsr7bmzen4a7-6>li:before{content:"\0025cf  "}ul.lst-kix_ne2kodfi1kix-8{list-style-type:none}.lst-kix_p3wa2k83s8cn-5>li:before{content:"\0025a0  "}.lst-kix_jbicnccaji3d-2>li:before{content:"\0025a0  "}ul.lst-kix_lqb82f5hzmh3-2{list-style-type:none}ul.lst-kix_lqb82f5hzmh3-1{list-style-type:none}ul.lst-kix_lqb82f5hzmh3-4{list-style-type:none}ul.lst-kix_lqb82f5hzmh3-3{list-style-type:none}ul.lst-kix_lqb82f5hzmh3-6{list-style-type:none}ul.lst-kix_lqb82f5hzmh3-5{list-style-type:none}ul.lst-kix_lqb82f5hzmh3-8{list-style-type:none}ul.lst-kix_lqb82f5hzmh3-7{list-style-type:none}.lst-kix_rop5sudr37z1-4>li:before{content:"\0025cb  "}.lst-kix_jbicnccaji3d-5>li:before{content:"\0025a0  "}.lst-kix_8d6owvwxykff-6>li:before{content:"\0025cf  "}ul.lst-kix_mfwegjdddisb-2{list-style-type:none}.lst-kix_msukp0gxz1c7-1>li:before{content:"\0025cb  "}ul.lst-kix_mfwegjdddisb-1{list-style-type:none}.lst-kix_rop5sudr37z1-7>li:before{content:"\0025cb  "}ul.lst-kix_mfwegjdddisb-0{list-style-type:none}.lst-kix_7pcs9kbg0qhi-0>li{counter-increment:lst-ctn-kix_7pcs9kbg0qhi-0}.lst-kix_3sh9r2c1w16g-4>li:before{content:"\0025cb  "}.lst-kix_8d6owvwxykff-3>li:before{content:"\0025cf  "}.lst-kix_g3mw20emt8ab-0>li:before{content:"\0025cf  "}.lst-kix_3sh9r2c1w16g-1>li:before{content:"\0025cb  "}.lst-kix_msukp0gxz1c7-4>li:before{content:"\0025cb  "}.lst-kix_rsr7bmzen4a7-1>li:before{content:"\0025cb  "}.lst-kix_6wlb53t5sfma-5>li:before{content:"\0025a0  "}.lst-kix_lqb82f5hzmh3-6>li:before{content:"\0025cf  "}ul.lst-kix_um8gcdc8t9m9-1{list-style-type:none}ul.lst-kix_um8gcdc8t9m9-2{list-style-type:none}ul.lst-kix_um8gcdc8t9m9-0{list-style-type:none}.lst-kix_3z0hr7pzpec4-4>li:before{content:"\0025cb  "}ul.lst-kix_um8gcdc8t9m9-5{list-style-type:none}ul.lst-kix_um8gcdc8t9m9-6{list-style-type:none}.lst-kix_38adif7m6rbx-3>li:before{content:"\0025cf  "}ul.lst-kix_um8gcdc8t9m9-3{list-style-type:none}ul.lst-kix_um8gcdc8t9m9-4{list-style-type:none}ul.lst-kix_6shy0i3drli2-7{list-style-type:none}ul.lst-kix_6shy0i3drli2-6{list-style-type:none}ul.lst-kix_um8gcdc8t9m9-7{list-style-type:none}ul.lst-kix_um8gcdc8t9m9-8{list-style-type:none}ul.lst-kix_6shy0i3drli2-8{list-style-type:none}.lst-kix_2aliey961vrg-8>li:before{content:"\0025a0  "}.lst-kix_s4rut6kfjze3-2>li:before{content:"\0025a0  "}.lst-kix_47504ojh0ba1-8>li:before{content:"\0025a0  "}.lst-kix_hait203o5dqo-7>li:before{content:"-  "}ul.lst-kix_6shy0i3drli2-3{list-style-type:none}.lst-kix_5gii2tr4ae90-7>li:before{content:"\0025cb  "}.lst-kix_mfwegjdddisb-0>li:before{content:"\0025cf  "}ul.lst-kix_6shy0i3drli2-2{list-style-type:none}.lst-kix_mfwegjdddisb-8>li:before{content:"\0025a0  "}ul.lst-kix_6shy0i3drli2-5{list-style-type:none}ul.lst-kix_6shy0i3drli2-4{list-style-type:none}.lst-kix_9i19e8txouig-7>li:before{content:"\0025cb  "}ul.lst-kix_6shy0i3drli2-1{list-style-type:none}ul.lst-kix_6shy0i3drli2-0{list-style-type:none}.lst-kix_3bekfbfd6o8j-4>li:before{content:"\0025cb  "}.lst-kix_7n1btf6e2iyv-4>li:before{content:"\0025cb  "}.lst-kix_ao97xhpq4hr9-2>li:before{content:"\0025a0  "}.lst-kix_9reixuath9e3-5>li:before{content:"\0025a0  "}ul.lst-kix_g3mw20emt8ab-8{list-style-type:none}.lst-kix_w93gux8gcekz-3>li:before{content:"-  "}.lst-kix_rj34zh1l62l0-8>li:before{content:"\0025a0  "}.lst-kix_r60cdwx1dlic-0>li:before{content:"\0025cf  "}.lst-kix_vd8i8whhh56x-1>li:before{content:"-  "}ul.lst-kix_g3mw20emt8ab-0{list-style-type:none}ul.lst-kix_g3mw20emt8ab-1{list-style-type:none}ul.lst-kix_g3mw20emt8ab-2{list-style-type:none}.lst-kix_dnm4ke3nhj1e-0>li:before{content:"\0025cf  "}ul.lst-kix_g3mw20emt8ab-3{list-style-type:none}ul.lst-kix_g3mw20emt8ab-4{list-style-type:none}.lst-kix_c4453ihcjob0-4>li:before{content:"\0025cb  "}ul.lst-kix_g3mw20emt8ab-5{list-style-type:none}ul.lst-kix_g3mw20emt8ab-6{list-style-type:none}ul.lst-kix_g3mw20emt8ab-7{list-style-type:none}.lst-kix_rj34zh1l62l0-0>li:before{content:"\0025cf  "}.lst-kix_9uzslqp6q29w-5>li:before{content:"\0025a0  "}.lst-kix_e8kaqrotva4-4>li:before{content:"\0025cb  "}.lst-kix_5iyzxwlmxp1f-5>li:before{content:"\0025a0  "}.lst-kix_47504ojh0ba1-0>li:before{content:"\0025cf  "}.lst-kix_6shy0i3drli2-1>li:before{content:"-  "}.lst-kix_8dw3ycrgt362-6>li:before{content:"" counter(lst-ctn-kix_8dw3ycrgt362-6,decimal) ". "}.lst-kix_mvnr2hoa94kd-7>li:before{content:"\0025cb  "}.lst-kix_ne2kodfi1kix-0>li:before{content:"\0025cf  "}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}ul.lst-kix_rc9zyflo7924-8{list-style-type:none}ul.lst-kix_rc9zyflo7924-7{list-style-type:none}ul.lst-kix_rj34zh1l62l0-7{list-style-type:none}ul.lst-kix_rc9zyflo7924-4{list-style-type:none}ul.lst-kix_rj34zh1l62l0-8{list-style-type:none}ul.lst-kix_rc9zyflo7924-3{list-style-type:none}ul.lst-kix_rc9zyflo7924-6{list-style-type:none}.lst-kix_r60cdwx1dlic-8>li:before{content:"\0025a0  "}ul.lst-kix_rc9zyflo7924-5{list-style-type:none}.lst-kix_5we8o0btcr67-8>li:before{content:"\0025a0  "}.lst-kix_5s83gilt2o7g-4>li:before{content:"\0025cb  "}ul.lst-kix_rj34zh1l62l0-3{list-style-type:none}ul.lst-kix_rc9zyflo7924-0{list-style-type:none}ul.lst-kix_rj34zh1l62l0-4{list-style-type:none}.lst-kix_owgllipbp8j5-1>li:before{content:"\0025cb  "}ul.lst-kix_rj34zh1l62l0-5{list-style-type:none}ul.lst-kix_rc9zyflo7924-2{list-style-type:none}ul.lst-kix_rj34zh1l62l0-6{list-style-type:none}ul.lst-kix_rc9zyflo7924-1{list-style-type:none}ul.lst-kix_rj34zh1l62l0-0{list-style-type:none}ul.lst-kix_rj34zh1l62l0-1{list-style-type:none}ul.lst-kix_rj34zh1l62l0-2{list-style-type:none}.lst-kix_rc9zyflo7924-0>li:before{content:"\0025cf  "}.lst-kix_rc9zyflo7924-2>li:before{content:"\0025a0  "}.lst-kix_rc9zyflo7924-3>li:before{content:"\0025cf  "}.lst-kix_rc9zyflo7924-1>li:before{content:"\0025cb  "}.lst-kix_8dw3ycrgt362-4>li{counter-increment:lst-ctn-kix_8dw3ycrgt362-4}ul.lst-kix_5s83gilt2o7g-0{list-style-type:none}ul.lst-kix_47504ojh0ba1-8{list-style-type:none}ul.lst-kix_5s83gilt2o7g-4{list-style-type:none}ul.lst-kix_5s83gilt2o7g-3{list-style-type:none}ul.lst-kix_5s83gilt2o7g-2{list-style-type:none}ul.lst-kix_5s83gilt2o7g-1{list-style-type:none}ul.lst-kix_5s83gilt2o7g-8{list-style-type:none}ul.lst-kix_5s83gilt2o7g-7{list-style-type:none}ul.lst-kix_5s83gilt2o7g-6{list-style-type:none}ul.lst-kix_5s83gilt2o7g-5{list-style-type:none}ul.lst-kix_47504ojh0ba1-2{list-style-type:none}ul.lst-kix_47504ojh0ba1-3{list-style-type:none}ul.lst-kix_47504ojh0ba1-0{list-style-type:none}ul.lst-kix_47504ojh0ba1-1{list-style-type:none}ul.lst-kix_47504ojh0ba1-6{list-style-type:none}ul.lst-kix_47504ojh0ba1-7{list-style-type:none}ul.lst-kix_47504ojh0ba1-4{list-style-type:none}ul.lst-kix_47504ojh0ba1-5{list-style-type:none}.lst-kix_rc9zyflo7924-8>li:before{content:"\0025a0  "}ol.lst-kix_7pcs9kbg0qhi-4.start{counter-reset:lst-ctn-kix_7pcs9kbg0qhi-4 0}.lst-kix_rc9zyflo7924-7>li:before{content:"\0025cb  "}.lst-kix_rc9zyflo7924-4>li:before{content:"\0025cb  "}.lst-kix_rc9zyflo7924-6>li:before{content:"\0025cf  "}.lst-kix_rc9zyflo7924-5>li:before{content:"\0025a0  "}.lst-kix_9reixuath9e3-6>li:before{content:"\0025cf  "}ol.lst-kix_8dw3ycrgt362-5{list-style-type:none}ol.lst-kix_8dw3ycrgt362-6{list-style-type:none}ol.lst-kix_8dw3ycrgt362-7{list-style-type:none}ol.lst-kix_8dw3ycrgt362-8{list-style-type:none}.lst-kix_9reixuath9e3-2>li:before{content:"\0025a0  "}.lst-kix_9reixuath9e3-8>li:before{content:"\0025a0  "}.lst-kix_9reixuath9e3-0>li:before{content:"\0025cf  "}.lst-kix_s4rut6kfjze3-7>li:before{content:"\0025cb  "}ul.lst-kix_6wlb53t5sfma-1{list-style-type:none}ul.lst-kix_6wlb53t5sfma-0{list-style-type:none}ul.lst-kix_6wlb53t5sfma-3{list-style-type:none}ul.lst-kix_6wlb53t5sfma-2{list-style-type:none}ol.lst-kix_7pcs9kbg0qhi-7{list-style-type:none}.lst-kix_47504ojh0ba1-5>li:before{content:"\0025a0  "}ol.lst-kix_7pcs9kbg0qhi-8{list-style-type:none}ul.lst-kix_6wlb53t5sfma-8{list-style-type:none}ol.lst-kix_7pcs9kbg0qhi-5{list-style-type:none}ol.lst-kix_7pcs9kbg0qhi-6{list-style-type:none}ul.lst-kix_6wlb53t5sfma-5{list-style-type:none}.lst-kix_47504ojh0ba1-3>li:before{content:"\0025cf  "}.lst-kix_47504ojh0ba1-7>li:before{content:"\0025cb  "}ul.lst-kix_6wlb53t5sfma-4{list-style-type:none}ul.lst-kix_6wlb53t5sfma-7{list-style-type:none}ul.lst-kix_3sh9r2c1w16g-8{list-style-type:none}.lst-kix_s4rut6kfjze3-3>li:before{content:"\0025cf  "}ul.lst-kix_6wlb53t5sfma-6{list-style-type:none}ul.lst-kix_3sh9r2c1w16g-7{list-style-type:none}.lst-kix_5gii2tr4ae90-8>li:before{content:"\0025a0  "}.lst-kix_47504ojh0ba1-1>li:before{content:"\0025cb  "}ol.lst-kix_8dw3ycrgt362-5.start{counter-reset:lst-ctn-kix_8dw3ycrgt362-5 0}.lst-kix_s4rut6kfjze3-5>li:before{content:"\0025a0  "}ol.lst-kix_8dw3ycrgt362-1{list-style-type:none}ul.lst-kix_3sh9r2c1w16g-6{list-style-type:none}ol.lst-kix_8dw3ycrgt362-2{list-style-type:none}ul.lst-kix_3sh9r2c1w16g-5{list-style-type:none}ol.lst-kix_8dw3ycrgt362-3{list-style-type:none}ul.lst-kix_3sh9r2c1w16g-4{list-style-type:none}ol.lst-kix_8dw3ycrgt362-4{list-style-type:none}.lst-kix_9reixuath9e3-4>li:before{content:"\0025cb  "}ul.lst-kix_3sh9r2c1w16g-3{list-style-type:none}ul.lst-kix_3sh9r2c1w16g-2{list-style-type:none}ul.lst-kix_3sh9r2c1w16g-1{list-style-type:none}ul.lst-kix_3sh9r2c1w16g-0{list-style-type:none}ol.lst-kix_8dw3ycrgt362-0{list-style-type:none}.lst-kix_s4rut6kfjze3-1>li:before{content:"\0025cb  "}.lst-kix_vd8i8whhh56x-2>li:before{content:"-  "}.lst-kix_vd8i8whhh56x-4>li:before{content:"-  "}ul.lst-kix_c4453ihcjob0-1{list-style-type:none}ul.lst-kix_c4453ihcjob0-2{list-style-type:none}ul.lst-kix_c4453ihcjob0-0{list-style-type:none}.lst-kix_rj34zh1l62l0-5>li:before{content:"\0025a0  "}ul.lst-kix_c4453ihcjob0-5{list-style-type:none}ul.lst-kix_c4453ihcjob0-6{list-style-type:none}ul.lst-kix_9reixuath9e3-0{list-style-type:none}ul.lst-kix_c4453ihcjob0-3{list-style-type:none}.lst-kix_5gii2tr4ae90-0>li:before{content:"\0025cf  "}ul.lst-kix_c4453ihcjob0-4{list-style-type:none}.lst-kix_vd8i8whhh56x-0>li:before{content:"-  "}.lst-kix_vd8i8whhh56x-6>li:before{content:"-  "}.lst-kix_vd8i8whhh56x-8>li:before{content:"-  "}ul.lst-kix_9reixuath9e3-2{list-style-type:none}ul.lst-kix_9reixuath9e3-1{list-style-type:none}ul.lst-kix_9reixuath9e3-4{list-style-type:none}ul.lst-kix_c4453ihcjob0-7{list-style-type:none}ul.lst-kix_9reixuath9e3-3{list-style-type:none}ul.lst-kix_c4453ihcjob0-8{list-style-type:none}.lst-kix_rj34zh1l62l0-7>li:before{content:"\0025cb  "}ul.lst-kix_9reixuath9e3-6{list-style-type:none}ul.lst-kix_9reixuath9e3-5{list-style-type:none}ul.lst-kix_9reixuath9e3-8{list-style-type:none}ul.lst-kix_9reixuath9e3-7{list-style-type:none}.lst-kix_5gii2tr4ae90-2>li:before{content:"\0025a0  "}.lst-kix_5gii2tr4ae90-4>li:before{content:"\0025cb  "}.lst-kix_5iyzxwlmxp1f-8>li:before{content:"\0025a0  "}.lst-kix_rj34zh1l62l0-1>li:before{content:"\0025cb  "}ol.lst-kix_7pcs9kbg0qhi-0{list-style-type:none}.lst-kix_5gii2tr4ae90-6>li:before{content:"\0025cf  "}.lst-kix_5iyzxwlmxp1f-6>li:before{content:"\0025cf  "}ol.lst-kix_7pcs9kbg0qhi-3{list-style-type:none}ol.lst-kix_7pcs9kbg0qhi-4{list-style-type:none}ol.lst-kix_7pcs9kbg0qhi-1{list-style-type:none}ol.lst-kix_7pcs9kbg0qhi-2{list-style-type:none}.lst-kix_rj34zh1l62l0-3>li:before{content:"\0025cf  "}.lst-kix_5iyzxwlmxp1f-0>li:before{content:"\0025cf  "}.lst-kix_5iyzxwlmxp1f-2>li:before{content:"\0025a0  "}.lst-kix_5iyzxwlmxp1f-4>li:before{content:"\0025cb  "}ul.lst-kix_rop5sudr37z1-0{list-style-type:none}ul.lst-kix_rop5sudr37z1-1{list-style-type:none}ul.lst-kix_rop5sudr37z1-2{list-style-type:none}ul.lst-kix_rop5sudr37z1-3{list-style-type:none}.lst-kix_8dw3ycrgt362-3>li{counter-increment:lst-ctn-kix_8dw3ycrgt362-3}.lst-kix_g3mw20emt8ab-6>li:before{content:"\0025cf  "}.lst-kix_owgllipbp8j5-0>li:before{content:"\0025cf  "}.lst-kix_g3mw20emt8ab-7>li:before{content:"\0025cb  "}.lst-kix_owgllipbp8j5-6>li:before{content:"\0025cf  "}.lst-kix_owgllipbp8j5-7>li:before{content:"\0025cb  "}.lst-kix_g3mw20emt8ab-2>li:before{content:"\0025a0  "}.lst-kix_g3mw20emt8ab-3>li:before{content:"\0025cf  "}ul.lst-kix_rop5sudr37z1-8{list-style-type:none}ul.lst-kix_rop5sudr37z1-4{list-style-type:none}ul.lst-kix_rop5sudr37z1-5{list-style-type:none}ul.lst-kix_rop5sudr37z1-6{list-style-type:none}ul.lst-kix_rop5sudr37z1-7{list-style-type:none}.lst-kix_8dw3ycrgt362-5>li{counter-increment:lst-ctn-kix_8dw3ycrgt362-5}.lst-kix_rop5sudr37z1-2>li:before{content:"\0025a0  "}.lst-kix_rop5sudr37z1-1>li:before{content:"\0025cb  "}.lst-kix_p3wa2k83s8cn-0>li:before{content:"\0025cf  "}.lst-kix_p3wa2k83s8cn-3>li:before{content:"\0025cf  "}.lst-kix_jbicnccaji3d-3>li:before{content:"\0025cf  "}.lst-kix_jbicnccaji3d-0>li:before{content:"\0025cf  "}.lst-kix_jbicnccaji3d-4>li:before{content:"\0025cb  "}.lst-kix_rsr7bmzen4a7-7>li:before{content:"\0025cb  "}ul.lst-kix_7n1btf6e2iyv-3{list-style-type:none}.lst-kix_rsr7bmzen4a7-4>li:before{content:"\0025cb  "}ul.lst-kix_7n1btf6e2iyv-2{list-style-type:none}.lst-kix_p3wa2k83s8cn-4>li:before{content:"\0025cb  "}ul.lst-kix_7n1btf6e2iyv-1{list-style-type:none}ul.lst-kix_7n1btf6e2iyv-0{list-style-type:none}.lst-kix_jbicnccaji3d-8>li:before{content:"\0025a0  "}.lst-kix_jbicnccaji3d-7>li:before{content:"\0025cb  "}.lst-kix_p3wa2k83s8cn-7>li:before{content:"\0025cb  "}ol.lst-kix_7pcs9kbg0qhi-5.start{counter-reset:lst-ctn-kix_7pcs9kbg0qhi-5 0}.lst-kix_3sh9r2c1w16g-6>li:before{content:"\0025cf  "}.lst-kix_rop5sudr37z1-6>li:before{content:"\0025cf  "}.lst-kix_rsr7bmzen4a7-8>li:before{content:"\0025a0  "}.lst-kix_3sh9r2c1w16g-7>li:before{content:"\0025cb  "}.lst-kix_p3wa2k83s8cn-8>li:before{content:"\0025a0  "}.lst-kix_rop5sudr37z1-5>li:before{content:"\0025a0  "}.lst-kix_3sh9r2c1w16g-3>li:before{content:"\0025cf  "}.lst-kix_3sh9r2c1w16g-2>li:before{content:"\0025a0  "}.lst-kix_rsr7bmzen4a7-3>li:before{content:"\0025cf  "}.lst-kix_rsr7bmzen4a7-0>li:before{content:"\0025cf  "}ul.lst-kix_r60cdwx1dlic-8{list-style-type:none}.lst-kix_2aliey961vrg-6>li:before{content:"\0025cf  "}.lst-kix_lqb82f5hzmh3-4>li:before{content:"\0025cb  "}ul.lst-kix_r60cdwx1dlic-5{list-style-type:none}ul.lst-kix_r60cdwx1dlic-4{list-style-type:none}ul.lst-kix_r60cdwx1dlic-7{list-style-type:none}.lst-kix_7pcs9kbg0qhi-1>li{counter-increment:lst-ctn-kix_7pcs9kbg0qhi-1}.lst-kix_9reixuath9e3-3>li:before{content:"\0025cf  "}.lst-kix_9reixuath9e3-7>li:before{content:"\0025cb  "}ul.lst-kix_r60cdwx1dlic-6{list-style-type:none}ul.lst-kix_r60cdwx1dlic-1{list-style-type:none}ul.lst-kix_r60cdwx1dlic-0{list-style-type:none}.lst-kix_2aliey961vrg-2>li:before{content:"\0025a0  "}ul.lst-kix_r60cdwx1dlic-3{list-style-type:none}ul.lst-kix_r60cdwx1dlic-2{list-style-type:none}.lst-kix_s4rut6kfjze3-8>li:before{content:"\0025a0  "}.lst-kix_38adif7m6rbx-5>li:before{content:"\0025a0  "}.lst-kix_lqb82f5hzmh3-0>li:before{content:"\0025cf  "}.lst-kix_s4rut6kfjze3-0>li:before{content:"\0025cf  "}ul.lst-kix_9i19e8txouig-8{list-style-type:none}ul.lst-kix_5we8o0btcr67-0{list-style-type:none}ul.lst-kix_9i19e8txouig-6{list-style-type:none}ul.lst-kix_9i19e8txouig-7{list-style-type:none}ul.lst-kix_9i19e8txouig-4{list-style-type:none}ul.lst-kix_5we8o0btcr67-3{list-style-type:none}.lst-kix_9i19e8txouig-5>li:before{content:"\0025a0  "}ul.lst-kix_9i19e8txouig-5{list-style-type:none}ul.lst-kix_5we8o0btcr67-4{list-style-type:none}ul.lst-kix_9i19e8txouig-2{list-style-type:none}ul.lst-kix_5we8o0btcr67-1{list-style-type:none}ul.lst-kix_9i19e8txouig-3{list-style-type:none}ul.lst-kix_5we8o0btcr67-2{list-style-type:none}.lst-kix_s4rut6kfjze3-4>li:before{content:"\0025cb  "}ul.lst-kix_9i19e8txouig-0{list-style-type:none}ul.lst-kix_5we8o0btcr67-7{list-style-type:none}ul.lst-kix_9i19e8txouig-1{list-style-type:none}ul.lst-kix_5we8o0btcr67-8{list-style-type:none}ul.lst-kix_5we8o0btcr67-5{list-style-type:none}.lst-kix_47504ojh0ba1-2>li:before{content:"\0025a0  "}ul.lst-kix_5we8o0btcr67-6{list-style-type:none}ul.lst-kix_7n1btf6e2iyv-7{list-style-type:none}ul.lst-kix_jbicnccaji3d-0{list-style-type:none}ul.lst-kix_7n1btf6e2iyv-6{list-style-type:none}ul.lst-kix_jbicnccaji3d-1{list-style-type:none}ul.lst-kix_7n1btf6e2iyv-5{list-style-type:none}ul.lst-kix_jbicnccaji3d-2{list-style-type:none}ul.lst-kix_7n1btf6e2iyv-4{list-style-type:none}ul.lst-kix_jbicnccaji3d-3{list-style-type:none}ul.lst-kix_7n1btf6e2iyv-8{list-style-type:none}ul.lst-kix_jbicnccaji3d-8{list-style-type:none}.lst-kix_3bekfbfd6o8j-2>li:before{content:"\0025a0  "}ul.lst-kix_jbicnccaji3d-4{list-style-type:none}.lst-kix_lqb82f5hzmh3-8>li:before{content:"\0025a0  "}ul.lst-kix_jbicnccaji3d-5{list-style-type:none}ul.lst-kix_jbicnccaji3d-6{list-style-type:none}.lst-kix_47504ojh0ba1-6>li:before{content:"\0025cf  "}ul.lst-kix_jbicnccaji3d-7{list-style-type:none}.lst-kix_c4453ihcjob0-6>li:before{content:"\0025cf  "}.lst-kix_c4453ihcjob0-2>li:before{content:"\0025a0  "}.lst-kix_rj34zh1l62l0-6>li:before{content:"\0025cf  "}.lst-kix_5gii2tr4ae90-1>li:before{content:"\0025cb  "}.lst-kix_vd8i8whhh56x-7>li:before{content:"-  "}.lst-kix_9i19e8txouig-1>li:before{content:"\0025cb  "}.lst-kix_rj34zh1l62l0-2>li:before{content:"\0025a0  "}ol.lst-kix_7pcs9kbg0qhi-8.start{counter-reset:lst-ctn-kix_7pcs9kbg0qhi-8 0}.lst-kix_5gii2tr4ae90-5>li:before{content:"\0025a0  "}.lst-kix_5iyzxwlmxp1f-7>li:before{content:"\0025cb  "}.lst-kix_6shy0i3drli2-3>li:before{content:"-  "}.lst-kix_6shy0i3drli2-7>li:before{content:"-  "}.lst-kix_7pcs9kbg0qhi-8>li{counter-increment:lst-ctn-kix_7pcs9kbg0qhi-8}.lst-kix_5iyzxwlmxp1f-3>li:before{content:"\0025cf  "}.lst-kix_38adif7m6rbx-1>li:before{content:"\0025cb  "}.lst-kix_owgllipbp8j5-3>li:before{content:"\0025cf  "}.lst-kix_vd8i8whhh56x-3>li:before{content:"-  "}ol.lst-kix_8dw3ycrgt362-4.start{counter-reset:lst-ctn-kix_8dw3ycrgt362-4 0}.lst-kix_7pcs9kbg0qhi-7>li:before{content:"" counter(lst-ctn-kix_7pcs9kbg0qhi-7,lower-latin) ". "}.lst-kix_7pcs9kbg0qhi-8>li:before{content:"" counter(lst-ctn-kix_7pcs9kbg0qhi-8,lower-roman) ". "}.lst-kix_7pcs9kbg0qhi-2>li:before{content:"" counter(lst-ctn-kix_7pcs9kbg0qhi-2,lower-roman) ". "}ol.lst-kix_7pcs9kbg0qhi-3.start{counter-reset:lst-ctn-kix_7pcs9kbg0qhi-3 0}.lst-kix_7pcs9kbg0qhi-5>li:before{content:"" counter(lst-ctn-kix_7pcs9kbg0qhi-5,lower-roman) ". "}.lst-kix_9sdxipg1h5wr-6>li:before{content:"\0025cf  "}.lst-kix_9sdxipg1h5wr-3>li:before{content:"\0025cf  "}.lst-kix_9sdxipg1h5wr-4>li:before{content:"\0025cb  "}.lst-kix_9sdxipg1h5wr-1>li:before{content:"\0025cb  "}.lst-kix_3bekfbfd6o8j-6>li:before{content:"\0025cf  "}.lst-kix_3bekfbfd6o8j-8>li:before{content:"\0025a0  "}.lst-kix_hait203o5dqo-5>li:before{content:"-  "}.lst-kix_hait203o5dqo-3>li:before{content:"-  "}.lst-kix_7pcs9kbg0qhi-6>li{counter-increment:lst-ctn-kix_7pcs9kbg0qhi-6}.lst-kix_hait203o5dqo-0>li:before{content:"-  "}.lst-kix_7pcs9kbg0qhi-0>li:before{content:"" counter(lst-ctn-kix_7pcs9kbg0qhi-0,decimal) ". "}.lst-kix_2aliey961vrg-1>li:before{content:"\0025cb  "}.lst-kix_2aliey961vrg-7>li:before{content:"\0025cb  "}.lst-kix_6wlb53t5sfma-4>li:before{content:"\0025cb  "}.lst-kix_6wlb53t5sfma-6>li:before{content:"\0025cf  "}.lst-kix_lqb82f5hzmh3-5>li:before{content:"\0025a0  "}.lst-kix_38adif7m6rbx-4>li:before{content:"\0025cb  "}.lst-kix_9i19e8txouig-8>li:before{content:"\0025a0  "}.lst-kix_hait203o5dqo-6>li:before{content:"-  "}.lst-kix_e8kaqrotva4-5>li:before{content:"\0025a0  "}.lst-kix_hait203o5dqo-8>li:before{content:"-  "}.lst-kix_9i19e8txouig-6>li:before{content:"\0025cf  "}.lst-kix_3bekfbfd6o8j-5>li:before{content:"\0025a0  "}.lst-kix_lqb82f5hzmh3-7>li:before{content:"\0025cb  "}.lst-kix_3bekfbfd6o8j-3>li:before{content:"\0025cf  "}.lst-kix_w93gux8gcekz-2>li:before{content:"-  "}.lst-kix_c4453ihcjob0-5>li:before{content:"\0025a0  "}.lst-kix_r60cdwx1dlic-1>li:before{content:"\0025cb  "}.lst-kix_c4453ihcjob0-3>li:before{content:"\0025cf  "}.lst-kix_9uzslqp6q29w-6>li:before{content:"\0025cf  "}.lst-kix_9i19e8txouig-0>li:before{content:"\0025cf  "}.lst-kix_mvnr2hoa94kd-6>li:before{content:"\0025cf  "}.lst-kix_w93gux8gcekz-4>li:before{content:"-  "}.lst-kix_e8kaqrotva4-3>li:before{content:"\0025cf  "}ol.lst-kix_7pcs9kbg0qhi-1.start{counter-reset:lst-ctn-kix_7pcs9kbg0qhi-1 0}.lst-kix_6shy0i3drli2-2>li:before{content:"-  "}.lst-kix_mvnr2hoa94kd-8>li:before{content:"\0025a0  "}.lst-kix_6shy0i3drli2-8>li:before{content:"-  "}.lst-kix_6shy0i3drli2-0>li:before{content:"-  "}.lst-kix_38adif7m6rbx-2>li:before{content:"\0025a0  "}ol.lst-kix_8dw3ycrgt362-6.start{counter-reset:lst-ctn-kix_8dw3ycrgt362-6 0}.lst-kix_r60cdwx1dlic-7>li:before{content:"\0025cb  "}.lst-kix_owgllipbp8j5-2>li:before{content:"\0025a0  "}.lst-kix_5s83gilt2o7g-8>li:before{content:"\0025a0  "}ol.lst-kix_7pcs9kbg0qhi-0.start{counter-reset:lst-ctn-kix_7pcs9kbg0qhi-0 0}.lst-kix_owgllipbp8j5-5>li:before{content:"\0025a0  "}.lst-kix_owgllipbp8j5-8>li:before{content:"\0025a0  "}.lst-kix_5we8o0btcr67-4>li:before{content:"\0025cb  "}.lst-kix_hm0ybxxbxp2l-8>li:before{content:"\0025a0  "}.lst-kix_msukp0gxz1c7-8>li:before{content:"\0025a0  "}.lst-kix_g3mw20emt8ab-1>li:before{content:"\0025cb  "}.lst-kix_5we8o0btcr67-1>li:before{content:"\0025cb  "}.lst-kix_g3mw20emt8ab-4>li:before{content:"\0025cb  "}ul.lst-kix_9uzslqp6q29w-0{list-style-type:none}.lst-kix_hm0ybxxbxp2l-0>li:before{content:"\0025cf  "}ul.lst-kix_9uzslqp6q29w-1{list-style-type:none}.lst-kix_9uzslqp6q29w-1>li:before{content:"\0025cb  "}.lst-kix_mvnr2hoa94kd-3>li:before{content:"\0025cf  "}.lst-kix_mvnr2hoa94kd-0>li:before{content:"\0025cf  "}.lst-kix_hm0ybxxbxp2l-5>li:before{content:"\0025a0  "}.lst-kix_rop5sudr37z1-0>li:before{content:"\0025cf  "}ol.lst-kix_8dw3ycrgt362-1.start{counter-reset:lst-ctn-kix_8dw3ycrgt362-1 0}ul.lst-kix_e8kaqrotva4-7{list-style-type:none}ul.lst-kix_e8kaqrotva4-8{list-style-type:none}ul.lst-kix_e8kaqrotva4-5{list-style-type:none}ul.lst-kix_e8kaqrotva4-6{list-style-type:none}ul.lst-kix_e8kaqrotva4-3{list-style-type:none}ul.lst-kix_e8kaqrotva4-4{list-style-type:none}ul.lst-kix_e8kaqrotva4-1{list-style-type:none}ul.lst-kix_e8kaqrotva4-2{list-style-type:none}.lst-kix_p3wa2k83s8cn-1>li:before{content:"\0025cb  "}ul.lst-kix_e8kaqrotva4-0{list-style-type:none}.lst-kix_jbicnccaji3d-1>li:before{content:"\0025cb  "}.lst-kix_p3wa2k83s8cn-6>li:before{content:"\0025cf  "}.lst-kix_rsr7bmzen4a7-5>li:before{content:"\0025a0  "}.lst-kix_8dw3ycrgt362-7>li{counter-increment:lst-ctn-kix_8dw3ycrgt362-7}.lst-kix_rop5sudr37z1-3>li:before{content:"\0025cf  "}ul.lst-kix_9uzslqp6q29w-8{list-style-type:none}ol.lst-kix_8dw3ycrgt362-2.start{counter-reset:lst-ctn-kix_8dw3ycrgt362-2 0}ul.lst-kix_9uzslqp6q29w-6{list-style-type:none}ul.lst-kix_9uzslqp6q29w-7{list-style-type:none}ul.lst-kix_9uzslqp6q29w-4{list-style-type:none}ul.lst-kix_9uzslqp6q29w-5{list-style-type:none}ul.lst-kix_9uzslqp6q29w-2{list-style-type:none}.lst-kix_jbicnccaji3d-6>li:before{content:"\0025cf  "}ul.lst-kix_9uzslqp6q29w-3{list-style-type:none}.lst-kix_rop5sudr37z1-8>li:before{content:"\0025a0  "}ul.lst-kix_vd8i8whhh56x-7{list-style-type:none}ul.lst-kix_vd8i8whhh56x-6{list-style-type:none}.lst-kix_msukp0gxz1c7-0>li:before{content:"\0025cf  "}ul.lst-kix_vd8i8whhh56x-5{list-style-type:none}ul.lst-kix_vd8i8whhh56x-4{list-style-type:none}ul.lst-kix_vd8i8whhh56x-8{list-style-type:none}.lst-kix_3sh9r2c1w16g-5>li:before{content:"\0025a0  "}.lst-kix_8d6owvwxykff-2>li:before{content:"\0025a0  "}.lst-kix_rsr7bmzen4a7-2>li:before{content:"\0025a0  "}ul.lst-kix_2aliey961vrg-7{list-style-type:none}ul.lst-kix_2aliey961vrg-6{list-style-type:none}.lst-kix_3sh9r2c1w16g-0>li:before{content:"\0025cf  "}ul.lst-kix_2aliey961vrg-8{list-style-type:none}.lst-kix_msukp0gxz1c7-5>li:before{content:"\0025a0  "}ul.lst-kix_2aliey961vrg-3{list-style-type:none}ul.lst-kix_2aliey961vrg-2{list-style-type:none}ul.lst-kix_2aliey961vrg-5{list-style-type:none}ul.lst-kix_2aliey961vrg-4{list-style-type:none}ul.lst-kix_2aliey961vrg-1{list-style-type:none}ul.lst-kix_2aliey961vrg-0{list-style-type:none}.lst-kix_8d6owvwxykff-7>li:before{content:"\0025cb  "}ul.lst-kix_vd8i8whhh56x-3{list-style-type:none}ul.lst-kix_vd8i8whhh56x-2{list-style-type:none}ul.lst-kix_vd8i8whhh56x-1{list-style-type:none}ul.lst-kix_vd8i8whhh56x-0{list-style-type:none}.lst-kix_lqb82f5hzmh3-2>li:before{content:"\0025a0  "}.lst-kix_6wlb53t5sfma-1>li:before{content:"\0025cb  "}ul.lst-kix_mvnr2hoa94kd-8{list-style-type:none}.lst-kix_ao97xhpq4hr9-6>li:before{content:"\0025cf  "}.lst-kix_2aliey961vrg-4>li:before{content:"\0025cb  "}.lst-kix_9reixuath9e3-1>li:before{content:"\0025cb  "}.lst-kix_7n1btf6e2iyv-0>li:before{content:"\0025cf  "}ul.lst-kix_mvnr2hoa94kd-0{list-style-type:none}ul.lst-kix_mvnr2hoa94kd-1{list-style-type:none}ul.lst-kix_mvnr2hoa94kd-2{list-style-type:none}.lst-kix_7pcs9kbg0qhi-2>li{counter-increment:lst-ctn-kix_7pcs9kbg0qhi-2}ul.lst-kix_mvnr2hoa94kd-3{list-style-type:none}ul.lst-kix_mvnr2hoa94kd-4{list-style-type:none}ul.lst-kix_mvnr2hoa94kd-5{list-style-type:none}ul.lst-kix_mvnr2hoa94kd-6{list-style-type:none}ul.lst-kix_mvnr2hoa94kd-7{list-style-type:none}.lst-kix_38adif7m6rbx-7>li:before{content:"\0025cb  "}.lst-kix_3z0hr7pzpec4-8>li:before{content:"\0025a0  "}.lst-kix_7n1btf6e2iyv-8>li:before{content:"\0025a0  "}.lst-kix_mfwegjdddisb-4>li:before{content:"\0025cb  "}.lst-kix_3sh9r2c1w16g-8>li:before{content:"\0025a0  "}.lst-kix_3bekfbfd6o8j-0>li:before{content:"\0025cf  "}.lst-kix_e8kaqrotva4-8>li:before{content:"\0025a0  "}.lst-kix_47504ojh0ba1-4>li:before{content:"\0025cb  "}.lst-kix_s4rut6kfjze3-6>li:before{content:"\0025cf  "}.lst-kix_c4453ihcjob0-0>li:before{content:"\0025cf  "}ul.lst-kix_3bekfbfd6o8j-7{list-style-type:none}ul.lst-kix_3bekfbfd6o8j-8{list-style-type:none}ul.lst-kix_3bekfbfd6o8j-5{list-style-type:none}ul.lst-kix_3bekfbfd6o8j-6{list-style-type:none}ul.lst-kix_3bekfbfd6o8j-3{list-style-type:none}.lst-kix_dnm4ke3nhj1e-4>li:before{content:"\0025cb  "}ul.lst-kix_3bekfbfd6o8j-4{list-style-type:none}ul.lst-kix_3bekfbfd6o8j-1{list-style-type:none}ul.lst-kix_3bekfbfd6o8j-2{list-style-type:none}ul.lst-kix_ao97xhpq4hr9-0{list-style-type:none}.lst-kix_r60cdwx1dlic-4>li:before{content:"\0025cb  "}ul.lst-kix_3bekfbfd6o8j-0{list-style-type:none}.lst-kix_vd8i8whhh56x-5>li:before{content:"-  "}ul.lst-kix_ao97xhpq4hr9-2{list-style-type:none}ul.lst-kix_ao97xhpq4hr9-1{list-style-type:none}ul.lst-kix_ao97xhpq4hr9-4{list-style-type:none}.lst-kix_w93gux8gcekz-7>li:before{content:"-  "}ul.lst-kix_ao97xhpq4hr9-3{list-style-type:none}ul.lst-kix_ao97xhpq4hr9-6{list-style-type:none}ul.lst-kix_ao97xhpq4hr9-5{list-style-type:none}ul.lst-kix_ao97xhpq4hr9-8{list-style-type:none}ul.lst-kix_ao97xhpq4hr9-7{list-style-type:none}.lst-kix_e8kaqrotva4-0>li:before{content:"\0025cf  "}.lst-kix_5gii2tr4ae90-3>li:before{content:"\0025cf  "}.lst-kix_rj34zh1l62l0-4>li:before{content:"\0025cb  "}.lst-kix_9i19e8txouig-3>li:before{content:"\0025cf  "}.lst-kix_c4453ihcjob0-8>li:before{content:"\0025a0  "}.lst-kix_6shy0i3drli2-5>li:before{content:"-  "}.lst-kix_8dw3ycrgt362-2>li:before{content:"" counter(lst-ctn-kix_8dw3ycrgt362-2,lower-roman) ". "}.lst-kix_5iyzxwlmxp1f-1>li:before{content:"\0025cb  "}.lst-kix_5s83gilt2o7g-0>li:before{content:"\0025cf  "}.lst-kix_ne2kodfi1kix-4>li:before{content:"\0025cb  "}ol.lst-kix_8dw3ycrgt362-0.start{counter-reset:lst-ctn-kix_8dw3ycrgt362-0 0}.lst-kix_3z0hr7pzpec4-0>li:before{content:"\0025cf  "}ol{margin:0;padding:0}table td,table th{padding:0}.c37{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:middle;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;background-color:#cfe2f3;border-left-style:solid;border-bottom-width:1pt;width:276pt;border-top-color:#000000;border-bottom-style:solid}.c77{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:161.2pt;border-top-color:#000000;border-bottom-style:solid}.c71{border-right-style:solid;padding:8.5pt 8.5pt 8.5pt 8.5pt;border-bottom-color:#d9d9d9;border-top-width:1pt;border-right-width:1pt;border-left-color:#d9d9d9;vertical-align:top;border-right-color:#d9d9d9;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:330pt;border-top-color:#d9d9d9;border-bottom-style:solid}.c79{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:213pt;border-top-color:#000000;border-bottom-style:solid}.c112{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:239.2pt;border-top-color:#000000;border-bottom-style:solid}.c15{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:bottom;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:239.2pt;border-top-color:#000000;border-bottom-style:solid}.c82{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#ffffff;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:170.1pt;border-top-color:#000000;border-bottom-style:solid}.c60{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:170.1pt;border-top-color:#ffffff;border-bottom-style:solid}.c65{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:middle;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:135pt;border-top-color:#000000;border-bottom-style:solid}.c6{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:bottom;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:276pt;border-top-color:#000000;border-bottom-style:solid}.c30{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:bottom;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:510.8pt;border-top-color:#000000;border-bottom-style:solid}.c74{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:middle;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:161.2pt;border-top-color:#000000;border-bottom-style:solid}.c59{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:bottom;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:510pt;border-top-color:#000000;border-bottom-style:solid}.c40{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:510.2pt;border-top-color:#000000;border-bottom-style:solid}.c98{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:middle;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:149.2pt;border-top-color:#000000;border-bottom-style:solid}.c31{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:bottom;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:122.2pt;border-top-color:#000000;border-bottom-style:solid}.c41{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:bottom;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:149.2pt;border-top-color:#000000;border-bottom-style:solid}.c51{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:middle;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:213pt;border-top-color:#000000;border-bottom-style:solid}.c96{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:middle;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:122.2pt;border-top-color:#000000;border-bottom-style:solid}.c85{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:135pt;border-top-color:#000000;border-bottom-style:solid}.c94{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:middle;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:239.2pt;border-top-color:#000000;border-bottom-style:solid}.c47{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:98.2pt;border-top-color:#000000;border-bottom-style:solid}.c108{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:middle;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:105.8pt;border-top-color:#000000;border-bottom-style:solid}.c4{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:bottom;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:105.8pt;border-top-color:#000000;border-bottom-style:solid}.c16{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:bottom;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:128.2pt;border-top-color:#000000;border-bottom-style:solid}.c57{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:114pt;border-top-color:#000000;border-bottom-style:solid}.c24{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:170.1pt;border-top-color:#000000;border-bottom-style:solid}.c97{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:middle;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:128.2pt;border-top-color:#000000;border-bottom-style:solid}.c92{padding-top:6pt;border-top-width:1pt;border-bottom-color:#0b5394;padding-bottom:6pt;line-height:1.0;border-top-style:solid;background-color:#e7f3fd;text-indent:10.1pt;border-bottom-width:1pt;border-top-color:#0b5394;border-bottom-style:solid;text-align:left}.c38{-webkit-text-decoration-skip:none;color:#666666;font-weight:400;text-decoration:underline;vertical-align:baseline;text-decoration-skip-ink:none;font-size:12pt;font-family:"Trebuchet MS";font-style:normal}.c39{padding-top:8pt;border-bottom-color:#3c78d8;border-bottom-width:1pt;padding-bottom:0pt;line-height:1.0;border-bottom-style:solid;page-break-after:avoid;text-align:left}.c19{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c62{color:#666666;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Georgia";font-style:italic}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c8{color:#45818e;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Trebuchet MS";font-style:normal}.c10{color:#3c78d8;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Trebuchet MS";font-style:normal}.c5{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Arial";font-style:normal}.c14{margin-left:36pt;padding-top:3pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c1{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Inconsolata";font-style:normal}.c21{padding-top:0pt;padding-bottom:0pt;line-height:1.4285714285714286;orphans:2;widows:2;text-align:left}.c2{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c46{padding-top:6pt;padding-bottom:3pt;line-height:1.0;page-break-after:avoid;text-align:left}.c33{margin-left:18pt;padding-top:3pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c27{color:#000000;text-decoration:none;vertical-align:baseline;font-size:8pt;font-style:normal}.c35{color:#24292e;text-decoration:none;vertical-align:baseline;font-size:11pt;font-style:normal}.c68{margin-left:36pt;padding-top:3pt;padding-bottom:4pt;line-height:1.0;text-align:left}.c48{margin-left:54pt;padding-top:3pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c26{color:#000000;text-decoration:none;vertical-align:baseline;font-size:11pt;font-style:normal}.c43{background-color:#ffffff;color:#24292e;text-decoration:none;vertical-align:baseline;font-style:normal}.c22{font-weight:400;vertical-align:baseline;font-size:10pt;font-family:"Arial";font-style:normal}.c29{color:#000000;text-decoration:none;vertical-align:baseline;font-size:10pt;font-style:normal}.c109{padding-top:4pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c44{background-color:#ffffff;font-size:10pt;font-style:italic;color:#3e4349}.c102{margin-left:-1.5pt;border-spacing:0;border-collapse:collapse;margin-right:auto}.c52{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:center}.c55{border-spacing:0;border-collapse:collapse;margin-right:auto}.c11{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c32{color:#000000;text-decoration:none;vertical-align:baseline;font-style:normal}.c70{font-weight:400;vertical-align:baseline;font-family:"Arial";font-style:normal}.c3{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c72{padding-top:10pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c88{margin-left:auto;border-spacing:0;border-collapse:collapse;margin-right:auto}.c18{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c20{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:center}.c63{background-color:#ecf0f3;font-style:italic;color:#222222}.c107{color:#1c4587;font-size:30pt;font-family:"Trebuchet MS"}.c61{text-decoration:none;vertical-align:baseline;font-style:normal}.c23{font-weight:400;font-family:"Consolas"}.c114{max-width:510.2pt;padding:21.3pt 42.5pt 21.3pt 42.5pt}.c99{margin-left:108pt;padding-left:0pt}.c64{padding:0;margin:0}.c13{font-weight:400;font-family:"Inconsolata"}.c25{orphans:2;widows:2}.c42{margin-left:36pt;padding-left:0pt}.c56{font-family:"Courier New";font-weight:400}.c49{border:1px solid black;margin:5px}.c9{color:inherit;text-decoration:inherit}.c100{font-size:18pt;font-family:"Trebuchet MS"}.c53{margin-left:72pt;padding-left:0pt}.c95{font-weight:400}.c66{color:#1155cc}.c78{font-style:italic}.c90{page-break-after:avoid}.c45{height:33pt}.c91{color:#032f62}.c58{font-size:10pt}.c104{vertical-align:baseline}.c83{font-family:"Arial"}.c110{color:#d73a49}.c87{color:#ff0000}.c76{height:21pt}.c93{height:44pt}.c106{font-size:12pt}.c54{color:#24292e}.c86{height:12pt}.c103{height:116.2pt}.c113{font-family:"Georgia"}.c80{color:#000000}.c7{font-size:9pt}.c75{text-decoration:none}.c69{font-size:11pt}.c36{font-weight:700}.c115{height:22pt}.c105{height:26pt}.c17{height:11pt}.c81{height:54pt}.c73{background-color:#ffffff}.c12{height:15pt}.c101{height:19pt}.c34{height:24pt}.c84{height:16pt}.c28{height:0pt}.c67{color:#0000ff}.c89{height:14pt}.c111{background-color:#d9d9d9}.c50{background-color:#cfe2f3}.title{padding-top:0pt;color:#1c4587;font-weight:700;font-size:30pt;padding-bottom:0pt;font-family:"Trebuchet MS";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:11pt;padding-bottom:0pt;font-family:"Georgia";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:6pt;color:#1155cc;border-top-width:1pt;border-bottom-color:#0b5394;font-weight:700;font-size:18pt;padding-bottom:6pt;line-height:1.0;page-break-after:avoid;border-top-style:solid;background-color:#e7f3fd;border-bottom-width:1pt;border-top-color:#0b5394;font-family:"Trebuchet MS";border-bottom-style:solid;orphans:2;widows:2;text-align:left}h2{padding-top:8pt;color:#3c78d8;border-bottom-color:#3c78d8;font-weight:700;font-size:16pt;padding-bottom:0pt;line-height:1.0;page-break-after:avoid;border-bottom-width:1pt;font-family:"Trebuchet MS";border-bottom-style:solid;orphans:2;widows:2;text-align:left}h3{padding-top:6pt;color:#45818e;font-weight:700;font-size:14pt;padding-bottom:3pt;font-family:"Trebuchet MS";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:6pt;-webkit-text-decoration-skip:none;color:#666666;text-decoration:underline;font-size:12pt;padding-bottom:3pt;line-height:1.0;page-break-after:avoid;text-decoration-skip-ink:none;font-family:"Trebuchet MS";orphans:2;widows:2;text-align:left}h5{padding-top:6pt;color:#777777;font-weight:700;font-size:11pt;padding-bottom:3pt;font-family:"Trebuchet MS";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:6pt;color:#666666;font-size:11pt;padding-bottom:3pt;font-family:"Trebuchet MS";line-height:1.0;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c73 c114"><p class="c2 c90 title" id="h.tc504ybtnf8q"><span class="c61 c36 c107">Framework Migration Guide</span></p><p class="c2 c90 subtitle" id="h.4pdauswx7nci"><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://chainer.github.io/migration-guide/&amp;sa=D&amp;source=editors&amp;ust=1615451627237000&amp;usg=AOvVaw2Y-vcC80535SgXNvjYNyNN">https://chainer.github.io/migration-guide/</a></span></p><p class="c2 c90 subtitle" id="h.4ooc9p4a0bty"><span>Authors: Chainer Team</span></p><p class="c2 c17"><span class="c0"></span></p><a id="t.b6c3151b065c41d1a077ec06541d8e5348b9d199"></a><a id="t.0"></a><table class="c55"><tbody><tr class="c28"><td class="c71" colspan="1" rowspan="1"><p class="c18 c17"><span class="c22 c80 c75"></span></p><p class="c25 c109"><span class="c22 c11"><a class="c9" href="#h.6qt1ntczqwrx">General Information</a></span></p><p class="c25 c33"><span class="c22 c11"><a class="c9" href="#h.r2o0yn30s8tq">Concepts and components in both frameworks</a></span></p><p class="c14"><span class="c22 c11"><a class="c9" href="#h.ms7vvkoz6vru">Array Library</a></span></p><p class="c14"><span class="c22 c11"><a class="c9" href="#h.8z53kzu8pby0">Core Framework and Training Loop</a></span></p><p class="c33 c25"><span class="c22 c11"><a class="c9" href="#h.y7ovx8en8cp7">Migration scenarios</a></span></p><p class="c14"><span class="c22 c11"><a class="c9" href="#h.e88d7eicrcd0">I want to port my Chainer script to PyTorch, step by step</a></span></p><p class="c14"><span class="c22 c11"><a class="c9" href="#h.8w1snpmhh4kj">I want to let my Chainer code train a PyTorch model</a></span></p><p class="c33 c25"><span class="c22 c11"><a class="c9" href="#h.9wc1iaeyqb2c">Migration tools (cpm)</a></span></p><p class="c14"><span class="c22 c11"><a class="c9" href="#h.im0z6zf5ujzw">cpm.TorchModule</a></span></p><p class="c14"><span class="c22 c11"><a class="c9" href="#h.pflkcrjo8y89">cpm.ChainerParameter</a></span></p><p class="c14"><span class="c22 c11"><a class="c9" href="#h.46et3pwf65s">cpm.LinkAsTorchModel</a></span></p><p class="c14"><span class="c22 c11"><a class="c9" href="#h.49ptm66ugzcy">cpm.ignite.add_trainer_extension</a></span></p><p class="c14"><span class="c22 c11"><a class="c9" href="#h.207juqqyebo8">cpm.use_torch_in_cupy_malloc</a></span></p><p class="c25 c72"><span class="c22 c11"><a class="c9" href="#h.9eib52bt6ke5">Porting Guides</a></span></p><p class="c33 c25"><span class="c22 c11"><a class="c9" href="#h.7u4t6laltff5">Dataset and data pre/post-processing</a></span></p><p class="c14"><span class="c22 c11"><a class="c9" href="#h.tw9winyigiz6">Negative strides</a></span></p><p class="c14"><span class="c11 c58"><a class="c9" href="#h.iu5zwqbulp2u">Rewriting Custom Converter Functions to collate_fn</a></span></p><p class="c14"><span class="c11 c58"><a class="c9" href="#h.5zvn9j4xwu6j">NumPy bridge</a></span></p><p class="c14"><span class="c22 c11"><a class="c9" href="#h.84jg73vglm0b">CuPy bridge</a></span></p><p class="c14"><span class="c11 c58"><a class="c9" href="#h.uohx43j9p18u">Difference between PyTorch and NumPy/CuPy</a></span></p><p class="c48 c25"><span class="c11 c58"><a class="c9" href="#h.6qe1h751g6oj">Division Behavior</a></span></p><p class="c48 c25"><span class="c11 c58"><a class="c9" href="#h.br8vew20r7k">Feature Mapping</a></span></p><p class="c33 c25"><span class="c22 c11"><a class="c9" href="#h.h1ei3avajrbn">Training loop</a></span></p><p class="c33 c25"><span class="c22 c11"><a class="c9" href="#h.3b2g5rapv9ec">Evaluation loop</a></span></p><p class="c33 c25"><span class="c22 c11"><a class="c9" href="#h.wanb8rb4d6lo">Training and evaluation using Ignite</a></span></p><p class="c33 c25"><span class="c22 c11"><a class="c9" href="#h.m8sz4mg7ioxl">Using Chainer extensions with Ignite</a></span></p><p class="c14"><span class="c22 c11"><a class="c9" href="#h.ojt9418jpkms">Snapshots</a></span></p><p class="c33 c25"><span class="c22 c11"><a class="c9" href="#h.s8lyt5panb7">Porting custom updater using Ignite</a></span></p><p class="c33 c25"><span class="c22 c11"><a class="c9" href="#h.xmsyb5asd2c9">Rewriting existing Chainer model</a></span></p><p class="c33 c25"><span class="c22 c11"><a class="c9" href="#h.mkuuagm60br0">Functions and Links</a></span></p><p class="c14"><span class="c22 c11"><a class="c9" href="#h.e17stx2v9ds3">Functions</a></span></p><p class="c14"><span class="c22 c11"><a class="c9" href="#h.ryz2089jygqa">Links</a></span></p><p class="c33 c25"><span class="c22 c11"><a class="c9" href="#h.hb9kqa7i3enr">Configuration</a></span></p><p class="c33 c25"><span class="c22 c11"><a class="c9" href="#h.rxt9pteccajv">Hooks</a></span></p><p class="c14"><span class="c22 c11"><a class="c9" href="#h.gsgtoxixjcm4">Function Hooks</a></span></p><p class="c14"><span class="c22 c11"><a class="c9" href="#h.rvlhszhkp1bw">Link Hooks</a></span></p><p class="c14"><span class="c22 c11"><a class="c9" href="#h.5a204an0cllk">Optimizer Hooks</a></span></p><p class="c33 c25"><span class="c22 c11"><a class="c9" href="#h.79n4spowwnun">Training PyTorch model using Chainer</a></span></p><p class="c33 c25"><span class="c22 c11"><a class="c9" href="#h.33jrfcvcf9zh">Distributed training</a></span></p><p class="c14"><span class="c22 c11"><a class="c9" href="#h.iyhfmfndl13j">Pytorch model using torch.distributed</a></span></p><p class="c48 c25"><span class="c22 c11"><a class="c9" href="#h.moliknyk6ru3">Invocation</a></span></p><p class="c48 c25"><span class="c22 c11"><a class="c9" href="#h.7r5y5xoqrywb">Initialization</a></span></p><p class="c48 c25"><span class="c22 c11"><a class="c9" href="#h.nzh1eoy3bny9">Dataset scattering</a></span></p><p class="c48 c25"><span class="c22 c11"><a class="c9" href="#h.kyzvorr2x2ij">Data transfer to devices</a></span></p><p class="c48 c25"><span class="c22 c11"><a class="c9" href="#h.s0hd5ycxwpdh">Optimizer wrapping</a></span></p><p class="c48 c25"><span class="c22 c11"><a class="c9" href="#h.d1nulkakooh">Initial values broadcast</a></span></p><p class="c48 c25"><span class="c22 c11"><a class="c9" href="#h.x5dny5mqq9px">Metrics average and reductions</a></span></p><p class="c48 c25"><span class="c22 c11"><a class="c9" href="#h.5t9pfndndqnr">https://pytorch.org/docs/stable/distributed.html#multi-gpu-collective-functions</a></span></p><p class="c48 c25"><span class="c22 c11"><a class="c9" href="#h.3lb7ncfjheoo">Synchronization</a></span></p><p class="c14"><span class="c22 c11"><a class="c9" href="#h.dis62849w9cz">PyTorch model using Horovod</a></span></p><p class="c25 c48"><span class="c22 c11"><a class="c9" href="#h.tje6mmfdx3pf">Horovod initialization</a></span></p><p class="c48 c25"><span class="c22 c11"><a class="c9" href="#h.37g288q2db5i">Dataset scattering</a></span></p><p class="c48 c25"><span class="c22 c11"><a class="c9" href="#h.z7g1nmbbb3uh">Optimizer wrapping</a></span></p><p class="c48 c25"><span class="c22 c11"><a class="c9" href="#h.gq621f9b16zf">Initial values broadcast</a></span></p><p class="c48 c25"><span class="c22 c11"><a class="c9" href="#h.9t9k4lfat6t2">Metrics average and reductions</a></span></p><p class="c48 c25"><span class="c22 c11"><a class="c9" href="#h.bfpbumwh7xzs">Horovod code structure</a></span></p><p class="c48 c25"><span class="c22 c11"><a class="c9" href="#h.apxtaciyccdc">Obtaining Horovod traces to measure performance</a></span></p><p class="c48 c25"><span class="c22 c11"><a class="c9" href="#h.9jn0jqinrgx3">Tuning Horovod performance</a></span></p><p class="c48 c25"><span class="c22 c11"><a class="c9" href="#h.wuax6jpon8sy">Using Horovod with apex</a></span></p><p class="c48 c25"><span class="c11 c58"><a class="c9" href="#h.pdah33gpi4li">Multi-Node Batch Normalization in Horovod</a></span></p><p class="c48 c25"><span class="c11 c58"><a class="c9" href="#h.eqj7dzgbr9y7">Gathering arbitrary objects using Horovod and mpi4py</a></span></p><p class="c48 c25"><span class="c22 c11"><a class="c9" href="#h.y8eom3uzrv56">Alternatives to Horovod</a></span></p><p class="c14"><span class="c22 c11"><a class="c9" href="#h.qfxp9de828j4">Chainer model using Horovod</a></span></p><p class="c14"><span class="c22 c11"><a class="c9" href="#h.vlwpxcx5nw9c">PyTorch model using ChainerMN</a></span></p><p class="c33 c25"><span class="c22 c11"><a class="c9" href="#h.2jl4lfh90jqb">Porting code that edits the computational graph</a></span></p><p class="c14"><span class="c22 c11"><a class="c9" href="#h.gbqjtjbzzzno">Unchaining nodes</a></span></p><p class="c14"><span class="c22 c11"><a class="c9" href="#h.mc69ie7hbl6j">Backprop modes</a></span></p><p class="c14"><span class="c22 c11"><a class="c9" href="#h.bka3yqtw2rpy">Train/Test modes</a></span></p><p class="c72 c25"><span class="c22 c11"><a class="c9" href="#h.r8th8e47f02c">Ecosystem</a></span></p><p class="c14"><span class="c22 c11"><a class="c9" href="#h.d863gr6235hf">PyTorch</a></span></p><p class="c14"><span class="c22 c11"><a class="c9" href="#h.w69092dw2rtl">Ignite</a></span></p><p class="c14"><span class="c22 c11"><a class="c9" href="#h.b2r58wgcvv09">torchvision</a></span></p><p class="c14"><span class="c22 c11"><a class="c9" href="#h.ctcyjos25n6z">torchtext</a></span></p><p class="c14"><span class="c22 c11"><a class="c9" href="#h.l9nxjlcurusl">torchaudio</a></span></p><p class="c14"><span class="c22 c11"><a class="c9" href="#h.p90hj6w1ucn0">Fairseq</a></span></p><p class="c25 c68"><span class="c22 c11"><a class="c9" href="#h.eaofa6s7sf0k">Other</a></span></p><p class="c18 c17"><span class="c22 c80 c75"></span></p></td></tr></tbody></table><p class="c2 c17"><span class="c0"></span></p><p class="c2 c17"><span class="c0"></span></p><p class="c2"><span class="c0">This document provides technical information for migration from Chainer to PyTorch.</span></p><p class="c2 c17"><span class="c0"></span></p><h1 class="c92 c25 c90" id="h.6qt1ntczqwrx"><span class="c61 c100 c66 c36">General Information</span></h1><h2 class="c39 c25" id="h.r2o0yn30s8tq"><span class="c10">Concepts and components in both frameworks</span></h2><h3 class="c46 c25" id="h.ms7vvkoz6vru"><span class="c8">Array Library</span></h3><p class="c2"><span>Chainer uses NumPy/CuPy (</span><span class="c23">xp.ndarray</span><span>) as an array library, and wraps them as </span><span class="c23">chainer.Variable</span><span>&nbsp;to support autograd. Similarly, PyTorch uses </span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/cppdocs/%23aten&amp;sa=D&amp;source=editors&amp;ust=1615451627260000&amp;usg=AOvVaw1_uorIy2vpSPGeyWM5sGcb">ATen</a></span><span>&nbsp;(</span><span class="c23">at::Tensor</span><span>&nbsp;(C++)) as an array library (&quot;tensor library&quot; in PyTorch terms), and wraps it as </span><span class="c23">torch::Tensor</span><span>&nbsp;(C++ API) / </span><span class="c23">torch.Tensor</span><span>&nbsp;(Python API) to support autograd. </span><span class="c23">torch.*</span><span>&nbsp;provides API similar to (but not compatible with) NumPy, e.g. </span><span class="c23">torch.dot, torch.float32</span><span class="c0">, etc.</span></p><h3 class="c46 c25" id="h.8z53kzu8pby0"><span class="c8">Core Framework and Training Loop</span></h3><p class="c2"><span class="c0">As both frameworks share the same concept, define-by-run, the look-and-feel of code written in PyTorch is pretty similar to Chainer. Here is the high-level mapping of features:</span></p><p class="c2 c17"><span class="c0"></span></p><a id="t.e6a46275bcd72c5ee54dae665a0d891476f5e165"></a><a id="t.1"></a><table class="c55"><tbody><tr class="c28"><td class="c24 c50" colspan="1" rowspan="1"><p class="c20"><span class="c19">Chainer</span></p></td><td class="c24 c50" colspan="1" rowspan="1"><p class="c20"><span class="c19">PyTorch</span></p></td><td class="c24 c50" colspan="1" rowspan="1"><p class="c20"><span class="c19">Notes</span></p></td></tr><tr class="c28"><td class="c24" colspan="1" rowspan="1"><p class="c18"><span class="c19">Variable</span></p><p class="c18"><span class="c0">chainer.Variable</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c18"><span class="c19">Tensor</span></p><p class="c18"><span class="c0">torch.Tensor</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c18 c17"><span class="c0"></span></p></td></tr><tr class="c28"><td class="c24" colspan="1" rowspan="1"><p class="c18"><span class="c19">Function</span></p><p class="c18"><span class="c0">chainer.FunctionNode</span></p><p class="c18"><span class="c0">(chainer.functions.*)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c18"><span class="c19">Function</span></p><p class="c18"><span class="c0">torch.autograd.Function</span></p><p class="c18"><span class="c0">(torch.nn.functional.*)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c18"><span class="c0">`torch.*` also provides NumPy-like (but not compatible) operations.</span></p></td></tr><tr class="c28"><td class="c24" colspan="1" rowspan="1"><p class="c18"><span class="c19">Link / Chain</span></p><p class="c18"><span class="c0">chainer.{Link, Chain}</span></p><p class="c18"><span class="c0">(chainer.links.*)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c18"><span class="c19">Module</span></p><p class="c18"><span class="c0">torch.nn.Module</span></p><p class="c18"><span class="c0">(torch.nn.*)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c18 c17"><span class="c0"></span></p></td></tr><tr class="c28"><td class="c24" colspan="1" rowspan="1"><p class="c18"><span class="c19">Sequential</span></p><p class="c18"><span class="c0">chainer.Sequential</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c18"><span class="c19">Sequential</span></p><p class="c18"><span class="c0">torch.nn.Sequential</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c18"><span class="c0">You can use function modules as member (e.g., torch.nn.ReLU instead of torch.nn.functional.relu).</span></p></td></tr><tr class="c28"><td class="c24" colspan="1" rowspan="1"><p class="c18"><span class="c19">Dataset</span></p><p class="c18"><span class="c0">chainer.dataset.DatasetMixin</span></p><p class="c18"><span class="c0">(chainer.datasets.*)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c18"><span class="c19">Dataset</span></p><p class="c18"><span class="c0">torch.utils.data.Dataset</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c18"><span>There are no TransformDataset in PyTorch (there is one in CPM as cpm.TransformDataset); </span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torchvision/datasets.html&amp;sa=D&amp;source=editors&amp;ust=1615451627267000&amp;usg=AOvVaw1zo9xj4BlSNSUO5V_ITCmK">datasets conventionally accepts</a></span><span class="c0">&nbsp;`transforms` argument that perform per-example preprocessing.</span></p></td></tr><tr class="c28"><td class="c24" colspan="1" rowspan="1"><p class="c18"><span class="c19">Iterator</span></p><p class="c18"><span class="c0">chainer.iterators.*</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c18"><span class="c36">DataLoader</span></p><p class="c18"><span class="c0">torch.utils.data.DataLoader</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c18"><span>Unlike Chainer&#39;s Iterator, DataLoader automatically collates all samples into one Tensor by default; use </span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/data.html%23working-with-collate-fn&amp;sa=D&amp;source=editors&amp;ust=1615451627268000&amp;usg=AOvVaw3VVPt7YrSAHsSu4lLQe-Ns">collate_fn</a></span><span class="c0">&nbsp;to customize this behavior.</span></p><p class="c18"><span class="c0">DataLoader itself supports multi-process iteration (using num_workers option).</span></p></td></tr><tr class="c28"><td class="c24" colspan="1" rowspan="1"><p class="c18"><span class="c19">Optimizer</span></p><p class="c18"><span class="c0">chainer.Optimizer</span></p><p class="c18"><span>(</span><span class="c0">chainer.optimizers.*)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c18"><span class="c19">Optimizer</span></p><p class="c18"><span class="c0">torch.optim.Optimizer</span></p><p class="c18"><span>(</span><span class="c0">torch.optim.*)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c18 c17"><span class="c0"></span></p></td></tr><tr class="c76"><td class="c24" colspan="1" rowspan="1"><p class="c18"><span class="c19">Trainer</span></p><p class="c18"><span class="c0">chainer.training.Trainer</span></p></td><td class="c82" colspan="1" rowspan="3"><p class="c18"><span class="c19">Engine</span></p><p class="c18"><span class="c0">ignite.Engine</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c18"><span class="c0">ignite.engine.create_supervised_trainer()</span></p></td></tr><tr class="c76"><td class="c24" colspan="1" rowspan="1"><p class="c18"><span class="c36">Updater</span><span class="c0">&nbsp;(with converter)</span></p><p class="c18"><span class="c0">chainer.training.Updater</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c18"><span class="c0">As noted above, Iterator concatenates examples by default. Transfer to device is handled by Engine (or custom loop code if you don&#39;t use Ignite)</span></p></td></tr><tr class="c76"><td class="c24" colspan="1" rowspan="1"><p class="c18"><span class="c19">Evaluator</span></p><p class="c18"><span class="c0">chainer.training.extensions.Evaluator</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c18"><span class="c0">ignite.engine.create_supervised_evaluator()</span></p></td></tr><tr class="c28"><td class="c24" colspan="1" rowspan="1"><p class="c18"><span class="c19">Extension</span></p><p class="c18"><span class="c0">chainer.training.Extension</span></p><p class="c18"><span class="c0">(chainer.training.extensions.*)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c18"><span class="c19">Handler</span></p><p class="c18"><span class="c0">(ignite.handlers.*, ignite.contrib.handlers.*)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c18 c17"><span class="c0"></span></p></td></tr></tbody></table><p class="c2 c17"><span class="c0"></span></p><p class="c2"><span>Refer to the </span><span class="c11"><a class="c9" href="#h.9eib52bt6ke5">Porting Guide</a></span><span class="c0">&nbsp;section for the details of the difference of each component.</span></p><h2 class="c39 c25" id="h.y7ovx8en8cp7"><span class="c10">Migration scenarios</span></h2><h3 class="c46 c25" id="h.e88d7eicrcd0"><span class="c8">I want to port my Chainer script to PyTorch, step by step</span></h3><p class="c2"><span class="c0">Arguably the model is the hardest part to port without affecting the outcome of the training.</span></p><p class="c2"><span class="c0">It might be easier to port in this order:</span></p><ol class="c64 lst-kix_7pcs9kbg0qhi-0 start" start="1"><li class="c2 c42 li-bullet-0"><span class="c0">Training script (optimizer / updater / evaluator / ...)</span></li></ol><ul class="c64 lst-kix_3bekfbfd6o8j-0 start"><li class="c2 c53 li-bullet-0"><span>In order to use PyTorch optimizer to train a Chainer model, you will need </span><span class="c11 c13"><a class="c9" href="#h.46et3pwf65s">cpm.LinkAsTorchModel</a></span><span class="c0">.</span></li></ul><ol class="c64 lst-kix_7pcs9kbg0qhi-0" start="2"><li class="c2 c42 li-bullet-0"><span class="c0">Dataset / preprocessing</span></li></ol><ul class="c64 lst-kix_rj34zh1l62l0-0 start"><li class="c2 c53 li-bullet-0"><span class="c0">Dataset is in general compatible between Chainer and PyTorch. This part can be delayed but also should be easy to do.</span></li></ul><ol class="c64 lst-kix_7pcs9kbg0qhi-0" start="3"><li class="c2 c42 li-bullet-0"><span class="c0">Model</span></li></ol><ul class="c64 lst-kix_e8kaqrotva4-0 start"><li class="c2 c53 li-bullet-0"><span class="c0">See the mapping of functions/modules below in this document.</span></li></ul><h3 class="c25 c46" id="h.8w1snpmhh4kj"><span class="c8">I want to let my Chainer code train a PyTorch model</span></h3><p class="c2"><span>You can use </span><span class="c11 c13"><a class="c9" href="#h.im0z6zf5ujzw">cpm.TorchModule</a></span><span class="c0">&nbsp;to wrap a PyTorch module as a Chainer model.</span></p><h2 class="c39 c25" id="h.9wc1iaeyqb2c"><span class="c10">Migration tools (CPM)</span></h2><p class="c2"><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://github.com/chainer/chainer-pytorch-migration&amp;sa=D&amp;source=editors&amp;ust=1615451627275000&amp;usg=AOvVaw1NX6JD5kskcM-MBMgdDGBc">chainer-pytorch-migration</a></span><span class="c0">&nbsp;Python module (called CPM or &quot;cpm&quot; (module name) in this document) provides various utilities to help migration from Chainer to PyTorch.</span></p><p class="c2 c17"><span class="c0"></span></p><p class="c2"><span class="c0">Example code assumes that cpm is imported as follows:</span></p><a id="t.b6d4e8a49de865470ffee8db15d9b8ba2e23ba94"></a><a id="t.2"></a><table class="c55"><tbody><tr class="c28"><td class="c40" colspan="1" rowspan="1"><p class="c18"><span class="c29 c13">import chainer_pytorch_migration as cpm</span></p><p class="c18"><span class="c13 c58">import chainer_pytorch_migration.ignite</span></p></td></tr></tbody></table><p class="c2 c17"><span class="c0"></span></p><h3 class="c46 c25" id="h.im0z6zf5ujzw"><span class="c8">cpm.TorchModule</span></h3><p class="c2"><span class="c0">This class wraps a PyTorch module as a Chainer link. It allows training PyTorch models in Chainer training scripts. The graph (forward/backward) must be constructed and traversed in PyTorch.</span></p><a id="t.a38bfd190b7eeeae684f9cee2c1f0d3585c106d8"></a><a id="t.3"></a><table class="c55"><tbody><tr class="c28"><td class="c40" colspan="1" rowspan="1"><p class="c18"><span class="c29 c13">model = torchvision.models.resnet50()</span></p><p class="c18"><span class="c29 c13">model.cuda()</span></p><p class="c18"><span class="c29 c13">w_model = cpm.TorchModule(model)</span></p><p class="c18"><span class="c13 c58">w_model.to_gpu(device) </span><span class="c61 c13 c58 c67"># Just synchronizes the metadata, does not transfer data</span></p></td></tr></tbody></table><p class="c2 c17"><span class="c0"></span></p><h3 class="c46 c25" id="h.pflkcrjo8y89"><span class="c8">cpm.ChainerParameter</span></h3><p class="c2"><span>This class wraps a Chainer parameter as a PyTorch parameter. It allows training of Chainer models (</span><span class="c13">chainer.Link</span><span>) in PyTorch training scripts (with </span><span class="c13">torch.optim.Optimizer</span><span>). The graph (forward/backward) must be constructed and traversed in Chainer. </span><span class="c23">cpm.LinkAsTorchModel</span><span class="c0">&nbsp;internally uses it.</span></p><p class="c2 c17"><span class="c0"></span></p><a id="t.629215085eff3b976e0bfd5a36f2220c2566c72f"></a><a id="t.4"></a><table class="c55"><tbody><tr class="c28"><td class="c40" colspan="1" rowspan="1"><p class="c18"><span class="c61 c13 c58 c67"># initialized parameter</span></p><p class="c18"><span class="c29 c13">arr = numpy.full(shape, 17, &#39;float32&#39;)</span></p><p class="c18"><span class="c29 c13">chainer_param = chainer.Parameter(arr)</span></p><p class="c18"><span class="c13 c58">torch_param = cpm.ChainerParameter(chainer_param)</span></p></td></tr></tbody></table><p class="c2 c17"><span class="c0"></span></p><h3 class="c46 c25" id="h.46et3pwf65s"><span class="c8">cpm.LinkAsTorchModel</span></h3><p class="c2"><span>This class automatically creates all the </span><span class="c13">cpm.ChainerParameter</span><span>&nbsp;objects for a given chainer link and provides methods such as </span><span class="c13">parameters()</span><span>, </span><span class="c13">named_parameters()</span><span>&nbsp;or </span><span class="c13">state_dict() </span><span class="c0">required by pytorch optimizers or tools such as horovod.</span></p><p class="c2 c17"><span class="c0"></span></p><a id="t.7c4d4e63fadd2ec7a99e994e6797d2ceef166dfb"></a><a id="t.5"></a><table class="c55"><tbody><tr class="c28"><td class="c40" colspan="1" rowspan="1"><p class="c18"><span class="c1">model = ChainerModel()</span></p><p class="c18"><span class="c1">model.to_device(ch_device)</span></p><p class="c18"><span class="c61 c13 c69 c67"># Initialize parameters before converting to `ChainerParameter`s.</span></p><p class="c18"><span class="c1">model(ch_device.xp.zeros((1, 784)).astype(&#39;f&#39;))</span></p><p class="c18"><span class="c61 c13 c69 c67"># Convert parameters to `ChainerParameter`s to share memory with PyTorch.</span></p><p class="c18"><span class="c1">torched_model = cpm.LinkAsTorchModel(model)</span></p><p class="c18"><span class="c13">optimizer = optim.SGD(torched_model.parameters(), lr=args.lr)</span></p></td></tr></tbody></table><p class="c2 c17"><span class="c0"></span></p><h3 class="c46 c25" id="h.49ptm66ugzcy"><span class="c8">cpm.ignite.add_trainer_extension</span></h3><p class="c2"><span class="c0">This function registers a chainer trainer extension to be used with ignite.</span></p><p class="c2"><span class="c0">Function call requires the ignite trainer, torch optimizer and the chainer extension as the parameters</span></p><a id="t.03de8277e29c4d3c8e35d4343afc863ae26c47ed"></a><a id="t.6"></a><table class="c55"><tbody><tr class="c28"><td class="c40" colspan="1" rowspan="1"><p class="c2"><span class="c29 c13">optimizer.target = model</span></p><p class="c2"><span class="c29 c13">trainer.out = &#39;path to store extension results&#39;</span></p><p class="c2"><span class="c13 c58">cpm.ignite.add_trainer_extension(trainer, optimizer, extensions.ExponentialShift(&#39;lr&#39;, 0.9, 1.0, 0.1))</span></p></td></tr></tbody></table><p class="c2 c17"><span class="c29 c13"></span></p><h2 class="c39 c25" id="h.kl6y9lj6jzv3"><span class="c10">pytorch-pfn-extras (ppe)</span></h2><p class="c2"><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://github.com/pfnet/pytorch-pfn-extras&amp;sa=D&amp;source=editors&amp;ust=1615451627280000&amp;usg=AOvVaw34lbGynsxqpxAwRZaQdv2r">pytorch-pfn-extras</a></span><span>&nbsp;Python module (called PPE or &quot;ppe&quot; (module name) in this document) provides various supplementary components for PyTorch, including APIs similar to Chainer, e.g. Extensions, Reporter, Lazy modules (automatically infer shapes of parameters). Here are some notable features Refer to the </span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://github.com/pfnet/pytorch-pfn-extras%23documentation&amp;sa=D&amp;source=editors&amp;ust=1615451627280000&amp;usg=AOvVaw00PHHLgribK3gZEp7RWaT6">Documentation</a></span><span class="c0">&nbsp;for the full list of features.</span></p><p class="c2 c17"><span class="c0"></span></p><p class="c2"><span class="c0">PPE also provides the interoperability feature between CuPy and PyTorch memory pool.</span></p><h3 class="c46 c25" id="h.yloxgi30b1es"><span>ppe.cuda.use_torch_mempool_in_cupy</span></h3><p class="c2"><span class="c0">This function makes CuPy use a memory pool from PyTorch. You need to call it before any operations using CuPy.</span></p><a id="t.86dee0b1ccc034af427baa294461f315e730273f"></a><a id="t.7"></a><table class="c55"><tbody><tr class="c28"><td class="c40" colspan="1" rowspan="1"><p class="c18"><span class="c29 c13"># Enable using PyTorch memory allocator in CuPy.</span></p><p class="c18"><span class="c29 c13">ppe.cuda.use_torch_mempool_in_cupy()</span></p><p class="c18 c17"><span class="c29 c13"></span></p><p class="c18"><span class="c29 c13"># Revert back to CuPy&#39;s default memory pool.</span></p><p class="c18"><span class="c29 c13">ppe.cuda.use_default_mempool_in_cupy()</span></p></td></tr></tbody></table><p class="c2 c17"><span class="c0"></span></p><p class="c2"><span>Note: The feature was originally implemented in CPM as </span><span class="c23">cpm.use_torch_in_cupy_malloc</span><span>, but has been moved to PPE. CPM version has been deprecated and not recommended any more.</span></p><h1 class="c92 c25 c90" id="h.9eib52bt6ke5"><span class="c61 c100 c66 c36">Porting Guides</span></h1><h2 class="c39 c25" id="h.7u4t6laltff5"><span class="c10">Dataset and data pre/post-processing</span></h2><p class="c2"><span>PyTorch datasets (</span><span class="c13">pytorch.utils.data.Dataset</span><span class="c0">) are basically compatible with Chainer&rsquo;s. In most cases they are interchangeable in both directions.</span></p><h3 class="c46 c25" id="h.tw9winyigiz6"><span class="c8">Negative strides</span></h3><p class="c2"><span>As of PyTorch 1.2.0, PyTorch cannot handle data arrays with negative strides (can result from </span><span class="c13">numpy.flip</span><span>&nbsp;or </span><span class="c13">chainercv.transforms.flip</span><span class="c0">, for example).</span></p><p class="c2 c17"><span class="c0"></span></p><p class="c2"><span>Perhaps the easiest way to circumvent this problem is to wrap the dataset with </span><span class="c13">numpy.ascontiguousarray</span><span class="c0">.</span></p><p class="c2 c17"><span class="c0"></span></p><a id="t.941bd69b434ad04726fb40c437dd8753bdd82a23"></a><a id="t.8"></a><table class="c55"><tbody><tr class="c28"><td class="c40" colspan="1" rowspan="1"><p class="c2"><span class="c1">def avoid_negative_strides(in_data):</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; data, label = in_data</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; data = numpy.ascontiguousarray(data)</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; return data, label</span></p><p class="c2 c17"><span class="c1"></span></p><p class="c2"><span class="c1">dataset = cpm.TransformDataset(dataset, avoid_negative_strides)</span></p><p class="c2"><span class="c13">data_loader = torch.utils.data.DataLoader(dataset, ...)</span></p></td></tr></tbody></table><p class="c2 c17"><span class="c0"></span></p><p class="c2"><span>Another way is to customize the collation function with </span><span class="c13">collate_fn</span><span>&nbsp;argument in </span><span class="c13">torch.utils.data.DataLoader</span><span class="c0">.</span></p><p class="c2 c17"><span class="c0"></span></p><a id="t.a749c5add683a394ba81c8606178ef82a83ebc58"></a><a id="t.9"></a><table class="c55"><tbody><tr class="c28"><td class="c40" colspan="1" rowspan="1"><p class="c2"><span class="c1">def collate(batch):</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; data = numpy.stack([d for d, l in batch])</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; label = numpy.stack([l for d, l in batch])</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; data_tensor = torch.from_numpy(data)</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; label_tensor = torch.from_numpy(label)</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; return data_tensor, label_tensor</span></p><p class="c2 c17"><span class="c1"></span></p><p class="c2"><span class="c13">data_loader = torch.utils.data.DataLoader(dataset, ..., collate_fn=collate)</span></p></td></tr></tbody></table><p class="c2 c17"><span class="c0"></span></p><h3 class="c46 c25" id="h.iu5zwqbulp2u"><span>Rewriting Custom Converter Functions to </span><span class="c8">collate_fn</span></h3><p class="c2"><span>In Chainer, it&rsquo;s possible to specify custom converters for each batch via `</span><span class="c23">training.updaters.StandardUpdater(train_iter, optimizer, device=device, converter=_converter)</span><span>`. In PyTorch, similar functionality can be achieved via the data loader: `</span><span class="c23">DataLoader(..., collate_fn=_converter)</span><span class="c0">`.</span></p><p class="c2"><span class="c0">There is, however, an important difference when used in conjunction with multiprocessing. In Chainer, `_converter` will be run in the main process, so it&rsquo;s safe to access CUDA in the function when using multiprocessing&rsquo;s `fork` mode. In PyTorch, however, `_converter` will be run inside each forker worker processes of the data loader. This means that we cannot access CUDA without getting a CUDA init error. It seems like in PyTorch, the correct usage is instead to only do CPU-related operations inside `_convert`, and only send the resulting tensors to the GPU *after* retrieving them from the data loader. The following is an example of correct PyTorch usage:</span></p><a id="t.da2a07b197e87147cdca50be1b609a1e175438f1"></a><a id="t.10"></a><table class="c55"><tbody><tr class="c28"><td class="c40" colspan="1" rowspan="1"><p class="c18"><span class="c1">it = DataLoader(..., collate_fn=_converter)</span></p><p class="c18"><span class="c1">for img, label, metadata in it:</span></p><p class="c18"><span class="c1">&nbsp; &nbsp; &nbsp;img = img.cuda()</span></p><p class="c18"><span class="c1">&nbsp; &nbsp; &nbsp;label = label.cuda()</span></p><p class="c18"><span class="c1">&nbsp; &nbsp; &nbsp;# metadata is still on CPU</span></p><p class="c18"><span class="c1">&nbsp; &nbsp; &nbsp;...</span></p></td></tr></tbody></table><p class="c2"><span class="c0">Note that the above scenario is different from what we expect in Chainer, where the `_converter` is called in the main process, which is why Chainer code might have CUDA-related operations inside the `_converter`.</span></p><p class="c2"><span>Note that in the above use case, _convert should also use `pin_memory` in order to speed up the transfer of `(img, label)` from CPU to GPU: </span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://discuss.pytorch.org/t/when-to-set-pin-memory-to-true/19723&amp;sa=D&amp;source=editors&amp;ust=1615451627289000&amp;usg=AOvVaw3E5iXoXCrANpipI4GHgc3T">https://discuss.pytorch.org/t/when-to-set-pin-memory-to-true/19723</a></span></p><h3 class="c46 c25" id="h.5zvn9j4xwu6j"><span class="c8">NumPy bridge</span></h3><p class="c2"><span class="c13">torch.DataLoader</span><span>&nbsp;automatically converts NumPy arrays to PyTorch tensors, but if you want to do that manually, refer to </span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/tutorials/beginner/former_torchies/tensor_tutorial.html%23numpy-bridge&amp;sa=D&amp;source=editors&amp;ust=1615451627289000&amp;usg=AOvVaw2jzOte93cpFwnNZ-uXDtEx">NumPy Bridge</a></span><span>.</span></p><h3 class="c46 c25" id="h.84jg73vglm0b"><span class="c8">CuPy bridge</span></h3><p class="c2"><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://docs-cupy.chainer.org/en/stable/reference/interoperability.html%23dlpack&amp;sa=D&amp;source=editors&amp;ust=1615451627290000&amp;usg=AOvVaw2-ClgARPSWdekPWz-LXqof">DLPack</a></span><span>&nbsp;can be used to bridge between CuPy and </span><span class="c13">torch.Tensor</span><span>. Note that DLPack does not handle ownership, so you have to make sure the original buffer (the original </span><span class="c23">cupy.ndarray</span><span>&nbsp;object or dltensor capsule object returned by </span><span class="c23">toDlpack()</span><span class="c0">) survives while the converted tensor/array is in use.</span></p><p class="c2 c17"><span class="c0"></span></p><p class="c2"><span>If you allocate a memory both in PyTorch and CuPy, it is also recommended to call </span><span class="c11"><a class="c9" href="#h.yloxgi30b1es">ppe.cuda.use_torch_mempool_in_cupy</a></span><span class="c0">&nbsp;before using CuPy to let CuPy use the PyTorch memory pool. Otherwise memories allocated and freed in CuPy will be kept in the CuPy memory pool which cannot be used by PyTorch.</span></p><h3 class="c46 c25" id="h.uohx43j9p18u"><span class="c8">Difference between PyTorch and NumPy/CuPy</span></h3><h4 class="c46 c25" id="h.6qe1h751g6oj"><span class="c38">Division Behavior</span></h4><p class="c2"><span>The behavior is different from NumPy/CuPy, which respects Python 3 division rules. You need to explicitly cast to float in PyTorch (</span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://github.com/pytorch/pytorch/issues/5411&amp;sa=D&amp;source=editors&amp;ust=1615451627291000&amp;usg=AOvVaw22d6oR7MF0xgjcfazSkjau">discussion</a></span><span class="c0">).</span></p><a id="t.e6393095a79238ebe833bf96db1ca9890627bbc4"></a><a id="t.11"></a><table class="c55"><tbody><tr class="c103"><td class="c40" colspan="1" rowspan="1"><p class="c2"><span class="c1">&gt;&gt;&gt; x = numpy.arange(5)</span></p><p class="c2"><span class="c1">&gt;&gt;&gt; x</span></p><p class="c2"><span class="c1">array([0, 1, 2, 3, 4])</span></p><p class="c2"><span class="c61 c13 c87 c69">&gt;&gt;&gt; x / 5</span></p><p class="c2"><span class="c61 c13 c87 c69">array([0. , 0.2, 0.4, 0.6, 0.8])</span></p><p class="c2"><span class="c61 c13 c87 c69">&gt;&gt;&gt; torch.from_numpy(x) / 5</span></p><p class="c2"><span class="c61 c13 c69 c87">tensor([0, 0, 0, 0, 0])</span></p><p class="c2"><span class="c1">&gt;&gt;&gt; torch.from_numpy(x).float() / 5</span></p><p class="c2"><span class="c1">tensor([0.0000, 0.2000, 0.4000, 0.6000, 0.8000])</span></p></td></tr></tbody></table><p class="c2 c17"><span class="c0"></span></p><h4 class="c46 c25" id="h.br8vew20r7k"><span class="c38">Feature Mapping</span></h4><p class="c2"><span>See </span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://github.com/wkentaro/pytorch-for-numpy-users&amp;sa=D&amp;source=editors&amp;ust=1615451627294000&amp;usg=AOvVaw0o1W7EH4y2dlOnWr_rxny2">PyTorch for Numpy users</a></span><span>&nbsp;for the comparison table.</span></p><h2 class="c39 c25" id="h.h1ei3avajrbn"><span class="c10">Training loop</span></h2><p class="c2"><span>This is an example code of training loop. Note </span><span class="c13">model.train()</span><span class="c0">.</span></p><p class="c2 c17"><span class="c0"></span></p><p class="c2 c17"><span class="c0"></span></p><a id="t.df4d8e7b83e3847f40cedf4a49147baac0a37f94"></a><a id="t.12"></a><table class="c55"><tbody><tr class="c28"><td class="c40" colspan="1" rowspan="1"><p class="c2"><span class="c1">device = torch.device(&#39;cuda:0&#39;)</span></p><p class="c2"><span class="c1">for i_epoch in range(args.epoch):</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; train_loss = 0</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; train_correct = 0</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; model.train()</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; for x, t in data_loader:</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; x = x.to(device)</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; t = t.to(device).long()</span></p><p class="c2 c17"><span class="c1"></span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; optimizer.zero_grad()</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; y = model(x)</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; loss = F.nll_loss(y, t)</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; loss.backward()</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; optimizer.step()</span></p><p class="c2 c17"><span class="c1"></span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; train_loss += loss.sum().item()</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; _, pred = torch.max(y, 1)</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; train_correct += (pred == t).sum().item()</span></p><p class="c2 c17"><span class="c1"></span></p><p class="c2"><span class="c1">&nbsp; &nbsp; train_loss /= len(data_loader.dataset)</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; train_accuracy = train_correct / len(data_loader.dataset)</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; print(&#39;Train average loss: {:.03f}&#39;.format(train_loss))</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; print(&#39;Train accuracy : {:.03f} %&#39;.format(train_accuracy * 100))</span></p></td></tr></tbody></table><p class="c2 c17"><span class="c0"></span></p><h2 class="c39 c25" id="h.3b2g5rapv9ec"><span class="c10">Evaluation loop</span></h2><p class="c2"><span>This is an example code of evaluation loop. Note </span><span class="c13">model.eval() </span><span>and </span><span class="c13">with torch.no_grad()</span><span>.</span></p><p class="c2 c17"><span class="c0"></span></p><a id="t.11fc8eb3ce1b6cb26c10953d413254d7b7fbaa04"></a><a id="t.13"></a><table class="c55"><tbody><tr class="c28"><td class="c40" colspan="1" rowspan="1"><p class="c2"><span class="c1">device = torch.device(&#39;cuda:0&#39;)</span></p><p class="c2"><span class="c1">total_loss = 0</span></p><p class="c2"><span class="c1">total_correct = 0</span></p><p class="c2"><span class="c1">model.eval()</span></p><p class="c2"><span class="c1">with torch.no_grad():</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; for x, t in data_loader:</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; x = x.to(device)</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; t = t.to(device).long()</span></p><p class="c2 c17"><span class="c1"></span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; y = model(x)</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; total_loss += F.nll_loss(y, t, reduction=&#39;sum&#39;).item()</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; _, pred = torch.max(y, 1)</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; total_correct += (pred == t).sum().item()</span></p><p class="c2 c17"><span class="c1"></span></p><p class="c2"><span class="c1">average_loss = total_loss / len(loader.dataset)</span></p><p class="c2"><span class="c1">accuracy = total_correct / len(loader.dataset)</span></p></td></tr></tbody></table><p class="c2 c17"><span class="c0"></span></p><h2 class="c39 c25" id="h.wanb8rb4d6lo"><span class="c10">Training and evaluation using Ignite</span></h2><p class="c2"><span>Ignite is something corresponding to </span><span class="c13">chainer.training.Trainer</span><span class="c0">&nbsp;in Chainer.</span></p><p class="c2"><span>This Chainer code:</span></p><p class="c2 c17"><span class="c1"></span></p><p class="c2 c17"><span class="c1"></span></p><p class="c2 c17"><span class="c0"></span></p><a id="t.06fe944358155c5c1c1720ada7a200c56fa902b2"></a><a id="t.14"></a><table class="c55"><tbody><tr class="c28"><td class="c40" colspan="1" rowspan="1"><p class="c2"><span class="c1">updater = chainer.training.StandardUpdater(</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; train_iter,</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; optimizer,</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; device=device)</span></p><p class="c2"><span class="c1">trainer = chainer.training.Trainer(updater, (100, &lsquo;epoch&rsquo;))</span></p><p class="c2"><span class="c1">trainer.extend(</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; extensions.Evaluator(</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; val_iter,</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; model,</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; device=device),</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; trigger=(1, &lsquo;epoch&rsquo;))</span></p><p class="c2"><span class="c1">trainer.run()</span></p></td></tr></tbody></table><p class="c2 c17"><span class="c0"></span></p><p class="c2"><span>can be written in PyTorch using Ignite:</span></p><p class="c2 c17"><span class="c0"></span></p><a id="t.26f609c9f19badd14a1ca750000df5cbc6d342ee"></a><a id="t.15"></a><table class="c55"><tbody><tr class="c28"><td class="c40" colspan="1" rowspan="1"><p class="c2"><span class="c1">trainer = ignite.engine.create_supervised_trainer(</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; model,</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; optimizer,</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; F.nll_loss,</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; device=device)</span></p><p class="c2"><span class="c1">evaluator = ignite.engine.create_supervised_evaluator(</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; model,</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; metrics={</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; &#39;accuracy&#39;: ignite.metrics.Accuracy(),</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; &#39;loss&#39;: ignite.metrics.Loss(F.nll_loss),</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; },</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; device=device)</span></p><p class="c2 c17"><span class="c1"></span></p><p class="c2"><span class="c1">@trainer.on(ignite.engine.Events.EPOCH_COMPLETED)</span></p><p class="c2"><span class="c1">def validation(engine):</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; evaluator.run(val_loader)</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; average_accuracy = evaluator.state.metrics[&lsquo;accuracy&rsquo;]</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; average_loss = evaluator.state.metrics[&lsquo;loss&rsquo;]</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; print(average_accuracy, average_loss)</span></p><p class="c2 c17"><span class="c1"></span></p><p class="c2"><span class="c1">trainer.run(train_loader, max_epochs=100)</span></p></td></tr></tbody></table><p class="c2 c17"><span class="c0"></span></p><p class="c2"><span>For a list of supported metrics, see </span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/ignite/metrics.html&amp;sa=D&amp;source=editors&amp;ust=1615451627309000&amp;usg=AOvVaw0SqrdQgqtVlxYkvw4m7onw">https://pytorch.org/ignite/metrics.html</a></span><span class="c0">.</span></p><h2 class="c39 c25" id="h.m8sz4mg7ioxl"><span class="c10">Using Chainer extensions with Ignite</span></h2><p class="c2"><span>Using </span><span class="c11"><a class="c9" href="#h.49ptm66ugzcy">cpm.ignite.add_trainer_extension</a></span><span class="c0">&nbsp;it is possible to register a chainer extension to be called within the ignite training loop.</span></p><p class="c2"><span class="c0">A list of the supported extensions follows:</span></p><p class="c2 c17"><span class="c0"></span></p><a id="t.d4eff46624142b2b1a44288ca2ae5657d3d651eb"></a><a id="t.16"></a><table class="c102"><tbody><tr class="c28"><td class="c57 c111" colspan="1" rowspan="1"><p class="c20"><span class="c22 c80 c75">Works</span></p></td><td class="c47 c111" colspan="1" rowspan="1"><p class="c20"><span class="c22 c75 c80">Doesn&rsquo;t work</span></p></td></tr><tr class="c28"><td class="c57" colspan="1" rowspan="1"><p class="c20"><span class="c27 c13">ExponentialShift</span></p></td><td class="c47" colspan="1" rowspan="1"><p class="c20"><span class="c27 c13">DumpGraph</span></p></td></tr><tr class="c28"><td class="c57" colspan="1" rowspan="1"><p class="c20"><span class="c27 c13">FailOnNonNumber</span></p></td><td class="c47" colspan="1" rowspan="1"><p class="c20"><span class="c27 c13">Evaluator</span></p></td></tr><tr class="c101"><td class="c57" colspan="1" rowspan="1"><p class="c20"><span class="c27 c13">InverseShift</span></p></td><td class="c47" colspan="1" rowspan="1"><p class="c20"><span class="c13 c27">unchain_variables</span></p></td></tr><tr class="c28"><td class="c57" colspan="1" rowspan="1"><p class="c20"><span class="c27 c13">LinearShift</span></p></td><td class="c47" colspan="1" rowspan="1"><p class="c20 c17"><span class="c27 c13"></span></p></td></tr><tr class="c28"><td class="c57" colspan="1" rowspan="1"><p class="c20"><span class="c27 c13">LogReport</span></p></td><td class="c47" colspan="1" rowspan="1"><p class="c20 c17"><span class="c27 c13"></span></p></td></tr><tr class="c28"><td class="c57" colspan="1" rowspan="1"><p class="c20"><span class="c27 c13">MicroAverage</span></p></td><td class="c47" colspan="1" rowspan="1"><p class="c20 c17"><span class="c27 c13"></span></p></td></tr><tr class="c28"><td class="c57" colspan="1" rowspan="1"><p class="c20"><span class="c27 c13">MultistepShift</span></p></td><td class="c47" colspan="1" rowspan="1"><p class="c20 c17"><span class="c27 c13"></span></p></td></tr><tr class="c28"><td class="c57" colspan="1" rowspan="1"><p class="c20"><span class="c27 c13">ParameterStatistics</span></p></td><td class="c47" colspan="1" rowspan="1"><p class="c20 c17"><span class="c27 c13"></span></p></td></tr><tr class="c28"><td class="c57" colspan="1" rowspan="1"><p class="c20"><span class="c27 c13">PlotReport</span></p></td><td class="c47" colspan="1" rowspan="1"><p class="c20 c17"><span class="c27 c13"></span></p></td></tr><tr class="c28"><td class="c57" colspan="1" rowspan="1"><p class="c20"><span class="c27 c13">PolynomialShift</span></p></td><td class="c47" colspan="1" rowspan="1"><p class="c20 c17"><span class="c27 c13"></span></p></td></tr><tr class="c28"><td class="c57" colspan="1" rowspan="1"><p class="c20"><span class="c27 c13">PrintReport</span></p></td><td class="c47" colspan="1" rowspan="1"><p class="c20 c17"><span class="c27 c13"></span></p></td></tr><tr class="c28"><td class="c57" colspan="1" rowspan="1"><p class="c20"><span class="c27 c13">ProgressBar</span></p></td><td class="c47" colspan="1" rowspan="1"><p class="c20 c17"><span class="c27 c13"></span></p></td></tr><tr class="c28"><td class="c57" colspan="1" rowspan="1"><p class="c20"><span class="c27 c13">snapshot(read docs)</span></p></td><td class="c47" colspan="1" rowspan="1"><p class="c20 c17"><span class="c27 c13"></span></p></td></tr><tr class="c28"><td class="c57" colspan="1" rowspan="1"><p class="c20"><span class="c27 c13">StepShift</span></p></td><td class="c47" colspan="1" rowspan="1"><p class="c20 c17"><span class="c27 c13"></span></p></td></tr><tr class="c28"><td class="c57" colspan="1" rowspan="1"><p class="c20"><span class="c27 c13">observe_lr</span></p></td><td class="c47" colspan="1" rowspan="1"><p class="c20 c17"><span class="c27 c13"></span></p></td></tr><tr class="c28"><td class="c57" colspan="1" rowspan="1"><p class="c20"><span class="c27 c13">VariableStatisticsPlot</span></p></td><td class="c47" colspan="1" rowspan="1"><p class="c20 c17"><span class="c27 c13"></span></p></td></tr><tr class="c28"><td class="c57" colspan="1" rowspan="1"><p class="c20"><span class="c27 c13">WarmupShift</span></p></td><td class="c47" colspan="1" rowspan="1"><p class="c20 c17"><span class="c27 c13"></span></p></td></tr></tbody></table><p class="c2"><span class="c0">Some drawbacks rely on that metrics associated to the model or links might not accessible by default.</span></p><p class="c2"><span class="c0">For example the user will need to report the loss or accuracy per iteration by using an ignite callback as this was done inside the chainer model.</span></p><p class="c2 c17"><span class="c0"></span></p><p class="c2"><span class="c0">Also for some extensions to work it is necessary for the user to assign the torch or chainer model to the optimizer target attribute and the output directory path for the LogReport, plotters and snapshot extensions</span></p><a id="t.d3bbb7d0330927644916c6808491d67b611bb39a"></a><a id="t.17"></a><table class="c55"><tbody><tr class="c28"><td class="c40" colspan="1" rowspan="1"><p class="c18"><span class="c29 c13">from chainer import reporter<br>@trainer.on(Events.ITERATION_COMPLETED)</span></p><p class="c18"><span class="c29 c13">def report_loss(engine):</span></p><p class="c18"><span class="c29 c13">&nbsp; &nbsp; reporter.report({&#39;loss&#39;:engine.state.output})</span></p></td></tr></tbody></table><p class="c2 c17"><span class="c0"></span></p><p class="c2"><span class="c0">An example of how to register multiple extensions:</span></p><a id="t.8bbf20fa9e631df5f36bd1501c7e922ff7c73524"></a><a id="t.18"></a><table class="c55"><tbody><tr class="c28"><td class="c40" colspan="1" rowspan="1"><p class="c18"><span class="c13 c58 c67"># Torch optimizer</span><span class="c29 c13"><br>optimizer = SGD(model.parameters(), lr=lr, momentum=momentum)</span></p><p class="c18"><span class="c61 c13 c58 c67"># Ignite trainer</span></p><p class="c18"><span class="c29 c13">trainer = create_supervised_trainer(model, optimizer, F.nll_loss, device=device)</span></p><p class="c18"><span class="c13 c58"><br></span><span class="c61 c13 c58 c67"># Add the model to the target attribute of the optimizer</span></p><p class="c18"><span class="c29 c13">optimizer.target = model</span></p><p class="c18 c17"><span class="c29 c13"></span></p><p class="c18"><span class="c13 c58 c67"># Set the output dir for some of the extensions</span></p><p class="c18"><span class="c29 c13">trainer.out = &#39;result&#39;</span></p><p class="c18 c17"><span class="c29 c13"></span></p><p class="c18"><span class="c61 c13 c58 c67"># Restore the snapshot</span></p><p class="c18"><span class="c29 c13">cpm.ignite.load_chainer_snapshot(trainer, optimizer, &#39;result/snapshot_iter_4691&#39;)</span></p><p class="c18 c17"><span class="c29 c13"></span></p><p class="c18"><span class="c61 c13 c58 c67"># Add a bunch of extensions</span></p><p class="c18"><span class="c29 c13">cpm.ignite.add_trainer_extension(trainer, optimizer, extensions.ProgressBar())</span></p><p class="c18"><span class="c29 c13">cpm.ignite.add_trainer_extension(trainer, optimizer, extensions.observe_lr())</span></p><p class="c18"><span class="c29 c13">cpm.ignite.add_trainer_extension(trainer, optimizer, extensions.MicroAverage(&#39;loss&#39;,&#39;lr&#39;,&#39;mav&#39;,(1, &#39;iteration&#39;)))</span></p><p class="c18"><span class="c29 c13">cpm.ignite.add_trainer_extension(trainer, optimizer, extensions.LogReport())</span></p><p class="c18"><span class="c29 c13">cpm.ignite.add_trainer_extension(trainer, optimizer, extensions.FailOnNonNumber())</span></p><p class="c18"><span class="c29 c13">cpm.ignite.add_trainer_extension(trainer, optimizer, extensions.ExponentialShift(&#39;lr&#39;, 0.9, 1.0, 0.1))</span></p><p class="c18"><span class="c29 c13">cpm.ignite.add_trainer_extension(trainer, optimizer, extensions.ParameterStatistics(model, prefix=&#39;model&#39;))</span></p><p class="c18"><span class="c29 c13">cpm.ignite.add_trainer_extension(trainer, optimizer, extensions.VariableStatisticsPlot(model))</span></p><p class="c18"><span class="c29 c13">cpm.ignite.add_trainer_extension(trainer, optimizer, extensions.PrintReport(</span></p><p class="c18"><span class="c29 c13">&nbsp; &nbsp; [&#39;epoch&#39;, &#39;iteration&#39;, &#39;loss&#39;, &#39;lr&#39;, &#39;mav&#39;, &#39;model/fc2/weight/grad/percentile/1&#39;]))</span></p><p class="c18"><span class="c29 c13">cpm.ignite.add_trainer_extension(trainer, optimizer, extensions.PlotReport([&#39;loss&#39;],</span></p><p class="c18"><span class="c29 c13">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &#39;epoch&#39;, filename=&#39;loss.png&#39;))</span></p><p class="c18"><span class="c13 c58">cpm.ignite.add_trainer_extension(trainer, optimizer, extensions.snapshot(writer=</span><span class="c13 c58">writer)</span><span class="c13 c58">, trigger=(1, &#39;epoch&#39;)) &nbsp;# writer is a SimpleWriter</span></p></td></tr></tbody></table><p class="c2 c17"><span class="c0"></span></p><h3 class="c46 c25" id="h.ojt9418jpkms"><span class="c8">Snapshots</span></h3><p class="c2 c17"><span class="c0"></span></p><p class="c2"><span class="c0">When using the snapshot extension with an ignite trainer, the pytorch objects are saved to an additional &ldquo;snapshot-torch&rdquo; file in the output folder. This allows to keep using these snapshots once the migration is finished and directly load pytorch models or the optimizer state from these files.</span></p><p class="c2"><span class="c0">Additionally, if you are mixing chainer models or optimizers with ignite and pytorch, these objects will be saved in the chainer snapshot file.</span></p><p class="c2 c17"><span class="c0"></span></p><p class="c2"><span>The correct way to restore a snapshot is by using </span><span class="c13">cpm.ignite.load_chainer_snapshot(engine, optimizer, snapshot_path) </span><span class="c0">with the Chainer snapshot path.</span></p><p class="c2"><span class="c70 c87 c75 c69">Note that previously taken Chainer snapshots are not compatible.</span></p><h2 class="c39 c25" id="h.s8lyt5panb7"><span class="c10">Porting custom updater using Ignite</span></h2><p class="c2"><span class="c0">You can pass a step function to an Ignite engine.</span></p><ul class="c64 lst-kix_ne2kodfi1kix-0 start"><li class="c2 c42 li-bullet-0"><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://github.com/pytorch/ignite/blob/55cb30b06330a1fd7bf1c7e0d23cba89b10f75bf/examples/gan/dcgan.py%23L308&amp;sa=D&amp;source=editors&amp;ust=1615451627322000&amp;usg=AOvVaw0OnhfoWj62eOAkEA7Fuymp">DCGAN example</a></span></li></ul><h2 class="c39 c25" id="h.xmsyb5asd2c9"><span class="c10">Rewriting existing Chainer model</span></h2><p class="c2"><span>Use </span><span class="c11"><a class="c9" href="#h.mkuuagm60br0">Mapping of functions and links</a></span><span class="c0">&nbsp;to find and replace with the corresponding feature in PyTorch. You can also find existing model implementations in:</span></p><ul class="c64 lst-kix_mfwegjdddisb-0 start"><li class="c2 c42 li-bullet-0"><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://github.com/pytorch/examples&amp;sa=D&amp;source=editors&amp;ust=1615451627323000&amp;usg=AOvVaw1ikpoZ9T3QOxmr-EaRMbuR">PyTorch Examples</a></span></li><li class="c2 c42 li-bullet-0"><span class="c23">torchvision.models</span><span>&nbsp;module (CV models) (</span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torchvision/models.html&amp;sa=D&amp;source=editors&amp;ust=1615451627323000&amp;usg=AOvVaw3vH63d6HDNS5UNkb4kWhS8">API Reference</a></span><span class="c0">)</span></li><li class="c2 c42 li-bullet-0"><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/hub&amp;sa=D&amp;source=editors&amp;ust=1615451627324000&amp;usg=AOvVaw2pBsvji20MbkriYFtDaKMa">PyTorch Hub</a></span><span>&nbsp;(models from community) (</span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/master/hub.html&amp;sa=D&amp;source=editors&amp;ust=1615451627324000&amp;usg=AOvVaw3iCXg24-KbCHR5gSsxTpkk">API Reference</a></span><span class="c0">)</span></li></ul><p class="c2 c17"><span class="c0"></span></p><p class="c2"><span class="c0">Common pitfalls:</span></p><ul class="c64 lst-kix_r60cdwx1dlic-0 start"><li class="c2 c42 li-bullet-0"><span class="c0">Image format: RGB or BGR</span></li><li class="c2 c42 li-bullet-0"><span class="c0">Image normalization: [0,1] or [0,255]</span></li><li class="c2 c42 li-bullet-0"><span>Some old PyTorch examples and community projects are using </span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/autograd.html%23variable-deprecated&amp;sa=D&amp;source=editors&amp;ust=1615451627324000&amp;usg=AOvVaw2LFuProdDciwzpFAucHXH3">torch.autograd.Variable</a></span><span class="c0">, which is a deprecated interface.</span></li><li class="c2 c42 li-bullet-0"><span>Some well-known models such as resnet might have different behavior in ChainerCV and torchvision. For example, </span><span class="c23">chainercv.links.ResNet50</span><span>&nbsp;applies softmax to the output while </span><span class="c23">torchvision.models.resnet50</span><span class="c0">&nbsp;does not.</span></li></ul><h2 class="c25 c39" id="h.mkuuagm60br0"><span class="c10">Functions and Links</span></h2><p class="c2"><span class="c0">You can find the PyTorch equivalent of Chainer&#39;s functions and links in tables below.</span></p><p class="c2 c17"><span class="c0"></span></p><p class="c2"><span class="c0">Notes:</span></p><ul class="c64 lst-kix_c4453ihcjob0-0 start"><li class="c2 c42 li-bullet-0"><span>Unlike NumPy/CuPy, PyTorch Tensor itself supports gradient computation (you can safely use </span><span class="c23">torch.*</span><span>&nbsp;or </span><span class="c23">torch.nn.functional.*</span><span>&nbsp;on </span><span class="c23">torch.Tensor</span><span class="c0">)</span></li><li class="c2 c42 li-bullet-0"><span>Conventions of keyword arguments: </span><span class="c23">dim</span><span>&nbsp;and </span><span class="c23">keepdim</span><span>&nbsp;is used in PyTorch instead of </span><span class="c23">axis</span><span>&nbsp;and </span><span class="c23">keepdims</span><span class="c0">&nbsp;in Chainer/NumPy.</span></li><li class="c2 c42 li-bullet-0"><span>Unlike Chainer, PyTorch provides the Module version of each function (e.g., </span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.html%23torch.nn.ReLU&amp;sa=D&amp;source=editors&amp;ust=1615451627326000&amp;usg=AOvVaw3pIGNrTF7pKWOvAB7S3a-N">nn.ReLU</a></span><span>&nbsp;for </span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.relu&amp;sa=D&amp;source=editors&amp;ust=1615451627326000&amp;usg=AOvVaw0zo2U5ZnviZIJ5E8fyGC2N">F.relu</a></span><span>), so you can use the Module version when defining model using functions in </span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.html%23sequential&amp;sa=D&amp;source=editors&amp;ust=1615451627326000&amp;usg=AOvVaw19kpDQ-2895tPPTtqwOvPr">Sequential</a></span><span class="c0">&nbsp;container.</span></li></ul><h3 class="c46 c25" id="h.e17stx2v9ds3"><span class="c8">Functions</span></h3><p class="c2"><span class="c23">F</span><span>&nbsp;refers to </span><span class="c23">chainer.functions</span><span>&nbsp;(Chainer) / </span><span class="c23">torch.nn.functional</span><span>&nbsp;(PyTorch).</span></p><p class="c2 c17"><span class="c0"></span></p><a id="t.43a2867db6e97b9fee6ed5cc18420f263a69f5ec"></a><a id="t.19"></a><table class="c55"><tbody><tr class="c34"><td class="c50 c97" colspan="1" rowspan="1"><p class="c52 c25"><span class="c7 c36">Chainer</span></p></td><td class="c50 c108" colspan="1" rowspan="1"><p class="c52 c25"><span class="c7 c36">PyTorch</span></p></td><td class="c37" colspan="1" rowspan="1"><p class="c25 c52"><span class="c7 c36">Notes</span></p></td></tr><tr class="c12"><td class="c59" colspan="3" rowspan="1"><p class="c2"><span class="c7 c36">Arithmetic functions</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.add.html%23chainer.functions.add&amp;sa=D&amp;source=editors&amp;ust=1615451627330000&amp;usg=AOvVaw2wt3wYrvjJU9aLMqiztEkv">F.add</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.add&amp;sa=D&amp;source=editors&amp;ust=1615451627330000&amp;usg=AOvVaw3njqXmIpOkF5SoiRIeYsLa">torch.add</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">Batched addition (accumulating multiple tensors in a single call) is not supported.</span></p></td></tr><tr class="c12"><td class="c59" colspan="3" rowspan="1"><p class="c2"><span class="c7 c36">Activation functions</span></p></td></tr><tr class="c34"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.clipped_relu.html%23chainer.functions.clipped_relu&amp;sa=D&amp;source=editors&amp;ust=1615451627332000&amp;usg=AOvVaw27p53WHF_5XUioBKcTRGdG">F.clipped_relu</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">Rewrite as:</span></p><p class="c2"><span class="c23 c7">x.</span><span class="c11 c23 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/tensors.html%23torch.Tensor.clamp&amp;sa=D&amp;source=editors&amp;ust=1615451627333000&amp;usg=AOvVaw3ByMU5eLvdgeN-Og4VgRmm">clamp</a></span><span class="c32 c23 c7">(0, z)</span></p></td></tr><tr class="c34"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.crelu.html%23chainer.functions.crelu&amp;sa=D&amp;source=editors&amp;ust=1615451627334000&amp;usg=AOvVaw03cJRbrwwp7snqfvqklKNp">F.crelu</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">Rewrite as:</span></p><p class="c2"><span class="c11 c23 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.cat&amp;sa=D&amp;source=editors&amp;ust=1615451627335000&amp;usg=AOvVaw3Fda1fXNWjvykIi6V0j5SV">torch.cat</a></span><span class="c32 c23 c7">((F.relu(x), F.relu(-x)))</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.elu.html%23chainer.functions.elu&amp;sa=D&amp;source=editors&amp;ust=1615451627335000&amp;usg=AOvVaw1Z-NwgPZpfhrUVp_FUCSE2">F.elu</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.elu&amp;sa=D&amp;source=editors&amp;ust=1615451627336000&amp;usg=AOvVaw3KExSrTdw8TcPA0J23EbWZ">F.elu</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c34"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.hard_sigmoid.html%23chainer.functions.hard_sigmoid&amp;sa=D&amp;source=editors&amp;ust=1615451627336000&amp;usg=AOvVaw2P-egL9T2CZzpEPcbGeDK3">F.hard_sigmoid</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">Rewrite as:</span></p><p class="c2"><span class="c11 c23 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.clamp&amp;sa=D&amp;source=editors&amp;ust=1615451627338000&amp;usg=AOvVaw1tLBasOn3flrpJr8lyYcmf">torch.clamp</a></span><span class="c32 c23 c7">(x * 0.2 + 0.5, 0, 1)</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.leaky_relu.html%23chainer.functions.leaky_relu&amp;sa=D&amp;source=editors&amp;ust=1615451627338000&amp;usg=AOvVaw3nQM-2dCpR858q6Te0Yq0f">F.leaky_relu</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.leaky_relu&amp;sa=D&amp;source=editors&amp;ust=1615451627339000&amp;usg=AOvVaw2YjUUgXtSzUGcA_LjXZs6H">F.leaky_relu</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">The default slope value is different.</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.log_softmax.html%23chainer.functions.log_softmax&amp;sa=D&amp;source=editors&amp;ust=1615451627340000&amp;usg=AOvVaw3QyQTHs_bgluIv5ipzF6RP">F.log_softmax</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.log_softmax&amp;sa=D&amp;source=editors&amp;ust=1615451627340000&amp;usg=AOvVaw3-FfdsFtvroMxYKfwg9zJo">F.log_softmax</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.lstm.html%23chainer.functions.lstm&amp;sa=D&amp;source=editors&amp;ust=1615451627341000&amp;usg=AOvVaw3TXqaMypJyMOU0CkxFYs5P">F.lstm</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">See L.LSTM.</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.maxout.html%23chainer.functions.maxout&amp;sa=D&amp;source=editors&amp;ust=1615451627342000&amp;usg=AOvVaw1ZxYBeR79bi1PMeiJz9j8L">F.maxout</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c7">Need to implement manually; see </span><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://github.com/pytorch/pytorch/issues/805&amp;sa=D&amp;source=editors&amp;ust=1615451627343000&amp;usg=AOvVaw3QXTMMxS9u_rV_hOSNFhbU">https://github.com/pytorch/pytorch/issues/805</a></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.prelu.html%23chainer.functions.prelu&amp;sa=D&amp;source=editors&amp;ust=1615451627344000&amp;usg=AOvVaw2Bo246-4H713mKPDJoDmAt">F.prelu</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.prelu&amp;sa=D&amp;source=editors&amp;ust=1615451627345000&amp;usg=AOvVaw1vCqLXOhjU5fhCVsUu6zrA">F.prelu</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.rrelu.html%23chainer.functions.rrelu&amp;sa=D&amp;source=editors&amp;ust=1615451627346000&amp;usg=AOvVaw1-n0IHu-_2FlFIINv6PZyV">F.rrelu</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.rrelu&amp;sa=D&amp;source=editors&amp;ust=1615451627347000&amp;usg=AOvVaw06yHnGjQXr8U8ZEK12Eska">F.rrelu</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">`training` option must be explicitly specified instead of `train` config in Chainer.</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.relu.html%23chainer.functions.relu&amp;sa=D&amp;source=editors&amp;ust=1615451627348000&amp;usg=AOvVaw2KzkVZN90clY4NSZAG0cMX">F.relu</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.relu&amp;sa=D&amp;source=editors&amp;ust=1615451627349000&amp;usg=AOvVaw2Udv1PT7Auin6Mac_jRnEm">F.relu</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.relu6.html%23chainer.functions.relu6&amp;sa=D&amp;source=editors&amp;ust=1615451627350000&amp;usg=AOvVaw2EPtIpvg5HMspFRBhUheZX">F.relu6</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.relu6&amp;sa=D&amp;source=editors&amp;ust=1615451627351000&amp;usg=AOvVaw1PJT5Bx0RZShCAt91P9inh">F.relu6</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.selu.html%23chainer.functions.selu&amp;sa=D&amp;source=editors&amp;ust=1615451627352000&amp;usg=AOvVaw1lbaL-_z5k8-7go2M4UI_g">F.selu</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.selu&amp;sa=D&amp;source=editors&amp;ust=1615451627353000&amp;usg=AOvVaw1Of1SkhGDP5eQl3s-sP8M0">F.selu</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.sigmoid.html%23chainer.functions.sigmoid&amp;sa=D&amp;source=editors&amp;ust=1615451627355000&amp;usg=AOvVaw2fPPTqoMjqbGkC63QUQ_xq">F.sigmoid</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.sigmoid&amp;sa=D&amp;source=editors&amp;ust=1615451627355000&amp;usg=AOvVaw1atbgKoJ9piCfeld19P_wU">F.sigmoid</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.slstm.html%23chainer.functions.slstm&amp;sa=D&amp;source=editors&amp;ust=1615451627357000&amp;usg=AOvVaw3SZny-nKrv4DhdNNp0hqoD">F.slstm</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c7">Some OSS implementations are available (e.g., </span><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://github.com/reachtarunhere/S-LSTM-PyTorch&amp;sa=D&amp;source=editors&amp;ust=1615451627358000&amp;usg=AOvVaw2SR_K9CkoA-2U5_SIZ9p19">https://github.com/reachtarunhere/S-LSTM-PyTorch</a></span><span class="c5">)</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.softmax.html%23chainer.functions.softmax&amp;sa=D&amp;source=editors&amp;ust=1615451627358000&amp;usg=AOvVaw3og77BbdAMcOkL1aVfMGme">F.softmax</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.softmax&amp;sa=D&amp;source=editors&amp;ust=1615451627359000&amp;usg=AOvVaw2-vVty9hmiSNp0QeJTXVBU">F.softmax</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.softplus.html%23chainer.functions.softplus&amp;sa=D&amp;source=editors&amp;ust=1615451627359000&amp;usg=AOvVaw0JjbMOATMg_Frhot3-8_Gx">F.softplus</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.softplus&amp;sa=D&amp;source=editors&amp;ust=1615451627360000&amp;usg=AOvVaw1hNKn8T43WZQ9qrfgyz1JG">F.softplus</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">PyTorch falls back to linear function by default; threshold option must be explicitly given.</span></p></td></tr><tr class="c34"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.swish.html%23chainer.functions.swish&amp;sa=D&amp;source=editors&amp;ust=1615451627361000&amp;usg=AOvVaw1gjTcNKNh198xB2cTRqtxm">F.swish</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">Rewrite as:</span></p><p class="c2"><span class="c23 c7">x * </span><span class="c11 c23 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.sigmoid&amp;sa=D&amp;source=editors&amp;ust=1615451627362000&amp;usg=AOvVaw1Gi4Etzjkin0wzSKjHA5kS">F.sigmoid</a></span><span class="c32 c23 c7">(beta * x)</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.tanh.html%23chainer.functions.tanh&amp;sa=D&amp;source=editors&amp;ust=1615451627362000&amp;usg=AOvVaw2iYN33Nr0TdtArF869Pcmf">F.tanh</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.tanh&amp;sa=D&amp;source=editors&amp;ust=1615451627363000&amp;usg=AOvVaw3CUP2umcJz2B-nr2p9MeNO">F.tanh</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.tree_lstm.html%23chainer.functions.tree_lstm&amp;sa=D&amp;source=editors&amp;ust=1615451627364000&amp;usg=AOvVaw1ls9TErj6_aDnWlirOmXul">F.tree_lstm</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c7">Some OSS implementations are available (e.g., </span><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://github.com/dasguptar/treelstm.pytorch&amp;sa=D&amp;source=editors&amp;ust=1615451627365000&amp;usg=AOvVaw32570fRnL9RG0RblB2FSCL">https://github.com/dasguptar/treelstm.pytorch</a></span><span class="c5">)</span></p></td></tr><tr class="c12"><td class="c59" colspan="3" rowspan="1"><p class="c2"><span class="c7 c36">Array manipulations</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.as_strided.html%23chainer.functions.as_strided&amp;sa=D&amp;source=editors&amp;ust=1615451627366000&amp;usg=AOvVaw2Dg2oGjJkfTwPLbG4jnA-I">F.as_strided</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.as_strided&amp;sa=D&amp;source=editors&amp;ust=1615451627367000&amp;usg=AOvVaw3GuafNKN6xEMUcI7qpLa8z">torch.as_strided</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c34"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.broadcast.html%23chainer.functions.broadcast&amp;sa=D&amp;source=editors&amp;ust=1615451627367000&amp;usg=AOvVaw2250WXgg92kCuElw0tsBjI">F.broadcast</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.broadcast_tensors&amp;sa=D&amp;source=editors&amp;ust=1615451627368000&amp;usg=AOvVaw37FhmDgm5x-IxAj1DojdYh">torch.broadcast_tensors</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c7">PyTorch operations perform broadcast automatically like as in NumPy: </span><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/notes/broadcasting.html&amp;sa=D&amp;source=editors&amp;ust=1615451627369000&amp;usg=AOvVaw01ZcxgISvME7GEAXfWv463">https://pytorch.org/docs/stable/notes/broadcasting.html</a></span></p></td></tr><tr class="c34"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.broadcast_to.html%23chainer.functions.broadcast_to&amp;sa=D&amp;source=editors&amp;ust=1615451627369000&amp;usg=AOvVaw2tKxh7SJIoOFL5N5avgnUK">F.broadcast_to</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c7">N/A: </span><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://github.com/pytorch/pytorch/pull/17160&amp;sa=D&amp;source=editors&amp;ust=1615451627370000&amp;usg=AOvVaw0iLGeZg0Tu5n55c1gQ3LnI">https://github.com/pytorch/pytorch/pull/17160</a></span></p><sup><a href="#cmnt1" id="cmnt_ref1">[a]</a></sup></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.cast.html%23chainer.functions.cast&amp;sa=D&amp;source=editors&amp;ust=1615451627371000&amp;usg=AOvVaw1TYBK1oInaXnqcSQ6PxeU8">F.cast</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/tensors.html%23torch.Tensor.to&amp;sa=D&amp;source=editors&amp;ust=1615451627372000&amp;usg=AOvVaw0WdOkujOfiRxT4Dr2g1u0b">Tensor.to</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.concat.html%23chainer.functions.concat&amp;sa=D&amp;source=editors&amp;ust=1615451627373000&amp;usg=AOvVaw2JeTPqs2YetAPFaNkNKEdE">F.concat</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.cat&amp;sa=D&amp;source=editors&amp;ust=1615451627373000&amp;usg=AOvVaw3FmIGkCWXpMaaOCJiciXpB">torch.cat</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.copy.html%23chainer.functions.copy&amp;sa=D&amp;source=editors&amp;ust=1615451627374000&amp;usg=AOvVaw3k1UNXy3fOJJ_lijO49-sA">F.copy</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/tensors.html%23torch.Tensor.to&amp;sa=D&amp;source=editors&amp;ust=1615451627375000&amp;usg=AOvVaw3NT8Ka3-YDTgt6Jy0Ck-qc">Tensor.to</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.depth2space.html%23chainer.functions.depth2space&amp;sa=D&amp;source=editors&amp;ust=1615451627376000&amp;usg=AOvVaw0raMm62heKZKeBY6EQkT00">F.depth2space</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.pixel_shuffle&amp;sa=D&amp;source=editors&amp;ust=1615451627377000&amp;usg=AOvVaw2c--FfTDp0JUGaH3KREubo">F.pixel_shuffle</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.diagonal.html%23chainer.functions.diagonal&amp;sa=D&amp;source=editors&amp;ust=1615451627378000&amp;usg=AOvVaw1FkZSLmdxTOOLX33PYzwcB">F.diagonal</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.diagonal&amp;sa=D&amp;source=editors&amp;ust=1615451627378000&amp;usg=AOvVaw1s_FE7H97kfLH7pb3pSd1a">torch.diagonal</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.dstack.html%23chainer.functions.dstack&amp;sa=D&amp;source=editors&amp;ust=1615451627379000&amp;usg=AOvVaw3QftqRvZn2f_7m-uT18s4v">F.dstack</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">Rewrite as:</span></p><p class="c2"><span class="c11 c23 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.cat&amp;sa=D&amp;source=editors&amp;ust=1615451627380000&amp;usg=AOvVaw0L-xHf2OtH1N4irYVJXe38">torch.cat</a></span><span class="c32 c23 c7">([a,b],dim=2)</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.expand_dims.html%23chainer.functions.expand_dims&amp;sa=D&amp;source=editors&amp;ust=1615451627380000&amp;usg=AOvVaw3La503XH7qSmsJYKOBbZs3">F.expand_dims</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c7">Rewrite as:</span></p><p class="c2"><span class="c11 c23 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.unsqueeze&amp;sa=D&amp;source=editors&amp;ust=1615451627381000&amp;usg=AOvVaw2dBZzYL4uvhlu_I_FaMpPw">torch.unsqueeze</a></span><span class="c32 c23 c7">(a, dim)</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.flatten.html%23chainer.functions.flatten&amp;sa=D&amp;source=editors&amp;ust=1615451627382000&amp;usg=AOvVaw16gXkqmqVPsNa2oTgyWqWf">F.flatten</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.flatten&amp;sa=D&amp;source=editors&amp;ust=1615451627382000&amp;usg=AOvVaw2AWQozgzFoSieDbUZZpWTx">torch.flatten</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.flip.html%23chainer.functions.flip&amp;sa=D&amp;source=editors&amp;ust=1615451627383000&amp;usg=AOvVaw3WkiEuyHnCMcZxlKdg_QQe">F.flip</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.flip&amp;sa=D&amp;source=editors&amp;ust=1615451627384000&amp;usg=AOvVaw25GCLUk03khFyECg0oPhQi">torch.flip</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.fliplr.html%23chainer.functions.fliplr&amp;sa=D&amp;source=editors&amp;ust=1615451627385000&amp;usg=AOvVaw2KeHENP_tHvo_eQg1ehMjN">F.fliplr</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.flip&amp;sa=D&amp;source=editors&amp;ust=1615451627385000&amp;usg=AOvVaw25GZAix6llenAI7Qh1J1_S">torch.flip</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">Use dims=1</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.flipud.html%23chainer.functions.flipud&amp;sa=D&amp;source=editors&amp;ust=1615451627386000&amp;usg=AOvVaw0Le-v84dbadSIUUmLeOnaj">F.flipud</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.flip&amp;sa=D&amp;source=editors&amp;ust=1615451627386000&amp;usg=AOvVaw2-FmlkaCiwo6Mu9KlDj-hq">torch.flip</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">Use dims=0</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.get_item.html%23chainer.functions.get_item&amp;sa=D&amp;source=editors&amp;ust=1615451627387000&amp;usg=AOvVaw3dnyui_eEcIN6tlQAnfPNZ">F.get_item</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">Use direct indexing: `x[indexes]`. Negative strides are not supported.</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.hstack.html%23chainer.functions.hstack&amp;sa=D&amp;source=editors&amp;ust=1615451627389000&amp;usg=AOvVaw0j28piPkLER6etFFvGJ84c">F.hstack</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">Rewrite as:</span></p><p class="c2"><span class="c11 c23 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.cat&amp;sa=D&amp;source=editors&amp;ust=1615451627390000&amp;usg=AOvVaw074QjvAmoPk490umwQGzHb">torch.cat</a></span><span class="c32 c23 c7">([a,b],dim=1)</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.im2col.html%23chainer.functions.im2col&amp;sa=D&amp;source=editors&amp;ust=1615451627390000&amp;usg=AOvVaw2wlnUgdYYUSeIUJU6IIV0y">F.im2col</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.unfold&amp;sa=D&amp;source=editors&amp;ust=1615451627391000&amp;usg=AOvVaw0dP3Pp6Dfydo6WriXZEUeo">F.unfold</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">NCHW is only supported</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.moveaxis.html%23chainer.functions.moveaxis&amp;sa=D&amp;source=editors&amp;ust=1615451627391000&amp;usg=AOvVaw36esNDJt3_pMrMon9POGvP">F.moveaxis</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23Tensor.permute&amp;sa=D&amp;source=editors&amp;ust=1615451627392000&amp;usg=AOvVaw2wlQG1CqofeEgrbMGv_gaX">Tensor.permute</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c7">See: </span><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://discuss.pytorch.org/t/swap-axes-in-pytorch/970/2&amp;sa=D&amp;source=editors&amp;ust=1615451627393000&amp;usg=AOvVaw1qStt4YKjePSiNdWEXpw6a">https://discuss.pytorch.org/t/swap-axes-in-pytorch/970/2</a></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.pad.html%23chainer.functions.pad&amp;sa=D&amp;source=editors&amp;ust=1615451627393000&amp;usg=AOvVaw0O_yH5ckgKS06EbodzAkwz">F.pad</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.pad&amp;sa=D&amp;source=editors&amp;ust=1615451627394000&amp;usg=AOvVaw2vLviDi_01lMp8jsFviIk4">F.pad</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">Replace `constant_values` argument with `value`. Modes other than `constant` are also available.</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.pad_sequence.html%23chainer.functions.pad_sequence&amp;sa=D&amp;source=editors&amp;ust=1615451627394000&amp;usg=AOvVaw11t3pLx1X-Gc2j4XgVh-84">F.pad_sequence</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.html?highlight%3Dpad_sequence%23torch.nn.utils.rnn.pad_sequence&amp;sa=D&amp;source=editors&amp;ust=1615451627395000&amp;usg=AOvVaw0R3RCg2u8B6sUhqLvuZlAp">nn.utils.rnn.pad_squence</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">You cannot specify the length but the maximum length among the inputs is used.</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.permutate.html%23chainer.functions.permutate&amp;sa=D&amp;source=editors&amp;ust=1615451627396000&amp;usg=AOvVaw1mmXUWiRVez6KOFkbtA1Uw">F.permutate</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/tensors.html%23torch.Tensor.permute&amp;sa=D&amp;source=editors&amp;ust=1615451627396000&amp;usg=AOvVaw0OUl09i2Wekbq6GyT1661W">Tensor.permute</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.repeat.html%23chainer.functions.repeat&amp;sa=D&amp;source=editors&amp;ust=1615451627397000&amp;usg=AOvVaw2Obd-Gx3ptItfPLzExb3tM">F.repeat</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/tensors.html%23torch.Tensor.repeat&amp;sa=D&amp;source=editors&amp;ust=1615451627398000&amp;usg=AOvVaw27iRg0ad8Dte3YzqBUUTOu">Tensor.repeat</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">Different behavior to F.repeat. F.tile is more similar.</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.reshape.html%23chainer.functions.reshape&amp;sa=D&amp;source=editors&amp;ust=1615451627399000&amp;usg=AOvVaw0c3H40mspF9UilHeREoj_K">F.reshape</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.reshape&amp;sa=D&amp;source=editors&amp;ust=1615451627399000&amp;usg=AOvVaw3iJSuXMMXDGlhvNxr99Uki">torch.reshape</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.resize_images.html%23chainer.functions.resize_images&amp;sa=D&amp;source=editors&amp;ust=1615451627400000&amp;usg=AOvVaw1XHY-2mgNHPC3WxAjVmG7p">F.resize_images</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.interpolate&amp;sa=D&amp;source=editors&amp;ust=1615451627401000&amp;usg=AOvVaw0nGMBWrmBns8sLdViBOe8j">F.interpolate</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.rollaxis.html%23chainer.functions.rollaxis&amp;sa=D&amp;source=editors&amp;ust=1615451627402000&amp;usg=AOvVaw3QeyW70JYk6UKk20ZWsc55">F.rollaxis</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/tensors.html%23torch.Tensor.permute&amp;sa=D&amp;source=editors&amp;ust=1615451627402000&amp;usg=AOvVaw38dc_ntZik02v3A7ruaNKd">Tensor.premute</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c7">See </span><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://discuss.pytorch.org/t/swap-axes-in-pytorch/970/2&amp;sa=D&amp;source=editors&amp;ust=1615451627403000&amp;usg=AOvVaw0X3ocvJU3gA2rsF6cWqMDn">https://discuss.pytorch.org/t/swap-axes-in-pytorch/970/2</a></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.scatter_add.html%23chainer.functions.scatter_add&amp;sa=D&amp;source=editors&amp;ust=1615451627403000&amp;usg=AOvVaw07pRxOcISErY91BNQTKTag">F.scatter_add</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/tensors.html%23torch.Tensor.scatter_add&amp;sa=D&amp;source=editors&amp;ust=1615451627404000&amp;usg=AOvVaw2EYu7Rfgs_Kg-eM9AQvkW0">Tensor.scatter_add</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c34"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.select_item.html%23chainer.functions.select_item&amp;sa=D&amp;source=editors&amp;ust=1615451627404000&amp;usg=AOvVaw02Wgpy9ZRSbqHRcKk8dR5R">F.select_item</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">Rewrite as:</span></p><p class="c2"><span class="c11 c23 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.gather&amp;sa=D&amp;source=editors&amp;ust=1615451627405000&amp;usg=AOvVaw1ACWGI3BW3LpIByl4Rzm2a">torch.gather</a></span><span class="c32 c23 c7">(x, 1, t[:, None])[:, 0]</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.separate.html%23chainer.functions.separate&amp;sa=D&amp;source=editors&amp;ust=1615451627406000&amp;usg=AOvVaw3qSqrJlk1X5APJ1t5QacgP">F.separate</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.split&amp;sa=D&amp;source=editors&amp;ust=1615451627406000&amp;usg=AOvVaw3fQROxdL0Ra0HGAnkg6FtO">torch.split</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">Requires manual manipulation of the results to achieve some of the separate functionality.</span></p></td></tr><tr class="c34"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.space2depth.html%23chainer.functions.space2depth&amp;sa=D&amp;source=editors&amp;ust=1615451627407000&amp;usg=AOvVaw1TW-xEb5QX2PVDdeY6fue0">F.space2depth</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">You need to implement it yourself. Ref:</span></p><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://discuss.pytorch.org/t/is-there-any-layer-like-tensorflows-space-to-depth-function/3487/14&amp;sa=D&amp;source=editors&amp;ust=1615451627408000&amp;usg=AOvVaw2yE7xRinWGM8L64HgNcH4A">https://discuss.pytorch.org/t/is-there-any-layer-like-tensorflows-space-to-depth-function/3487/14</a></span></p></td></tr><tr class="c81"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.spatial_transformer_grid.html%23chainer.functions.spatial_transformer_grid&amp;sa=D&amp;source=editors&amp;ust=1615451627409000&amp;usg=AOvVaw2Motyg0RZ-C5iDi_GXoH1h">F.spatial_transformer_grid</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.affine_grid&amp;sa=D&amp;source=editors&amp;ust=1615451627409000&amp;usg=AOvVaw1cVpQ-xHoPWr89xMuCEgbT">F.affine_grid</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c7">The second argument `size` takes `torch.Size` object that denotes the target output image size (N, C, H, W), while `F.spatial_transformer_grid` takes just a tuple of (H, W). The size of returned tensor is also different: (N x H x W x 2) is returned instead of (N x 2 x H x W). Also note the breaking change regarding align_corners in v1.3.0 (</span><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://github.com/pytorch/pytorch/releases/tag/v1.3.0&amp;sa=D&amp;source=editors&amp;ust=1615451627410000&amp;usg=AOvVaw3Vl7IYJS4CvLYJIjnHv15t">https://github.com/pytorch/pytorch/releases/tag/v1.3.0</a></span><span class="c5">)</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.spatial_transformer_sampler.html%23chainer.functions.spatial_transformer_sampler&amp;sa=D&amp;source=editors&amp;ust=1615451627410000&amp;usg=AOvVaw3ULDdzkvVm3Ds1WpSjg4mY">F.spatial_transformer_sampler</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.grid_sample&amp;sa=D&amp;source=editors&amp;ust=1615451627411000&amp;usg=AOvVaw2H1ARWp9gVHjXtTPfMhJcP">F.grid_sample</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">Grid shape is (N, 2, H, W) in Chainer while (N, H, W, 2) in PyTorch.</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.split_axis.html%23chainer.functions.split_axis&amp;sa=D&amp;source=editors&amp;ust=1615451627412000&amp;usg=AOvVaw0lWrv7DOr1_VFYUN1IJjpv">F.split_axis</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.split&amp;sa=D&amp;source=editors&amp;ust=1615451627412000&amp;usg=AOvVaw1twcq0SQyj1rpwc3AVSlns">torch.split</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">No `force_tuple`.</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.squeeze.html%23chainer.functions.squeeze&amp;sa=D&amp;source=editors&amp;ust=1615451627413000&amp;usg=AOvVaw02wc60-Nx-XD_nkSWjcdkP">F.squeeze</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.squeeze&amp;sa=D&amp;source=editors&amp;ust=1615451627414000&amp;usg=AOvVaw2hCOSj7jHvUWmLHgS3g0yp">torch.squeeze</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.stack.html%23chainer.functions.stack&amp;sa=D&amp;source=editors&amp;ust=1615451627415000&amp;usg=AOvVaw38gO2wSNx-Y9Bn5g6aYxI_">F.stack</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.stack&amp;sa=D&amp;source=editors&amp;ust=1615451627415000&amp;usg=AOvVaw1La1w4pPIlfSivyOzgt5jf">torch.stack</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c7">Use </span><span class="c11 c23 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.stack&amp;sa=D&amp;source=editors&amp;ust=1615451627416000&amp;usg=AOvVaw3tt7usuTn0sGdkd3Sc6d23">torch.stack</a></span><span class="c7">&nbsp;or </span><span class="c11 c23 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.cat&amp;sa=D&amp;source=editors&amp;ust=1615451627416000&amp;usg=AOvVaw1Ep9wIuyeCzX14oz28UqSO">torch.cat</a></span><span class="c32 c23 c7">([a,b],dim=axis)</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.swapaxes.html%23chainer.functions.swapaxes&amp;sa=D&amp;source=editors&amp;ust=1615451627417000&amp;usg=AOvVaw1GWbgOpEe75jEEvyTULYT2">F.swapaxes</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c7">Use permute instead: </span><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://discuss.pytorch.org/t/swap-axes-in-pytorch/970/7&amp;sa=D&amp;source=editors&amp;ust=1615451627418000&amp;usg=AOvVaw21LjufZtnVKFJ15nfHreRx">https://discuss.pytorch.org/t/swap-axes-in-pytorch/970/7</a></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.tile.html%23chainer.functions.tile&amp;sa=D&amp;source=editors&amp;ust=1615451627418000&amp;usg=AOvVaw1aP32UsyWLWLCMGjAcvuAE">F.tile</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.repeat&amp;sa=D&amp;source=editors&amp;ust=1615451627419000&amp;usg=AOvVaw2_xxkEplzYicVFCTNhn_9Q">F.repeat</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.transpose.html%23chainer.functions.transpose&amp;sa=D&amp;source=editors&amp;ust=1615451627420000&amp;usg=AOvVaw0Y0xsPy3YgIHYUCP3ZKVHO">F.transpose</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.t&amp;sa=D&amp;source=editors&amp;ust=1615451627420000&amp;usg=AOvVaw2ObxLqQMJP1N28zKGHCx9b">torch.t</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c7">Use </span><span class="c11 c23 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.t%2520/%2520Tensor.permute&amp;sa=D&amp;source=editors&amp;ust=1615451627421000&amp;usg=AOvVaw3845KzQ1sR47doJZDDc49P">Tensor.permute</a></span><span class="c7">&nbsp;or </span><span class="c11 c23 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.t&amp;sa=D&amp;source=editors&amp;ust=1615451627421000&amp;usg=AOvVaw05iE3xWzByw0rq53tLKNil">torch.t</a></span><span class="c5">&nbsp;for no axes version</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.transpose_sequence.html%23chainer.functions.transpose_sequence&amp;sa=D&amp;source=editors&amp;ust=1615451627422000&amp;usg=AOvVaw3tP5NQX75JlWPqF1qNcZ8M">F.transpose_sequence</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">N/A</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.vstack.html%23chainer.functions.vstack&amp;sa=D&amp;source=editors&amp;ust=1615451627423000&amp;usg=AOvVaw1mB4znwfgJPW2t55pfuyir">F.vstack</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">Rewrite as:</span></p><p class="c2"><span class="c11 c23 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.cat&amp;sa=D&amp;source=editors&amp;ust=1615451627424000&amp;usg=AOvVaw07OTWJIf6TTUPzbtJtPY3B">torch.cat</a></span><span class="c32 c23 c7">([a,b],dim=0)</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.where.html%23chainer.functions.where&amp;sa=D&amp;source=editors&amp;ust=1615451627424000&amp;usg=AOvVaw2vgl-RPhfT44g-5bxuL-SF">F.where</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.where&amp;sa=D&amp;source=editors&amp;ust=1615451627425000&amp;usg=AOvVaw3Y1RstFYOagdV6Q_e_9O4j">torch.where</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c84"><td class="c59" colspan="3" rowspan="1"><p class="c2"><span class="c7 c36">Neural network connections</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.bilinear.html%23chainer.functions.bilinear&amp;sa=D&amp;source=editors&amp;ust=1615451627426000&amp;usg=AOvVaw1N_wQJe5BdcHlZnDC3GFR6">F.bilinear</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.bilinear&amp;sa=D&amp;source=editors&amp;ust=1615451627427000&amp;usg=AOvVaw3zBkRs30-PDA4L1OmkAmh3">F.bilinear</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.convolution_1d.html%23chainer.functions.convolution_1d&amp;sa=D&amp;source=editors&amp;ust=1615451627428000&amp;usg=AOvVaw1R45i2R5QQIJt0l0BPsJYm">F.convolution_1d</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.conv1d&amp;sa=D&amp;source=editors&amp;ust=1615451627428000&amp;usg=AOvVaw3QrLd7rwbUoypbk0FCBX1S">F.conv1d</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">No `cover_all`.</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.convolution_2d.html%23chainer.functions.convolution_2d&amp;sa=D&amp;source=editors&amp;ust=1615451627429000&amp;usg=AOvVaw2dSFmBs8trJBPBI7GwD-pl">F.convolution_2d</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.conv2d&amp;sa=D&amp;source=editors&amp;ust=1615451627430000&amp;usg=AOvVaw11lIjAEoB017LX5Nbbl9ru">F.conv2d</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">No `cover_all`.</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.convolution_3d.html%23chainer.functions.convolution_3d&amp;sa=D&amp;source=editors&amp;ust=1615451627430000&amp;usg=AOvVaw3em9rxeX0Cph8if5uzYmWk">F.convolution_3d</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.conv3d&amp;sa=D&amp;source=editors&amp;ust=1615451627431000&amp;usg=AOvVaw1VacFC9yxBlUe_SUmylvVg">F.conv3d</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">No `cover_all`.</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.convolution_nd.html%23chainer.functions.convolution_nd&amp;sa=D&amp;source=editors&amp;ust=1615451627432000&amp;usg=AOvVaw0MtLXS2u61i05KetaOsoV5">F.convolution_nd</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c7">See </span><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://discuss.pytorch.org/t/is-there-a-way-to-realize-4d-convolution-using-the-convnd-function/5999&amp;sa=D&amp;source=editors&amp;ust=1615451627433000&amp;usg=AOvVaw0p8TeKdx72S-jsiQkQXGas">https://discuss.pytorch.org/t/is-there-a-way-to-realize-4d-convolution-using-the-convnd-function/5999</a></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.deconvolution_1d.html%23chainer.functions.deconvolution_1d&amp;sa=D&amp;source=editors&amp;ust=1615451627434000&amp;usg=AOvVaw2h2m0YkTIRpiWQK1Nnd2Ez">F.deconvolution_1d</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.conv_transpose1d&amp;sa=D&amp;source=editors&amp;ust=1615451627434000&amp;usg=AOvVaw2f-sSt7VRN1x6EkuSvDLit">F.conv_transpose1d</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.deconvolution_2d.html%23chainer.functions.deconvolution_2d&amp;sa=D&amp;source=editors&amp;ust=1615451627435000&amp;usg=AOvVaw3pE5o2m3pNiX_sM0Ruku13">F.deconvolution_2d</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.conv_transpose2d&amp;sa=D&amp;source=editors&amp;ust=1615451627436000&amp;usg=AOvVaw3_UMOcZjT3I78Wszp0fk9f">F.conv_transpose2d</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.deconvolution_3d.html%23chainer.functions.deconvolution_3d&amp;sa=D&amp;source=editors&amp;ust=1615451627437000&amp;usg=AOvVaw1CVW3ONPsgVrL7f41nILNw">F.deconvolution_3d</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.conv_transpose3d&amp;sa=D&amp;source=editors&amp;ust=1615451627438000&amp;usg=AOvVaw0a-bR3f7cfR88GGwfdt9CR">F.conv_transpose3d</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c34"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.deconvolution_nd.html%23chainer.functions.deconvolution_nd&amp;sa=D&amp;source=editors&amp;ust=1615451627439000&amp;usg=AOvVaw3q5XWv4KMGxOHxO2UpfO7Z">F.deconvolution_nd</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c7">N/A, see </span><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://discuss.pytorch.org/t/is-there-a-way-to-realize-4d-convolution-using-the-convnd-function/5999&amp;sa=D&amp;source=editors&amp;ust=1615451627439000&amp;usg=AOvVaw18b6Rjn-Z1edlsu3eo-wgM">https://discuss.pytorch.org/t/is-there-a-way-to-realize-4d-convolution-using-the-convnd-function/5999</a></span></p></td></tr><tr class="c34"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.depthwise_convolution_2d.html%23chainer.functions.depthwise_convolution_2d&amp;sa=D&amp;source=editors&amp;ust=1615451627440000&amp;usg=AOvVaw3m_K6vVM2X0wDvBU-mJVFy">F.depthwise_convolution_2d</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.conv2d&amp;sa=D&amp;source=editors&amp;ust=1615451627440000&amp;usg=AOvVaw3cgmLJZnccuXefipUm7KcM">F.conv2d</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c7">Use `groups` argument; see </span><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://discuss.pytorch.org/t/depthwise-and-separable-convolutions-in-pytorch/7315/2&amp;sa=D&amp;source=editors&amp;ust=1615451627441000&amp;usg=AOvVaw1j5oUmoSwAyo-dBPR0QLnT">https://discuss.pytorch.org/t/depthwise-and-separable-convolutions-in-pytorch/7315/2</a></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.deformable_convolution_2d_sampler.html%23chainer.functions.deformable_convolution_2d_sampler&amp;sa=D&amp;source=editors&amp;ust=1615451627442000&amp;usg=AOvVaw3tfJTwWuwr2Evuka1DFGio">F.deformable_convolution_2d_sampler</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c7">Not implemented: </span><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://github.com/pytorch/pytorch/issues/2260&amp;sa=D&amp;source=editors&amp;ust=1615451627443000&amp;usg=AOvVaw07udTSrb5udj8j-G2PhPa-">https://github.com/pytorch/pytorch/issues/2260</a></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.dilated_convolution_2d.html%23chainer.functions.dilated_convolution_2d&amp;sa=D&amp;source=editors&amp;ust=1615451627443000&amp;usg=AOvVaw11uK33CbSKXPbYClckXcsr">F.dilated_convolution_2d</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.conv2d&amp;sa=D&amp;source=editors&amp;ust=1615451627444000&amp;usg=AOvVaw1Qpz8CxVNAD0YPrsK2Jl0E">F.conv2d</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">Use `dilation` argument.</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.embed_id.html%23chainer.functions.embed_id&amp;sa=D&amp;source=editors&amp;ust=1615451627445000&amp;usg=AOvVaw1Z8d1q6rw1WCL6VJjZuhQ8">F.embed_id</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.embedding&amp;sa=D&amp;source=editors&amp;ust=1615451627445000&amp;usg=AOvVaw0PF0-P2OH1K4ZjBH_K-zkB">F.embedding</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.linear.html%23chainer.functions.linear&amp;sa=D&amp;source=editors&amp;ust=1615451627446000&amp;usg=AOvVaw31OoK7T_cLLih-kN82EPNi">F.linear</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.linear&amp;sa=D&amp;source=editors&amp;ust=1615451627447000&amp;usg=AOvVaw0Cbf2n41TO7z0bulePiJuF">F.linear</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">There is no option for `n_batch_axes`.</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.local_convolution_2d.html%23chainer.functions.local_convolution_2d&amp;sa=D&amp;source=editors&amp;ust=1615451627448000&amp;usg=AOvVaw3ER7nIXOkKsgKcOyzZcRGS">F.local_convolution_2d</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c7">See </span><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://github.com/pytorch/pytorch/pull/1583&amp;sa=D&amp;source=editors&amp;ust=1615451627448000&amp;usg=AOvVaw30Eo3-lUfPDKextOygqnXJ">https://github.com/pytorch/pytorch/pull/1583</a></span></p></td></tr><tr class="c34"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.n_step_bigru.html%23chainer.functions.n_step_bigru&amp;sa=D&amp;source=editors&amp;ust=1615451627449000&amp;usg=AOvVaw3qzde7EpgCCPij5_F07Gsz">F.n_step_bigru</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c7">Undocumented _C._VariableFunctions function torch.gru? The &quot;link&quot; is available </span><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.html%23torch.nn.GRU&amp;sa=D&amp;source=editors&amp;ust=1615451627450000&amp;usg=AOvVaw2swXdH1Btx94Ud_lXGZsPc">https://pytorch.org/docs/stable/nn.html#torch.nn.GRU</a></span><span class="c5">&nbsp;and this is probably the expected usage.</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.n_step_bilstm.html%23chainer.functions.n_step_bilstm&amp;sa=D&amp;source=editors&amp;ust=1615451627450000&amp;usg=AOvVaw3oc3yJ75UvzOADqhNN3V6h">F.n_step_bilstm</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">See L.NStepBiLSTM.</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.n_step_birnn.html%23chainer.functions.n_step_birnn&amp;sa=D&amp;source=editors&amp;ust=1615451627452000&amp;usg=AOvVaw37PjFSZiSkdLtX_pZhBnb5">F.n_step_birnn</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">See L.NStepBiRNNTanh or L.NStepBiRNNReLU.</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.n_step_gru.html%23chainer.functions.n_step_gru&amp;sa=D&amp;source=editors&amp;ust=1615451627453000&amp;usg=AOvVaw1xRJV43vtlP-5QnWHxCciR">F.n_step_gru</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">See L.NStepBiGRU.</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.n_step_lstm.html%23chainer.functions.n_step_lstm&amp;sa=D&amp;source=editors&amp;ust=1615451627455000&amp;usg=AOvVaw1OIRZSsVMPqp__5N1fcIda">F.n_step_lstm</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">See L.NStepLSTM.</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.n_step_rnn.html%23chainer.functions.n_step_rnn&amp;sa=D&amp;source=editors&amp;ust=1615451627456000&amp;usg=AOvVaw1AZ-nxZTZqvh_09-lioB7m">F.n_step_rnn</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">See L.NStepRNNTanh or L.NStepRNNReLU.</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.shift.html%23chainer.functions.shift&amp;sa=D&amp;source=editors&amp;ust=1615451627458000&amp;usg=AOvVaw1CRzHx0V8juMgUEwlN-4Eq">F.shift</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c7">See </span><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://github.com/pytorch/pytorch/issues/16408&amp;sa=D&amp;source=editors&amp;ust=1615451627460000&amp;usg=AOvVaw0RBejFQ0EEwmmAu0v58ESY">https://github.com/pytorch/pytorch/issues/16408</a></span></p></td></tr><tr class="c12"><td class="c59" colspan="3" rowspan="1"><p class="c2"><span class="c7 c36">Evaluation functions</span></p></td></tr><tr class="c34"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.accuracy.html%23chainer.functions.accuracy&amp;sa=D&amp;source=editors&amp;ust=1615451627462000&amp;usg=AOvVaw1lv3utPzYAtSUgqirHOX-R">F.accuracy</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c7">N/A, Ignite has an implementation: </span><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/ignite/metrics.html%23ignite.metrics.Accuracy&amp;sa=D&amp;source=editors&amp;ust=1615451627463000&amp;usg=AOvVaw0vCJ14ilX1yX-LM4n8s0Fg">https://pytorch.org/ignite/metrics.html#ignite.metrics.Accuracy</a></span></p></td></tr><tr class="c34"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.binary_accuracy.html%23chainer.functions.binary_accuracy&amp;sa=D&amp;source=editors&amp;ust=1615451627464000&amp;usg=AOvVaw1xZmRg_p7Am5b0kpfoD-xb">F.binary_accuracy</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c7">N/A, Ignite has an implementation: </span><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/ignite/metrics.html%23ignite.metrics.Accuracy&amp;sa=D&amp;source=editors&amp;ust=1615451627465000&amp;usg=AOvVaw0n1BpyHFymuZ51x8si4Szh">https://pytorch.org/ignite/metrics.html#ignite.metrics.Accuracy</a></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.classification_summary.html%23chainer.functions.classification_summary&amp;sa=D&amp;source=editors&amp;ust=1615451627466000&amp;usg=AOvVaw3RiwtypcGa9UX33xzZcpOa">F.classification_summary</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">N/A</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.f1_score.html%23chainer.functions.f1_score&amp;sa=D&amp;source=editors&amp;ust=1615451627468000&amp;usg=AOvVaw2VzE6VD4FVryJSh9uZmApX">F.f1_score</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c7">See </span><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://discuss.pytorch.org/t/calculating-precision-recall-and-f1-score-in-case-of-multi-label-classification/28265/1&amp;sa=D&amp;source=editors&amp;ust=1615451627469000&amp;usg=AOvVaw1mjHlUojwvKWgUDzFUCLWi">https://discuss.pytorch.org/t/calculating-precision-recall-and-f1-score-in-case-of-multi-label-classification/28265/1</a></span></p></td></tr><tr class="c34"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.precision.html%23chainer.functions.precision&amp;sa=D&amp;source=editors&amp;ust=1615451627469000&amp;usg=AOvVaw35Eiy7-GMta2GKwppWeVOP">F.precision</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c7">N/A, Ignite has an implementation: </span><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/ignite/metrics.html%23ignite.metrics.Precision&amp;sa=D&amp;source=editors&amp;ust=1615451627470000&amp;usg=AOvVaw1T4D9S0SQUobBr6A3oJiEe">https://pytorch.org/ignite/metrics.html#ignite.metrics.Precision</a></span></p></td></tr><tr class="c34"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.r2_score.html%23chainer.functions.r2_score&amp;sa=D&amp;source=editors&amp;ust=1615451627471000&amp;usg=AOvVaw15GRaa_vtXrYbu2tq0Z4EZ">F.r2_score</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c7">Not available. It&#39;s an evaluation metric that&#39;s not differentiable. It&#39;s implemented in Ignite though and could be used (as a reference) </span><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://github.com/pytorch/ignite/pull/496&amp;sa=D&amp;source=editors&amp;ust=1615451627472000&amp;usg=AOvVaw2iDM1VhyomtEQHDBVsR1w6">https://github.com/pytorch/ignite/pull/496</a></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.recall.html%23chainer.functions.recall&amp;sa=D&amp;source=editors&amp;ust=1615451627472000&amp;usg=AOvVaw1w1-jJxu4aNI9ZE5v6VUPV">F.recall</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c7">N/A, Ignite has an implementation: </span><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/ignite/metrics.html%23ignite.metrics.Recall&amp;sa=D&amp;source=editors&amp;ust=1615451627473000&amp;usg=AOvVaw2o9qLZ6MO-PfY1iVLfKdM0">https://pytorch.org/ignite/metrics.html#ignite.metrics.Recall</a></span></p></td></tr><tr class="c12"><td class="c59" colspan="3" rowspan="1"><p class="c2"><span class="c7 c36">Loss functions</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.absolute_error.html%23chainer.functions.absolute_error&amp;sa=D&amp;source=editors&amp;ust=1615451627474000&amp;usg=AOvVaw0j04LdaqBnL2UHNanR7zo0">F.absolute_error</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">N/A</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.bernoulli_nll.html%23chainer.functions.bernoulli_nll&amp;sa=D&amp;source=editors&amp;ust=1615451627476000&amp;usg=AOvVaw2JI0MbmO3kPeyLs0GO6fXM">F.bernoulli_nll</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">Possibly: -torch.distributions.Bernoulli(y).log_prob(x).sum()</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.black_out.html%23chainer.functions.black_out&amp;sa=D&amp;source=editors&amp;ust=1615451627477000&amp;usg=AOvVaw2GhGAXvQCfUNWKXaFCFazK">F.black_out</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.connectionist_temporal_classification.html%23chainer.functions.connectionist_temporal_classification&amp;sa=D&amp;source=editors&amp;ust=1615451627479000&amp;usg=AOvVaw0JjsGWlwpzJdcXMVelLTIr">F.connectionist_temporal_classification</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.ctc_loss&amp;sa=D&amp;source=editors&amp;ust=1615451627480000&amp;usg=AOvVaw2tCbOFCnhdWbkWqq3ettFq">F.ctc_loss</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.contrastive.html%23chainer.functions.contrastive&amp;sa=D&amp;source=editors&amp;ust=1615451627481000&amp;usg=AOvVaw38KnIHms0uRe7nJO2gSS32">F.contrastive</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c7">See </span><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://github.com/adambielski/siamese-triplet&amp;sa=D&amp;source=editors&amp;ust=1615451627482000&amp;usg=AOvVaw3AqZLqK-5iIBf6tBinwnq6">https://github.com/adambielski/siamese-triplet</a></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.crf1d.html%23chainer.functions.crf1d&amp;sa=D&amp;source=editors&amp;ust=1615451627483000&amp;usg=AOvVaw2miIT6G3Vy-kAWNWOF-C2J">F.crf1d</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c7">Not available: </span><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://github.com/pytorch/pytorch/issues/11134&amp;sa=D&amp;source=editors&amp;ust=1615451627484000&amp;usg=AOvVaw2y8K9pkgs0yjfD6u_G_lDp">https://github.com/pytorch/pytorch/issues/11134</a></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.argmax_crf1d.html%23chainer.functions.argmax_crf1d&amp;sa=D&amp;source=editors&amp;ust=1615451627485000&amp;usg=AOvVaw1XfoVU_Yx9fSHs7oVDuMld">F.argmax_crf1d</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c7">Not available: </span><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://github.com/pytorch/pytorch/issues/11134&amp;sa=D&amp;source=editors&amp;ust=1615451627485000&amp;usg=AOvVaw0x-FvKMqIT4msQu6y4W_T5">https://github.com/pytorch/pytorch/issues/11134</a></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.cross_covariance.html%23chainer.functions.cross_covariance&amp;sa=D&amp;source=editors&amp;ust=1615451627486000&amp;usg=AOvVaw0EN1nepeAFv4tzq-YXoiGS">F.cross_covariance</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.decov.html%23chainer.functions.decov&amp;sa=D&amp;source=editors&amp;ust=1615451627488000&amp;usg=AOvVaw1wm8kdi9dcnIKxYnk4yBPU">F.decov</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.discriminative_margin_based_clustering_loss.html%23chainer.functions.discriminative_margin_based_clustering_loss&amp;sa=D&amp;source=editors&amp;ust=1615451627489000&amp;usg=AOvVaw2M0u3JBmjprbuDNqteol6A">F.discriminative_margin_based_clustering_loss</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c7">Not available. See </span><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://github.com/Wizaron/instance-segmentation-pytorch&amp;sa=D&amp;source=editors&amp;ust=1615451627490000&amp;usg=AOvVaw3KiBJ_p5gwjorqeWPn1Qf6">https://github.com/Wizaron/instance-segmentation-pytorch</a></span><span class="c5">&nbsp;for a reproducing work.</span></p></td></tr><tr class="c34"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.gaussian_kl_divergence.html%23chainer.functions.gaussian_kl_divergence&amp;sa=D&amp;source=editors&amp;ust=1615451627491000&amp;usg=AOvVaw2RgYwj3uQS37YpgLYpKjyv">F.gaussian_kl_divergence</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">N/A</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.gaussian_nll.html%23chainer.functions.gaussian_nll&amp;sa=D&amp;source=editors&amp;ust=1615451627492000&amp;usg=AOvVaw15IMPGEOUcuIE0FXMer3UE">F.gaussian_nll</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">N/A</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.hinge.html%23chainer.functions.hinge&amp;sa=D&amp;source=editors&amp;ust=1615451627494000&amp;usg=AOvVaw0kEjnVEIfRGgRcOAG5X8Jh">F.hinge</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23hinge-embedding-loss&amp;sa=D&amp;source=editors&amp;ust=1615451627495000&amp;usg=AOvVaw3SkgdmJ6aZN4DmvK9tifK4">F.hinge_embedding_loss</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.huber_loss.html%23chainer.functions.huber_loss&amp;sa=D&amp;source=editors&amp;ust=1615451627496000&amp;usg=AOvVaw28sh-Igb8HuUuil6qHcFxY">F.huber_loss</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.smooth_l1_loss&amp;sa=D&amp;source=editors&amp;ust=1615451627497000&amp;usg=AOvVaw1lExPx01t9i1XWGDAvP2gq">F.smooth_l1_loss</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">Use reduction=&#39;sum&#39; to keep reduction method</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.mean_absolute_error.html%23chainer.functions.mean_absolute_error&amp;sa=D&amp;source=editors&amp;ust=1615451627498000&amp;usg=AOvVaw0g0jpLaAB-h4nZRIdJYpO1">F.mean_absolute_error</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.l1_loss&amp;sa=D&amp;source=editors&amp;ust=1615451627498000&amp;usg=AOvVaw39MGn1I0U6w73iAp6oFaqE">F.l1_loss</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">See also: ignite.metrics.MeanAbsoluteError</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.mean_squared_error.html%23chainer.functions.mean_squared_error&amp;sa=D&amp;source=editors&amp;ust=1615451627499000&amp;usg=AOvVaw29EN9m3O1RChFijcTjqQuk">F.mean_squared_error</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.mse_loss&amp;sa=D&amp;source=editors&amp;ust=1615451627500000&amp;usg=AOvVaw2RACET9Pi7Ssm8g87QhVc8">F.mse_loss</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.negative_sampling.html%23chainer.functions.negative_sampling&amp;sa=D&amp;source=editors&amp;ust=1615451627501000&amp;usg=AOvVaw1HGLyDO43prUfhzNXm29Ny">F.negative_sampling</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c7">See </span><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://github.com/kefirski/pytorch_NEG_loss&amp;sa=D&amp;source=editors&amp;ust=1615451627502000&amp;usg=AOvVaw3dpASqUOE6RHsQhSCjMhDk">https://github.com/kefirski/pytorch_NEG_loss</a></span><span class="c7">, </span><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://github.com/theeluwin/pytorch-sgns&amp;sa=D&amp;source=editors&amp;ust=1615451627502000&amp;usg=AOvVaw2SnU7YMHrOjkpTtHrjrXrh">https://github.com/theeluwin/pytorch-sgns</a></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.sigmoid_cross_entropy.html%23chainer.functions.sigmoid_cross_entropy&amp;sa=D&amp;source=editors&amp;ust=1615451627503000&amp;usg=AOvVaw1tXfmwCXkSnTLyFZ7H16VF">F.sigmoid_cross_entropy</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.binary_cross_entropy_with_logits&amp;sa=D&amp;source=editors&amp;ust=1615451627504000&amp;usg=AOvVaw14rY1kwwKOl6VRrzhuXQo-">F.binary_cross_entropy_with_logits</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.softmax_cross_entropy.html%23chainer.functions.softmax_cross_entropy&amp;sa=D&amp;source=editors&amp;ust=1615451627505000&amp;usg=AOvVaw1KnuLl0owV-MTtS4I20P1h">F.softmax_cross_entropy</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.html%23torch.nn.CrossEntropyLoss&amp;sa=D&amp;source=editors&amp;ust=1615451627506000&amp;usg=AOvVaw0mxibe-K4TKp8MFTDdOxnT">nn.CrossEntropyLoss</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.squared_error.html%23chainer.functions.squared_error&amp;sa=D&amp;source=editors&amp;ust=1615451627507000&amp;usg=AOvVaw3WmoxMG2S8ujNLKz57NXEK">F.squared_error</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.triplet.html%23chainer.functions.triplet&amp;sa=D&amp;source=editors&amp;ust=1615451627509000&amp;usg=AOvVaw1TCLEaJTl4JjnDDNbC9y2P">F.triplet</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.triplet_margin_loss&amp;sa=D&amp;source=editors&amp;ust=1615451627510000&amp;usg=AOvVaw014AZorl8IDy0a0AMDgOJS">F.triplet_margin_loss</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c59" colspan="3" rowspan="1"><p class="c2"><span class="c7 c36">Mathematical functions</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.absolute.html%23chainer.functions.absolute&amp;sa=D&amp;source=editors&amp;ust=1615451627513000&amp;usg=AOvVaw0W_-D3HaHxzr8mYyaMb3e7">F.absolute</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.abs&amp;sa=D&amp;source=editors&amp;ust=1615451627514000&amp;usg=AOvVaw3L22WUKGn9nh29aADEDHI6">torch.abs</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.arccos.html%23chainer.functions.arccos&amp;sa=D&amp;source=editors&amp;ust=1615451627515000&amp;usg=AOvVaw21KHWWCJKHN20A4BR8qcLz">F.arccos</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.acos&amp;sa=D&amp;source=editors&amp;ust=1615451627516000&amp;usg=AOvVaw1X2nouLigMrW7GXJYUXqDj">torch.acos</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.arcsin.html%23chainer.functions.arcsin&amp;sa=D&amp;source=editors&amp;ust=1615451627517000&amp;usg=AOvVaw1SH1wtt7PbCfCDp0PEreqg">F.arcsin</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.asin&amp;sa=D&amp;source=editors&amp;ust=1615451627518000&amp;usg=AOvVaw2OE0ivJZJ2sXNJYZ9Z3eRh">torch.asin</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.arctan.html%23chainer.functions.arctan&amp;sa=D&amp;source=editors&amp;ust=1615451627518000&amp;usg=AOvVaw1TQmxoNovlhWruIz1CM7fj">F.arctan</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.atan&amp;sa=D&amp;source=editors&amp;ust=1615451627519000&amp;usg=AOvVaw2r-n7dL5BAZE3H99wHYHw-">torch.atan</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.arctan2.html%23chainer.functions.arctan2&amp;sa=D&amp;source=editors&amp;ust=1615451627520000&amp;usg=AOvVaw2bmicu25HxEwJ_h9wKbQa9">F.arctan2</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.atan2&amp;sa=D&amp;source=editors&amp;ust=1615451627521000&amp;usg=AOvVaw3nOylWkT1C8Jbl9x_ENNov">torch.atan2</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.arctanh.html%23chainer.functions.arctanh&amp;sa=D&amp;source=editors&amp;ust=1615451627523000&amp;usg=AOvVaw1Nk1x3VFPXmic9jhjZHL_D">F.arctanh</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c7">N/A: </span><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://github.com/pytorch/pytorch/issues/10324&amp;sa=D&amp;source=editors&amp;ust=1615451627524000&amp;usg=AOvVaw0oNDeX0PoG8U-Z08RlcbWY">https://github.com/pytorch/pytorch/issues/10324</a></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.argmax.html%23chainer.functions.argmax&amp;sa=D&amp;source=editors&amp;ust=1615451627524000&amp;usg=AOvVaw0Wmjb60Mhw-qG6kwjehoTD">F.argmax</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.argmax&amp;sa=D&amp;source=editors&amp;ust=1615451627525000&amp;usg=AOvVaw3Ab4D-mabcfN73um7Yc3GE">torch.argmax</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.argmin.html%23chainer.functions.argmin&amp;sa=D&amp;source=editors&amp;ust=1615451627527000&amp;usg=AOvVaw3LSIkHz1yX1HkmTl6Zu-JT">F.argmin</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.argmin&amp;sa=D&amp;source=editors&amp;ust=1615451627528000&amp;usg=AOvVaw38DF-kzfz8HkTfhubt519h">torch.argmin</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.average.html%23chainer.functions.average&amp;sa=D&amp;source=editors&amp;ust=1615451627529000&amp;usg=AOvVaw0Qu1VNfU9NWWGr8JWXg8cZ">F.average</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">See F.mean.</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.batch_inv.html%23chainer.functions.batch_inv&amp;sa=D&amp;source=editors&amp;ust=1615451627531000&amp;usg=AOvVaw0bXdCxA7J5ezfuyL-N1ljM">F.batch_inv</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.inverse&amp;sa=D&amp;source=editors&amp;ust=1615451627531000&amp;usg=AOvVaw1OZQPvco5Y5rtRCSCU1aln">torch.inverse</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">linalg ops batch is on progress, inverse is already merged</span></p></td></tr><tr class="c34"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.batch_l2_norm_squared.html%23chainer.functions.batch_l2_norm_squared&amp;sa=D&amp;source=editors&amp;ust=1615451627532000&amp;usg=AOvVaw0CSxZqXzVe1Z0zSzX6mprt">F.batch_l2_norm_squared</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">Rewrite as:</span></p><p class="c2"><span class="c32 c23 c7">x.reshape(len(x), -1).norm(dim=1) ** 2</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.batch_matmul.html%23chainer.functions.batch_matmul&amp;sa=D&amp;source=editors&amp;ust=1615451627534000&amp;usg=AOvVaw0sP_6H4AYJvkoaSzT8XHFf">F.batch_matmul</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.matmul&amp;sa=D&amp;source=editors&amp;ust=1615451627535000&amp;usg=AOvVaw27JjPoW3It1z6QxGn-Fcjv">torch.matmul</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c34"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.bias.html%23chainer.functions.bias&amp;sa=D&amp;source=editors&amp;ust=1615451627536000&amp;usg=AOvVaw3RVwuRz6j9ubyOezbJBBVG">F.bias</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">Rewrite as</span></p><p class="c2"><span class="c23 c7 c32">x + y[(...,) + (None,) * (x.ndim - y.ndim - axis)]</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.ceil.html%23chainer.functions.ceil&amp;sa=D&amp;source=editors&amp;ust=1615451627538000&amp;usg=AOvVaw0_NJrh-CjppN4eYHdbP9V6">F.ceil</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.ceil&amp;sa=D&amp;source=editors&amp;ust=1615451627539000&amp;usg=AOvVaw1M8-IGo76GVVhKJIJGuHuy">torch.ceil</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.clip.html%23chainer.functions.clip&amp;sa=D&amp;source=editors&amp;ust=1615451627540000&amp;usg=AOvVaw2n7jFaz57KpNCdSLakjTkQ">F.clip</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.clamp&amp;sa=D&amp;source=editors&amp;ust=1615451627541000&amp;usg=AOvVaw3lU65TO1hkutNMFyJ7V6DS">torch.clamp</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.cos.html%23chainer.functions.cos&amp;sa=D&amp;source=editors&amp;ust=1615451627542000&amp;usg=AOvVaw1eorXpKzWvPXii3gzgemFZ">F.cos</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.cos&amp;sa=D&amp;source=editors&amp;ust=1615451627543000&amp;usg=AOvVaw3Z91kK29y2Hf_3kqRXq8jn">torch.cos</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.cosh.html%23chainer.functions.cosh&amp;sa=D&amp;source=editors&amp;ust=1615451627544000&amp;usg=AOvVaw0xP8OtNmlP6HoJ_wwpMtiK">F.cosh</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.cosh&amp;sa=D&amp;source=editors&amp;ust=1615451627545000&amp;usg=AOvVaw25rH4Rqs3YhB_2CUUekbaR">torch.cosh</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.cumprod.html%23chainer.functions.cumprod&amp;sa=D&amp;source=editors&amp;ust=1615451627546000&amp;usg=AOvVaw2UkSRLIom_C-bJX04jih64">F.cumprod</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.cumprod&amp;sa=D&amp;source=editors&amp;ust=1615451627546000&amp;usg=AOvVaw1nTv2FuW0JtO_qD21wOfvV">torch.cumprod</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.cumsum.html%23chainer.functions.cumsum&amp;sa=D&amp;source=editors&amp;ust=1615451627548000&amp;usg=AOvVaw2sBvWEOlwfCL_8niN9vyNY">F.cumsum</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.cumsum&amp;sa=D&amp;source=editors&amp;ust=1615451627548000&amp;usg=AOvVaw08rAiYYdGHForzGQY42EEs">torch.cumsum</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.det.html%23chainer.functions.det&amp;sa=D&amp;source=editors&amp;ust=1615451627550000&amp;usg=AOvVaw0_Hh4WRgWczdQm6fPKqlbV">F.det</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.det&amp;sa=D&amp;source=editors&amp;ust=1615451627550000&amp;usg=AOvVaw13HxqKbmmxFKS0bAmZe-o6">torch.det</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.batch_det.html%23chainer.functions.batch_det&amp;sa=D&amp;source=editors&amp;ust=1615451627551000&amp;usg=AOvVaw0MiqZYMzNyOMlJLF1BMdy-">F.batch_det</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.det&amp;sa=D&amp;source=editors&amp;ust=1615451627552000&amp;usg=AOvVaw1YBgLHisIYWIrWbCl6u8k1">torch.det</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">Arbitrary number of batch axes are supported.</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.digamma.html%23chainer.functions.digamma&amp;sa=D&amp;source=editors&amp;ust=1615451627554000&amp;usg=AOvVaw1vwLpLGzvwjLUCCH6dpHVe">F.digamma</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.digamma&amp;sa=D&amp;source=editors&amp;ust=1615451627555000&amp;usg=AOvVaw3AheUrZ4ZmPi4gqh5iylay">torch.digamma</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.einsum.html%23chainer.functions.einsum&amp;sa=D&amp;source=editors&amp;ust=1615451627556000&amp;usg=AOvVaw0oSVxlifDllBYliSd3INQk">F.einsum</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.einsum&amp;sa=D&amp;source=editors&amp;ust=1615451627557000&amp;usg=AOvVaw1LRVpfum1gryyHSBr14qO8">torch.einsum</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.erf.html%23chainer.functions.erf&amp;sa=D&amp;source=editors&amp;ust=1615451627558000&amp;usg=AOvVaw0Ntwa2fRl8DqT5pAP0Asdk">F.erf</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.erf&amp;sa=D&amp;source=editors&amp;ust=1615451627559000&amp;usg=AOvVaw2mGgloE1hSx22pblBWS-R-">torch.erf</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.erfc.html%23chainer.functions.erfc&amp;sa=D&amp;source=editors&amp;ust=1615451627560000&amp;usg=AOvVaw1lgkwMo6cmvktR8_MXmlUH">F.erfc</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.erfc&amp;sa=D&amp;source=editors&amp;ust=1615451627561000&amp;usg=AOvVaw37bHc_OW8GtwPKvfRlkKIc">torch.erfc</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.erfcinv.html%23chainer.functions.erfcinv&amp;sa=D&amp;source=editors&amp;ust=1615451627563000&amp;usg=AOvVaw03xD2VJcl7GIhQNsV3TmjV">F.erfcinv</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c7">Not available. Implement it similar to erf, erfinv? </span><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://github.com/pytorch/pytorch/pull/2799&amp;sa=D&amp;source=editors&amp;ust=1615451627564000&amp;usg=AOvVaw1n9Kv0j00y6S9_Ua0eZ4h7">https://github.com/pytorch/pytorch/pull/2799</a></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.erfcx.html%23chainer.functions.erfcx&amp;sa=D&amp;source=editors&amp;ust=1615451627565000&amp;usg=AOvVaw2jL-UyFJ-mUlXgy5Pw_Cj6">F.erfcx</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">N/A</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.erfinv.html%23chainer.functions.erfinv&amp;sa=D&amp;source=editors&amp;ust=1615451627566000&amp;usg=AOvVaw2zmC5fm1DDq5HxIKk0qB97">F.erfinv</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.erfinv&amp;sa=D&amp;source=editors&amp;ust=1615451627567000&amp;usg=AOvVaw25leMulyP23wyxBJcVsdQE">torch.erfinv</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.exp.html%23chainer.functions.exp&amp;sa=D&amp;source=editors&amp;ust=1615451627568000&amp;usg=AOvVaw3yZ2ganSVcxuvDTqx32OKT">F.exp</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.exp&amp;sa=D&amp;source=editors&amp;ust=1615451627568000&amp;usg=AOvVaw3Kz87mKz9BAr9BfBXugauF">torch.exp</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.expm1.html%23chainer.functions.expm1&amp;sa=D&amp;source=editors&amp;ust=1615451627569000&amp;usg=AOvVaw3k21e33fXoc8h0mCKLA33B">F.expm1</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.expm1&amp;sa=D&amp;source=editors&amp;ust=1615451627570000&amp;usg=AOvVaw2WkdNNog1QKyqRtpB9q1KR">torch.expm1</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.fft.html%23chainer.functions.fft&amp;sa=D&amp;source=editors&amp;ust=1615451627571000&amp;usg=AOvVaw1QCqvlsYnRubE6nYrfnEh1">F.fft</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.fft&amp;sa=D&amp;source=editors&amp;ust=1615451627572000&amp;usg=AOvVaw2lfKlbSmEapsulnmhJCLy7">torch.fft</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">Interface is quite different.</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.fix.html%23chainer.functions.fix&amp;sa=D&amp;source=editors&amp;ust=1615451627573000&amp;usg=AOvVaw0KgmxYQhPxcmPQsyeKNMP6">F.fix</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">N/A</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.fmod.html%23chainer.functions.fmod&amp;sa=D&amp;source=editors&amp;ust=1615451627575000&amp;usg=AOvVaw1bz5XUvgUUFc-PqiSaBCrX">F.fmod</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.fmod&amp;sa=D&amp;source=editors&amp;ust=1615451627575000&amp;usg=AOvVaw1WyUt2yXoXsu8G7o5OHyH1">torch.fmod</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.floor.html%23chainer.functions.floor&amp;sa=D&amp;source=editors&amp;ust=1615451627577000&amp;usg=AOvVaw3IdmXGLKx_2n5TqZUrI7aR">F.floor</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.floor&amp;sa=D&amp;source=editors&amp;ust=1615451627578000&amp;usg=AOvVaw3mDkKcF4OU_2qfAyXz22s3">torch.floor</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.identity.html%23chainer.functions.identity&amp;sa=D&amp;source=editors&amp;ust=1615451627580000&amp;usg=AOvVaw3v6FU9vfYHnHONGEk1RpOT">F.identity</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.html%23torch.nn.Identity&amp;sa=D&amp;source=editors&amp;ust=1615451627581000&amp;usg=AOvVaw18jooN9H3bGBDUZi4mePKE">nn.Identity</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.ifft.html%23chainer.functions.ifft&amp;sa=D&amp;source=editors&amp;ust=1615451627582000&amp;usg=AOvVaw1FnLI38Q6C_cgN4j6QQiIU">F.ifft</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.ifft&amp;sa=D&amp;source=editors&amp;ust=1615451627582000&amp;usg=AOvVaw3Qlq5q_LxFu6Zcxb663zPT">torch.ifft</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.inv.html%23chainer.functions.inv&amp;sa=D&amp;source=editors&amp;ust=1615451627584000&amp;usg=AOvVaw1GVAXtdTXTJwvkwRNfH8tl">F.inv</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.inverse&amp;sa=D&amp;source=editors&amp;ust=1615451627584000&amp;usg=AOvVaw029BwTRBygrdub4DatosN1">torch.inverse</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.lgamma.html%23chainer.functions.lgamma&amp;sa=D&amp;source=editors&amp;ust=1615451627586000&amp;usg=AOvVaw2xH5nFvDtWbtS5lNa5DaYD">F.lgamma</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/cppdocs/api/function_namespaceat_1ae80021c749d42d0625e90973b18a00bc.html%23_CPPv4N2at6lgammaERK6Tensor&amp;sa=D&amp;source=editors&amp;ust=1615451627587000&amp;usg=AOvVaw0qNYIpyhGM49iYdkSr5NeK">torch.lgamma</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c7">Currently undocumented: </span><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://github.com/pytorch/pytorch/pull/27812&amp;sa=D&amp;source=editors&amp;ust=1615451627588000&amp;usg=AOvVaw1Xb4bUDury6hsrqSfpL_cF">https://github.com/pytorch/pytorch/pull/27812</a></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.linear_interpolate.html%23chainer.functions.linear_interpolate&amp;sa=D&amp;source=editors&amp;ust=1615451627589000&amp;usg=AOvVaw1m-w_yRZ3JFZsruHs89ZWK">F.linear_interpolate</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">Normal math should suffice.</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.log.html%23chainer.functions.log&amp;sa=D&amp;source=editors&amp;ust=1615451627590000&amp;usg=AOvVaw0UhhJ4qwQt-L4JPAZxR1VR">F.log</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.log&amp;sa=D&amp;source=editors&amp;ust=1615451627591000&amp;usg=AOvVaw0zvuBhLBmP_Jh5JmkoMmp7">torch.log</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.log10.html%23chainer.functions.log10&amp;sa=D&amp;source=editors&amp;ust=1615451627592000&amp;usg=AOvVaw3w4WZ5SIKv_DsHZsP2QcXT">F.log10</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.log10&amp;sa=D&amp;source=editors&amp;ust=1615451627593000&amp;usg=AOvVaw1ytMCHc5xUijmm-J3FzenT">torch.log10</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.log1p.html%23chainer.functions.log1p&amp;sa=D&amp;source=editors&amp;ust=1615451627595000&amp;usg=AOvVaw22Qss3br3S3xcIXCnTuR81">F.log1p</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.log1p&amp;sa=D&amp;source=editors&amp;ust=1615451627596000&amp;usg=AOvVaw3RRoYy4wp-87n4nYiERa-z">torch.log1p</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.log2.html%23chainer.functions.log2&amp;sa=D&amp;source=editors&amp;ust=1615451627597000&amp;usg=AOvVaw0KlfiOYSU0hGb4QhW1Y8Hr">F.log2</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.log2&amp;sa=D&amp;source=editors&amp;ust=1615451627598000&amp;usg=AOvVaw3ERlLvo456eYA6IbP2CbFs">torch.log2</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.log_ndtr.html%23chainer.functions.log_ndtr&amp;sa=D&amp;source=editors&amp;ust=1615451627599000&amp;usg=AOvVaw2ZWwzEF4jow6fxItLlwCE5">F.log_ndtr</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">N/A</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.logsumexp.html%23chainer.functions.logsumexp&amp;sa=D&amp;source=editors&amp;ust=1615451627600000&amp;usg=AOvVaw0NOkbLh3hgrg8Eqn6bF1AZ">F.logsumexp</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.logsumexp&amp;sa=D&amp;source=editors&amp;ust=1615451627601000&amp;usg=AOvVaw2VsCdARmZmJozo5LBWPIjX">torch.logsumexp</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.matmul.html%23chainer.functions.matmul&amp;sa=D&amp;source=editors&amp;ust=1615451627602000&amp;usg=AOvVaw1pLCOZ8YYahtM36ndzhsaf">F.matmul</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.matmul&amp;sa=D&amp;source=editors&amp;ust=1615451627603000&amp;usg=AOvVaw2tv0XWx1314dpxjU7PhoYy">torch.matmul</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.max.html%23chainer.functions.max&amp;sa=D&amp;source=editors&amp;ust=1615451627604000&amp;usg=AOvVaw2nR_IEM3Jo6_NPY9nTjVdA">F.max</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.max&amp;sa=D&amp;source=editors&amp;ust=1615451627605000&amp;usg=AOvVaw1ZbSv9R7OAmAa415MbjE6d">torch.max</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.maximum.html%23chainer.functions.maximum&amp;sa=D&amp;source=editors&amp;ust=1615451627606000&amp;usg=AOvVaw3D4zAWbhLTpNaYwvyWDUO8">F.maximum</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.max&amp;sa=D&amp;source=editors&amp;ust=1615451627607000&amp;usg=AOvVaw0-jUt0mimvcqNtTPGjSeT2">torch.max</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c34"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.mean.html%23chainer.functions.mean&amp;sa=D&amp;source=editors&amp;ust=1615451627608000&amp;usg=AOvVaw04826FFZuCl5C4XNOMZtNp">F.mean</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.mean&amp;sa=D&amp;source=editors&amp;ust=1615451627609000&amp;usg=AOvVaw1TJ4AxxBOdeDb2K0r1CzkQ">torch.mean</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">Weighted average is not supported; rewrite as (without keepdims):</span></p><p class="c2"><span class="c11 c23 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.tensordot&amp;sa=D&amp;source=editors&amp;ust=1615451627610000&amp;usg=AOvVaw294GXG-ZioVsq5fGT5dGmc">torch.tensordot</a></span><span class="c32 c23 c7">(x, weights / weights.sum(), ([axis], [0]))</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.min.html%23chainer.functions.min&amp;sa=D&amp;source=editors&amp;ust=1615451627611000&amp;usg=AOvVaw1NfZh5J73K7_nF4NSkXUT4">F.min</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.min&amp;sa=D&amp;source=editors&amp;ust=1615451627611000&amp;usg=AOvVaw0D26W2jk5P60SrlRtFHnRy">torch.min</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.minimum.html%23chainer.functions.minimum&amp;sa=D&amp;source=editors&amp;ust=1615451627612000&amp;usg=AOvVaw15Jqc9EC2I8LlPwUHohh49">F.minimum</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.min&amp;sa=D&amp;source=editors&amp;ust=1615451627613000&amp;usg=AOvVaw0TbGEIRmXwe0Gt1-TpiTTQ">torch.min</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.ndtr.html%23chainer.functions.ndtr&amp;sa=D&amp;source=editors&amp;ust=1615451627614000&amp;usg=AOvVaw2kBmeg3A7No964l_9o13DM">F.ndtr</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23gelu&amp;sa=D&amp;source=editors&amp;ust=1615451627615000&amp;usg=AOvVaw0HLbWmqxlSDkzgg5dmfK_R">F.gelu</a></span><span class="c5">(x) is corresponding to x * F.ndtr(x).</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.ndtri.html%23chainer.functions.ndtri&amp;sa=D&amp;source=editors&amp;ust=1615451627616000&amp;usg=AOvVaw1ZdBHFG-fPYxoa3BOvbG4l">F.ndtri</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">N/A</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.prod.html%23chainer.functions.prod&amp;sa=D&amp;source=editors&amp;ust=1615451627617000&amp;usg=AOvVaw2vP71xTKs-rz0G7luPaDY3">F.prod</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.prod&amp;sa=D&amp;source=editors&amp;ust=1615451627618000&amp;usg=AOvVaw13HOiyoQB8RMfBMn8UHWUW">torch.prod</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c93"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.polygamma.html%23chainer.functions.polygamma&amp;sa=D&amp;source=editors&amp;ust=1615451627619000&amp;usg=AOvVaw2l7VMdQ9NFL5TIuEiWLMaU">F.polygamma</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/cppdocs/api/function_namespaceat_1a98080f784a3c761958a42f273ac5cfd2.html&amp;sa=D&amp;source=editors&amp;ust=1615451627620000&amp;usg=AOvVaw3zTPnTYTCmBnVzWrow_QpG">torch.polygamma</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c7">Not documented: </span><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://github.com/pytorch/pytorch/issues/25347&amp;sa=D&amp;source=editors&amp;ust=1615451627620000&amp;usg=AOvVaw3A7TdPh7MqMxHviM6WzU0V">https://github.com/pytorch/pytorch/issues/25347</a></span></p><p class="c2"><span class="c7">n&gt;=2 not supported: </span><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://github.com/pytorch/pytorch/blob/v1.3.1/aten/src/ATen/native/cpu/UnaryOpsKernel.cpp%23L179&amp;sa=D&amp;source=editors&amp;ust=1615451627621000&amp;usg=AOvVaw3XbRnLKd8KqR-vPv4_86Tc">https://github.com/pytorch/pytorch/blob/v1.3.1/aten/src/ATen/native/cpu/UnaryOpsKernel.cpp#L179</a></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.rsqrt.html%23chainer.functions.rsqrt&amp;sa=D&amp;source=editors&amp;ust=1615451627622000&amp;usg=AOvVaw3YYiulikX93myT_KnEWH-5">F.rsqrt</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.rsqrt&amp;sa=D&amp;source=editors&amp;ust=1615451627622000&amp;usg=AOvVaw3aKEOoSmj3oh51WuQs6mGU">torch.rsqrt</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">-</span></p></td></tr><tr class="c34"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.scale.html%23chainer.functions.scale&amp;sa=D&amp;source=editors&amp;ust=1615451627623000&amp;usg=AOvVaw1qsj6-Gt9My3wyW19m9HX-">F.scale</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">Rewrite as:</span></p><p class="c2"><span class="c32 c23 c7">x * y[(...,) + (None,) * (x.ndim - y.ndim - 1)]</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.sin.html%23chainer.functions.sin&amp;sa=D&amp;source=editors&amp;ust=1615451627625000&amp;usg=AOvVaw0yoz_6JtHvIno_DAjAB5XL">F.sin</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.sin&amp;sa=D&amp;source=editors&amp;ust=1615451627626000&amp;usg=AOvVaw07z0RBnY-_3gxpWkzliKNJ">torch.sin</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.sinh.html%23chainer.functions.sinh&amp;sa=D&amp;source=editors&amp;ust=1615451627627000&amp;usg=AOvVaw17fhk5XFuP_Tbb4WbWByf2">F.sinh</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.sinh&amp;sa=D&amp;source=editors&amp;ust=1615451627628000&amp;usg=AOvVaw1JkKoWSuIb2UCHIeV0WVtf">torch.sinh</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.sign.html%23chainer.functions.sign&amp;sa=D&amp;source=editors&amp;ust=1615451627628000&amp;usg=AOvVaw1sbHjgaHQk14sz28q7OyeZ">F.sign</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.sign&amp;sa=D&amp;source=editors&amp;ust=1615451627629000&amp;usg=AOvVaw0Ip0FIq2BrtbrZmAKoCcCw">torch.sign</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c34"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.sparse_matmul.html%23chainer.functions.sparse_matmul&amp;sa=D&amp;source=editors&amp;ust=1615451627630000&amp;usg=AOvVaw00t5ntwHfuoeQmm3Ua_RWu">F.sparse_matmul</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/sparse.html%23torch.sparse.mm&amp;sa=D&amp;source=editors&amp;ust=1615451627631000&amp;usg=AOvVaw3yYtAV48cOoo8zBU7ouNZZ">torch.sparse.mm</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">Only support dense-sparse product. For sparse-dense product, transpose the operands and the output.</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.sqrt.html%23chainer.functions.sqrt&amp;sa=D&amp;source=editors&amp;ust=1615451627633000&amp;usg=AOvVaw35ynbYaDb4FOzj4Yr_v2Nn">F.sqrt</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.sqrt&amp;sa=D&amp;source=editors&amp;ust=1615451627634000&amp;usg=AOvVaw37WFNfFGIPRMXpiUrFErfn">torch.sqrt</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.square.html%23chainer.functions.square&amp;sa=D&amp;source=editors&amp;ust=1615451627636000&amp;usg=AOvVaw3oxSGJgbsCFbhZOHkE-fCI">F.square</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">Rewrite as:</span></p><p class="c2"><span class="c32 c23 c7">x * x</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.squared_difference.html%23chainer.functions.squared_difference&amp;sa=D&amp;source=editors&amp;ust=1615451627639000&amp;usg=AOvVaw0u1YooKqOEHIs8EMBxBegU">F.squared_difference</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">N/A</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.sum.html%23chainer.functions.sum&amp;sa=D&amp;source=editors&amp;ust=1615451627641000&amp;usg=AOvVaw2xdJx2MOrhigfD8vUmCGzJ">F.sum</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.sum&amp;sa=D&amp;source=editors&amp;ust=1615451627642000&amp;usg=AOvVaw2OE0BHiHiFLJoLntKP352y">torch.sum</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.sum_to.html%23chainer.functions.sum_to&amp;sa=D&amp;source=editors&amp;ust=1615451627643000&amp;usg=AOvVaw09HxGZPYNpxdnwFwMEuQaP">F.sum_to</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">N/A</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.tanh.html%23chainer.functions.tanh&amp;sa=D&amp;source=editors&amp;ust=1615451627645000&amp;usg=AOvVaw2kSOVTmRhneRD98V21WfXZ">F.tanh</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.tanh&amp;sa=D&amp;source=editors&amp;ust=1615451627646000&amp;usg=AOvVaw2DBW1MIYsrv66D5aMeo8t7">torch.tanh</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.tan.html%23chainer.functions.tan&amp;sa=D&amp;source=editors&amp;ust=1615451627647000&amp;usg=AOvVaw0kpb5y3A5KA-x1XT1Z-H4p">F.tan</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.tan&amp;sa=D&amp;source=editors&amp;ust=1615451627649000&amp;usg=AOvVaw26CYPjxeTvBGz_pqYmKPq4">torch.tan</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.tensordot.html%23chainer.functions.tensordot&amp;sa=D&amp;source=editors&amp;ust=1615451627650000&amp;usg=AOvVaw0aivKCJkVmUBqSdvzxzpeB">F.tensordot</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.tensordot&amp;sa=D&amp;source=editors&amp;ust=1615451627651000&amp;usg=AOvVaw1lMrCDDutt4Y6HS3ayYw9b">torch.tensordot</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c59" colspan="3" rowspan="1"><p class="c2"><span class="c7 c36">Noise injections</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.dropout.html%23chainer.functions.dropout&amp;sa=D&amp;source=editors&amp;ust=1615451627653000&amp;usg=AOvVaw1Xc-chtsnK1rgBK9euqLcr">F.dropout</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23dropout&amp;sa=D&amp;source=editors&amp;ust=1615451627654000&amp;usg=AOvVaw3haFKkddMCaHmi-rVdNB4F">F.dropout</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">No mask support, elements are randomly zeroed.</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.gaussian.html%23chainer.functions.gaussian&amp;sa=D&amp;source=editors&amp;ust=1615451627654000&amp;usg=AOvVaw0_OnJrL5KiJbOBztyfXoT1">F.gaussian</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.distributions.normal.Normal&amp;sa=D&amp;source=editors&amp;ust=1615451627655000&amp;usg=AOvVaw3yvWk000nD872bqsi0nhVX">torch.distributions.normal.Normal</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.gumbel_softmax.html%23chainer.functions.gumbel_softmax&amp;sa=D&amp;source=editors&amp;ust=1615451627656000&amp;usg=AOvVaw1IAogl-0TeMG8_K5khaPsn">F.gumbel_softmax</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.gumbel_softmax&amp;sa=D&amp;source=editors&amp;ust=1615451627657000&amp;usg=AOvVaw2FtIEgvyWZR313A2aK0oCx">F.gumbel_softmax</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">The default value of tau is 1, while the Chainer&#39;s function takes 0.1.</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.simplified_dropconnect.html%23chainer.functions.simplified_dropconnect&amp;sa=D&amp;source=editors&amp;ust=1615451627658000&amp;usg=AOvVaw33hmtlNqG9GQoNxvY_KxU9">F.simplified_dropconnect</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">Not available. Use F.dropout on the weight, or try torchnlp.nn.WeightDrop.</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.zoneout.html%23chainer.functions.zoneout&amp;sa=D&amp;source=editors&amp;ust=1615451627660000&amp;usg=AOvVaw3ZARL3XxBzHa_JhvUMXhnT">F.zoneout</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">N/A</span></p></td></tr><tr class="c12"><td class="c59" colspan="3" rowspan="1"><p class="c2"><span class="c7 c36">Normalization functions</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.batch_normalization.html%23chainer.functions.batch_normalization&amp;sa=D&amp;source=editors&amp;ust=1615451627664000&amp;usg=AOvVaw3KmasqhsK9L-hAUle1vjUG">F.batch_normalization</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.batch_norm&amp;sa=D&amp;source=editors&amp;ust=1615451627665000&amp;usg=AOvVaw0msghBolXnXQleobkDc9Vx">F.batch_norm</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.batch_renormalization.html%23chainer.functions.batch_renormalization&amp;sa=D&amp;source=editors&amp;ust=1615451627666000&amp;usg=AOvVaw14mTV4J-2RqtEO9omZpxwI">F.batch_renormalization</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">N/A</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.decorrelated_batch_normalization.html%23chainer.functions.decorrelated_batch_normalization&amp;sa=D&amp;source=editors&amp;ust=1615451627668000&amp;usg=AOvVaw0yc4pVwoHzD1uVZv0j93FL">F.decorrelated_batch_normalization</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">N/A</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.fixed_batch_normalization.html%23chainer.functions.fixed_batch_normalization&amp;sa=D&amp;source=editors&amp;ust=1615451627670000&amp;usg=AOvVaw0xy0sdIQmb1GLRILqyTlLF">F.fixed_batch_normalization</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.batch_norm&amp;sa=D&amp;source=editors&amp;ust=1615451627671000&amp;usg=AOvVaw3ONP_CNkuFlFdUYqqZ_RXj">F.batch_norm</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">training=False?</span></p></td></tr><tr class="c45"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.fixed_batch_renormalization.html%23chainer.functions.fixed_batch_renormalization&amp;sa=D&amp;source=editors&amp;ust=1615451627673000&amp;usg=AOvVaw1sO89u8Sizon-kK0rzEtaQ">F.fixed_batch_renormalization</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c7">Batch Renormalization not implemented: </span><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://discuss.pytorch.org/t/support-for-batch-renormalization/29653&amp;sa=D&amp;source=editors&amp;ust=1615451627674000&amp;usg=AOvVaw3ZOHhnb0S99u6XhoBiYaqT">https://discuss.pytorch.org/t/support-for-batch-renormalization/2965</a></span><span class="c7">, </span><span class="c7">&nbsp;</span><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://discuss.pytorch.org/t/batch-renormalization-implementation-in-thcunn/5144&amp;sa=D&amp;source=editors&amp;ust=1615451627675000&amp;usg=AOvVaw0QQ2K8VLD75nElaLW5MmnF">https://discuss.pytorch.org/t/batch-renormalization-implementation-in-thcunn/5144</a></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.fixed_decorrelated_batch_normalization.html%23chainer.functions.fixed_decorrelated_batch_normalization&amp;sa=D&amp;source=editors&amp;ust=1615451627676000&amp;usg=AOvVaw2WNzysQ1j1B4zRPEPpbPy-">F.fixed_decorrelated_batch_normalization</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">N/A</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.group_normalization.html%23chainer.functions.group_normalization&amp;sa=D&amp;source=editors&amp;ust=1615451627677000&amp;usg=AOvVaw0UqQjufiXo8xwFnpf0pe1y">F.group_normalization</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.group_norm&amp;sa=D&amp;source=editors&amp;ust=1615451627677000&amp;usg=AOvVaw2L7aglKkenrDks1e31k9eo">F.group_norm</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">Currently undocumented.</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.layer_normalization.html%23chainer.functions.layer_normalization&amp;sa=D&amp;source=editors&amp;ust=1615451627679000&amp;usg=AOvVaw0oWv13L9JTwZw2bNM9QaGw">F.layer_normalization</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.layer_norm&amp;sa=D&amp;source=editors&amp;ust=1615451627679000&amp;usg=AOvVaw2K0lNVlNOFybbcwgf_eKbL">layer_norm</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">`gamma = weight` &amp; `beta= bias`</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.local_response_normalization.html%23chainer.functions.local_response_normalization&amp;sa=D&amp;source=editors&amp;ust=1615451627680000&amp;usg=AOvVaw1N3FFA7boSiHA5MKvYP879">F.local_response_normalization</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.local_response_norm&amp;sa=D&amp;source=editors&amp;ust=1615451627681000&amp;usg=AOvVaw05JFOpHEAR_p73q-QdquUH">F.local_response_norm</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c34"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.normalize.html%23chainer.functions.normalize&amp;sa=D&amp;source=editors&amp;ust=1615451627682000&amp;usg=AOvVaw3X9y-HTRJF3w7pj2o1QghS">F.normalize</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.normalize&amp;sa=D&amp;source=editors&amp;ust=1615451627683000&amp;usg=AOvVaw1Z73UAkpOOH4sYzLde-u5Q">F.normalize</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">The PyTorch&#39;s `F.normalize` is not only for L2 normalization. But the default behavior is for L2 normalization, i.e., the default value of the second argument `p` is set to 2.</span></p></td></tr><tr class="c12"><td class="c59" colspan="3" rowspan="1"><p class="c2"><span class="c7 c36">Spatial pooling</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.average_pooling_1d.html%23chainer.functions.average_pooling_1d&amp;sa=D&amp;source=editors&amp;ust=1615451627685000&amp;usg=AOvVaw1ERvIzDOX6ZEe_O4jix8aM">F.average_pooling_1d</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.avg_pool1d&amp;sa=D&amp;source=editors&amp;ust=1615451627686000&amp;usg=AOvVaw2hSR3PI9MQvsI5ogiz-okC">F.avg_pool1d</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.average_pooling_2d.html%23chainer.functions.average_pooling_2d&amp;sa=D&amp;source=editors&amp;ust=1615451627687000&amp;usg=AOvVaw24BnEMfWeYYpq0RVyHmacd">F.average_pooling_2d</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.avg_pool2d&amp;sa=D&amp;source=editors&amp;ust=1615451627688000&amp;usg=AOvVaw3KxJsTTg3wdP7Icth6EMin">F.avg_pool2d</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">Superset of Chainer&#39;s counterpart.</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.average_pooling_3d.html%23chainer.functions.average_pooling_3d&amp;sa=D&amp;source=editors&amp;ust=1615451627689000&amp;usg=AOvVaw19mSq9ql5LJBYkyhqMpzju">F.average_pooling_3d</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.avg_pool3d&amp;sa=D&amp;source=editors&amp;ust=1615451627690000&amp;usg=AOvVaw1BRzRaHqobl_x5NB2XF-Dm">F.avg_pool3d</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.average_pooling_nd.html%23chainer.functions.average_pooling_nd&amp;sa=D&amp;source=editors&amp;ust=1615451627691000&amp;usg=AOvVaw3Er5E__Wv7g_Z2zvDm3mUA">F.average_pooling_nd</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">N/A</span></p></td></tr><tr class="c34"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.max_pooling_1d.html%23chainer.functions.max_pooling_1d&amp;sa=D&amp;source=editors&amp;ust=1615451627693000&amp;usg=AOvVaw3wRNSEaHCGzss1k5wdJdO5">F.max_pooling_1d</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.max_pool1d&amp;sa=D&amp;source=editors&amp;ust=1615451627694000&amp;usg=AOvVaw0ADpC0hNutV6S5tFVOjEPC">F.max_pool1d</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">In addition to arguments documented in `nn.MaxPool1D`, `return_indices` argument is available to obtain index for unpooling.</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.max_pooling_2d.html%23chainer.functions.max_pooling_2d&amp;sa=D&amp;source=editors&amp;ust=1615451627695000&amp;usg=AOvVaw1wQ8XZub6JX9Uq3PU4vKIN">F.max_pooling_2d</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.max_pool2d&amp;sa=D&amp;source=editors&amp;ust=1615451627696000&amp;usg=AOvVaw2YFs0uq3wyHNd6RwKkgP5g">F.max_pool2d</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">ditto.</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.max_pooling_3d.html%23chainer.functions.max_pooling_3d&amp;sa=D&amp;source=editors&amp;ust=1615451627697000&amp;usg=AOvVaw04Con-gNwZDzpxOSpHVe7F">F.max_pooling_3d</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.max_pool3d&amp;sa=D&amp;source=editors&amp;ust=1615451627698000&amp;usg=AOvVaw3uGXV_mWHg8twg0sh5Qa8a">F.max_pool3d</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">ditto.</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.max_pooling_nd.html%23chainer.functions.max_pooling_nd&amp;sa=D&amp;source=editors&amp;ust=1615451627700000&amp;usg=AOvVaw2QMXtgsYPOqiaNgjDk40lG">F.max_pooling_nd</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">N/A</span></p></td></tr><tr class="c34"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.roi_average_align_2d.html%23chainer.functions.roi_average_align_2d&amp;sa=D&amp;source=editors&amp;ust=1615451627701000&amp;usg=AOvVaw0nAK2-_cMk9gmEB5XGs8UN">F.roi_average_align_2d</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torchvision/ops.html%23torchvision.ops.roi_align&amp;sa=D&amp;source=editors&amp;ust=1615451627702000&amp;usg=AOvVaw04GOA0GcbHiaFr0U-o-YER">torchvision.ops.roi_align</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">Requires Torchvision, torchvision has only 2 roi functions, roi_align uses the average of the pixels while roi_pool uses the max value</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.roi_average_pooling_2d.html%23chainer.functions.roi_average_pooling_2d&amp;sa=D&amp;source=editors&amp;ust=1615451627703000&amp;usg=AOvVaw1TPY1B4OzecgHO7rGA2c13">F.roi_average_pooling_2d</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">N/A</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.roi_max_align_2d.html%23chainer.functions.roi_max_align_2d&amp;sa=D&amp;source=editors&amp;ust=1615451627705000&amp;usg=AOvVaw0LOe0oGya8BP_fu7ZGwR_m">F.roi_max_align_2d</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">N/A</span></p></td></tr><tr class="c81"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.roi_max_pooling_2d.html%23chainer.functions.roi_max_pooling_2d&amp;sa=D&amp;source=editors&amp;ust=1615451627707000&amp;usg=AOvVaw1rz0m2WWBka6QT_tGRV-0B">F.roi_max_pooling_2d</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torchvision.ops.roi_pool&amp;sa=D&amp;source=editors&amp;ust=1615451627708000&amp;usg=AOvVaw1-ct7iueB6C2Z8xFdGZB-v">torchvision.ops.roi_pool</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c7">The `roi_pool` function of Torchvision is meant to be roi max pooling according to the source: </span><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://github.com/pytorch/vision/blob/ccd1b27d2b7312ebddb4d51b3a4f8ade1ba8fa8b/torchvision/csrc/cpu/ROIPool_cpu.cpp%23L65&amp;sa=D&amp;source=editors&amp;ust=1615451627709000&amp;usg=AOvVaw0kahDG7oeQ1hB5CukZGWkk">https://github.com/pytorch/vision/blob/ccd1b27d2b7312ebddb4d51b3a4f8ade1ba8fa8b/torchvision/csrc/cpu/ROIPool_cpu.cpp#L65</a></span></p><p class="c2"><span class="c5">Regarding the API, the way to pass the batch indices of each set of RoI coordinates is different.</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.roi_pooling_2d.html%23chainer.functions.roi_pooling_2d&amp;sa=D&amp;source=editors&amp;ust=1615451627710000&amp;usg=AOvVaw3HTHxtgjHvrLmz_WSAHURc">F.roi_pooling_2d</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torchvision/ops.html%23torchvision.ops.roi_pool&amp;sa=D&amp;source=editors&amp;ust=1615451627711000&amp;usg=AOvVaw3WWFY_UyTGJWK-j1GXY7WU">torchvision.ops.roi_pool</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">Requires torchvision.</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.spatial_pyramid_pooling_2d.html%23chainer.functions.spatial_pyramid_pooling_2d&amp;sa=D&amp;source=editors&amp;ust=1615451627712000&amp;usg=AOvVaw328vEF1iH6ghRfUbLmRM6S">F.spatial_pyramid_pooling_2d</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">Not available. Not too difficult to implement? It&#39;s a combination of existing functions in Chainer.</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.unpooling_1d.html%23chainer.functions.unpooling_1d&amp;sa=D&amp;source=editors&amp;ust=1615451627714000&amp;usg=AOvVaw1PfyKEbijS_SdwTWGmD--N">F.unpooling_1d</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.max_unpool1d&amp;sa=D&amp;source=editors&amp;ust=1615451627715000&amp;usg=AOvVaw2DBag7n-jhlHQVP7vfKI3T">F.max_unpool1d</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">Pass `indices` returned from F.max_pool1d.</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.unpooling_2d.html%23chainer.functions.unpooling_2d&amp;sa=D&amp;source=editors&amp;ust=1615451627716000&amp;usg=AOvVaw0qkyRPAqn8GZbYMmus3zA4">F.unpooling_2d</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.max_unpool2d&amp;sa=D&amp;source=editors&amp;ust=1615451627717000&amp;usg=AOvVaw0kX7Q-Zl4dAPDuVoZqUuHM">F.max_unpool2d</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">ditto.</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.unpooling_3d.html%23chainer.functions.unpooling_3d&amp;sa=D&amp;source=editors&amp;ust=1615451627718000&amp;usg=AOvVaw1pcbBCkMm9iSzrB1couzjl">F.unpooling_3d</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.max_unpool3d&amp;sa=D&amp;source=editors&amp;ust=1615451627719000&amp;usg=AOvVaw3vcNGwtVlkACKm_GKkmavV">F.max_unpool3d</a></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">ditto.</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.unpooling_nd.html%23chainer.functions.unpooling_nd&amp;sa=D&amp;source=editors&amp;ust=1615451627720000&amp;usg=AOvVaw2G4DtwvRQllKWEYRjnWk2D">F.unpooling_nd</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">N/A</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.upsampling_2d.html%23chainer.functions.upsampling_2d&amp;sa=D&amp;source=editors&amp;ust=1615451627722000&amp;usg=AOvVaw1eizO4tW-dXmm4hXUyukR0">F.upsampling_2d</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c5">N/A</span></p></td></tr><tr class="c12"><td class="c59" colspan="3" rowspan="1"><p class="c2"><span class="c7 c36">Utility functions</span></p></td></tr><tr class="c12"><td class="c16" colspan="1" rowspan="1"><p class="c2"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.functions.forget.html%23chainer.functions.forget&amp;sa=D&amp;source=editors&amp;ust=1615451627724000&amp;usg=AOvVaw1_RKTx1ja5GQFO-X3czN90">F.forget</a></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2 c17"><span class="c5"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c7">See </span><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/checkpoint.html&amp;sa=D&amp;source=editors&amp;ust=1615451627725000&amp;usg=AOvVaw0nBKrImvKR3Jdu1Eb2vTAL">https://pytorch.org/docs/stable/checkpoint.html</a></span></p></td></tr></tbody></table><p class="c2 c17"><span class="c0"></span></p><h3 class="c46 c25" id="h.ryz2089jygqa"><span class="c8">Links</span></h3><p class="c2"><span class="c23">L</span><span>&nbsp;refers to </span><span class="c23">chainer.links</span><span>&nbsp;(Chainer), and </span><span class="c23">nn</span><span>&nbsp;refers to </span><span class="c23">torch.nn</span><span class="c0">&nbsp;(PyTorch).</span></p><p class="c2 c17"><span class="c0"></span></p><a id="t.3b37620fdb6ced6923d1d3cbb3cd34113c1d0cb4"></a><a id="t.20"></a><table class="c55"><tbody><tr class="c34"><td class="c50 c98" colspan="1" rowspan="1"><p class="c52"><span class="c7 c36">Chainer</span></p></td><td class="c50 c96" colspan="1" rowspan="1"><p class="c52"><span class="c7 c36">PyTorch</span></p></td><td class="c50 c94" colspan="1" rowspan="1"><p class="c52"><span class="c7 c36">Notes</span></p></td></tr><tr class="c12"><td class="c30" colspan="3" rowspan="1"><p class="c3"><span class="c32 c83 c7 c36">Learnable connections</span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.Bias.html%23chainer.links.Bias&amp;sa=D&amp;source=editors&amp;ust=1615451627733000&amp;usg=AOvVaw22i0bT477DyEfehxXzalyE">L.Bias</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.Bilinear.html%23chainer.links.Bilinear&amp;sa=D&amp;source=editors&amp;ust=1615451627734000&amp;usg=AOvVaw1SFI5jM9Cmpk897Fu45UNG">L.Bilinear</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.html%23torch.nn.Bilinear&amp;sa=D&amp;source=editors&amp;ust=1615451627735000&amp;usg=AOvVaw3Q_b1WMG8WACmEEEeqyO_G">nn.Bilinear</a></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td></tr><tr class="c34"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.ChildSumTreeLSTM.html%23chainer.links.ChildSumTreeLSTM&amp;sa=D&amp;source=editors&amp;ust=1615451627736000&amp;usg=AOvVaw2v0iEkwAJuiSHNf48N9Sjo">L.ChildSumTreeLSTM</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3"><span class="c7">N/A. Reference user implementation at: </span><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://github.com/ttpro1995/TreeLSTMSentiment&amp;sa=D&amp;source=editors&amp;ust=1615451627737000&amp;usg=AOvVaw3-KTuYimrWhNk2LdIDehHA">https://github.com/ttpro1995/TreeLSTMSentiment</a></span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.Convolution1D.html%23chainer.links.Convolution1D&amp;sa=D&amp;source=editors&amp;ust=1615451627738000&amp;usg=AOvVaw3R_xtn1ee8pS-xJlMzNz8z">L.Convolution1D</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.html%23torch.nn.Conv1d&amp;sa=D&amp;source=editors&amp;ust=1615451627739000&amp;usg=AOvVaw1bsR1tDG-9tuVOvDAvHRXb">nn.Conv1d</a></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.Convolution2D.html%23chainer.links.Convolution2D&amp;sa=D&amp;source=editors&amp;ust=1615451627740000&amp;usg=AOvVaw1rAKuDm3N1QfkAkN4CoFRy">L.Convolution2D</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.html%23torch.nn.Conv2d&amp;sa=D&amp;source=editors&amp;ust=1615451627741000&amp;usg=AOvVaw0ic9o6d1l-okXwHmclD-x2">nn.Conv2d</a></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.Convolution3D.html%23chainer.links.Convolution3D&amp;sa=D&amp;source=editors&amp;ust=1615451627742000&amp;usg=AOvVaw33yRCAIrMngQzLYq9BYYjt">L.Convolution3D</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.html%23torch.nn.Conv3d&amp;sa=D&amp;source=editors&amp;ust=1615451627743000&amp;usg=AOvVaw298i0FssgXq9S8C_p2dPGI">nn.Conv3d</a></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.ConvolutionND.html%23chainer.links.ConvolutionND&amp;sa=D&amp;source=editors&amp;ust=1615451627744000&amp;usg=AOvVaw1b5-vby2pG4ROekONSbZwQ">L.ConvolutionND</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3"><span class="c5">N/A</span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.Deconvolution1D.html%23chainer.links.Deconvolution1D&amp;sa=D&amp;source=editors&amp;ust=1615451627746000&amp;usg=AOvVaw2XQKj6xr31V25uUcEiMwxu">L.Deconvolution1D</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.html%23torch.nn.ConvTranspose1d&amp;sa=D&amp;source=editors&amp;ust=1615451627747000&amp;usg=AOvVaw0TsZZFQqw0yYqaln2SRWly">nn.ConvTranspose1d</a></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.Deconvolution2D.html%23chainer.links.Deconvolution2D&amp;sa=D&amp;source=editors&amp;ust=1615451627748000&amp;usg=AOvVaw31rK2GtpF0R8o02czeulJz">L.Deconvolution2D</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.html%23torch.nn.ConvTranspose2d&amp;sa=D&amp;source=editors&amp;ust=1615451627749000&amp;usg=AOvVaw39I2W7_kGROGvHcYESyzt9">nn.ConvTranspose2d</a></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.Deconvolution3D.html%23chainer.links.Deconvolution3D&amp;sa=D&amp;source=editors&amp;ust=1615451627750000&amp;usg=AOvVaw0o0YEmx3Xz5HrFySgoK_PT">L.Deconvolution3D</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.html%23torch.nn.ConvTranspose3d&amp;sa=D&amp;source=editors&amp;ust=1615451627750000&amp;usg=AOvVaw1epwm3j0veTgIUfxKs8kzS">nn.ConvTranspose3d</a></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.DeconvolutionND.html%23chainer.links.DeconvolutionND&amp;sa=D&amp;source=editors&amp;ust=1615451627751000&amp;usg=AOvVaw3371FnEMRBZNk07H2vFOqW">L.DeconvolutionND</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.DeformableConvolution2D.html%23chainer.links.DeformableConvolution2D&amp;sa=D&amp;source=editors&amp;ust=1615451627753000&amp;usg=AOvVaw0RfJTH3xDBGovIVE-GX9XE">L.DeformableConvolution2D</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3"><span class="c7">N/A: </span><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://github.com/pytorch/pytorch/issues/2260&amp;sa=D&amp;source=editors&amp;ust=1615451627754000&amp;usg=AOvVaw3F5Md5pR44c4th3IhmiCjO">https://github.com/pytorch/pytorch/issues/2260</a></span></p></td></tr><tr class="c45"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.DepthwiseConvolution2D.html%23chainer.links.DepthwiseConvolution2D&amp;sa=D&amp;source=editors&amp;ust=1615451627755000&amp;usg=AOvVaw1cQsqGU_V7-hG5zuw8tq8j">L.DepthwiseConvolution2D</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.html%23torch.nn.Conv2d&amp;sa=D&amp;source=editors&amp;ust=1615451627756000&amp;usg=AOvVaw3I8K8kK6p969yufEV4kn7S">nn.Conv2d</a></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3"><span class="c7">Use `groups` argument; see </span><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://discuss.pytorch.org/t/depthwise-and-separable-convolutions-in-pytorch/7315/2&amp;sa=D&amp;source=editors&amp;ust=1615451627757000&amp;usg=AOvVaw3KxSK6FhBT0kokD6zTNep_">https://discuss.pytorch.org/t/depthwise-and-separable-convolutions-in-pytorch/7315/2</a></span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.DilatedConvolution2D.html%23chainer.links.DilatedConvolution2D&amp;sa=D&amp;source=editors&amp;ust=1615451627758000&amp;usg=AOvVaw0QEllR79O8SHzy12C6K1vg">L.DilatedConvolution2D</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.html%23torch.nn.Conv2d&amp;sa=D&amp;source=editors&amp;ust=1615451627758000&amp;usg=AOvVaw13Yqilq4AoXWSMY6EzqE2v">nn.Conv2d</a></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3"><span class="c5">Use `dilation` argument.</span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.EmbedID.html%23chainer.links.EmbedID&amp;sa=D&amp;source=editors&amp;ust=1615451627759000&amp;usg=AOvVaw0u5TCtezE4q8W3ehrN4B-V">L.EmbedID</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.html%23torch.nn.Embedding&amp;sa=D&amp;source=editors&amp;ust=1615451627760000&amp;usg=AOvVaw1t8IjHhFlBHZcaVoo2e-ln">nn.Embedding</a></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.GRU.html%23chainer.links.GRU&amp;sa=D&amp;source=editors&amp;ust=1615451627761000&amp;usg=AOvVaw1w5Mye7SxWGmSTfNsl6vaJ">L.GRU</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.html%23torch.nn.GRU&amp;sa=D&amp;source=editors&amp;ust=1615451627762000&amp;usg=AOvVaw2XUdxFyRSAw8Ze3PlW0Dr-">nn.GRU</a></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.Highway.html%23chainer.links.Highway&amp;sa=D&amp;source=editors&amp;ust=1615451627763000&amp;usg=AOvVaw0VFmLdvyrbKjthZvHkcX0o">L.Highway</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3"><span class="c5">N/A</span></p></td></tr><tr class="c45"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.Inception.html%23chainer.links.Inception&amp;sa=D&amp;source=editors&amp;ust=1615451627764000&amp;usg=AOvVaw1aAXbV_ValLIEkg0m0rX0E">L.Inception</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3"><span class="c5">`torchvision.models.inception.InceptionA` seems to be the corresponding module for Chainer&#39;s `L.Inception`, but is not documented.</span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.InceptionBN.html%23chainer.links.InceptionBN&amp;sa=D&amp;source=editors&amp;ust=1615451627766000&amp;usg=AOvVaw3EjL3ciVSTCYsv6zveh_ih">L.InceptionBN</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3"><span class="c5">See torchvision.models.inception for Inception v3</span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.Linear.html%23chainer.links.Linear&amp;sa=D&amp;source=editors&amp;ust=1615451627768000&amp;usg=AOvVaw2nFXpye-unnTkhYKQgCVER">L.Linear</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.html%23torch.nn.Linear&amp;sa=D&amp;source=editors&amp;ust=1615451627768000&amp;usg=AOvVaw0XZSOsyYhXYAprLOiMTCBv">nn.Linear</a></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.LocalConvolution2D.html%23chainer.links.LocalConvolution2D&amp;sa=D&amp;source=editors&amp;ust=1615451627769000&amp;usg=AOvVaw3EsrzwWQ4uKM1Xjkm5wan5">L.LocalConvolution2D</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3"><span class="c5">N/A</span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.LSTM.html%23chainer.links.LSTM&amp;sa=D&amp;source=editors&amp;ust=1615451627771000&amp;usg=AOvVaw0dYMHMseOpPwoQXgw6igeA">L.LSTM</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.html%23torch.nn.LSTM&amp;sa=D&amp;source=editors&amp;ust=1615451627772000&amp;usg=AOvVaw3OYPquIMruPw1W5HU9P2hg">nn.LSTM</a></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.MLPConvolution2D.html%23chainer.links.MLPConvolution2D&amp;sa=D&amp;source=editors&amp;ust=1615451627773000&amp;usg=AOvVaw2LBJMHAEkdhmP6hYEwrWW-">L.MLPConvolution2D</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.NaryTreeLSTM.html%23chainer.links.NaryTreeLSTM&amp;sa=D&amp;source=editors&amp;ust=1615451627775000&amp;usg=AOvVaw20wGwOUfcjxtzajbBSFQGN">L.NaryTreeLSTM</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3"><span class="c5">N/A</span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.NStepBiGRU.html%23chainer.links.NStepBiGRU&amp;sa=D&amp;source=editors&amp;ust=1615451627776000&amp;usg=AOvVaw3NBOQyWXEzNdy3z2R0uCqK">L.NStepBiGRU</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.html%23torch.nn.GRU&amp;sa=D&amp;source=editors&amp;ust=1615451627777000&amp;usg=AOvVaw3TaQ6ToVVBS4uK4Wjkg0d1">nn.GRU</a></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3"><span class="c5">bidirectional=True, no explicit activation, no stacking</span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.NStepBiLSTM.html%23chainer.links.NStepBiLSTM&amp;sa=D&amp;source=editors&amp;ust=1615451627779000&amp;usg=AOvVaw0spDLnCd-6forI8wzb_5if">L.NStepBiLSTM</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.html%23torch.nn.LSTM&amp;sa=D&amp;source=editors&amp;ust=1615451627780000&amp;usg=AOvVaw1cyquq3iUzBUOH8mAulWmH">nn.LSTM</a></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3"><span class="c5">bidirectional=True, no explicit activation, no stacking</span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.NStepBiRNNReLU.html%23chainer.links.NStepBiRNNReLU&amp;sa=D&amp;source=editors&amp;ust=1615451627781000&amp;usg=AOvVaw1CBJLctxfjsltE48jC_zk-">L.NStepBiRNNReLU</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.html%23torch.nn.RNN&amp;sa=D&amp;source=editors&amp;ust=1615451627782000&amp;usg=AOvVaw1gwxVx5DeinKYs_gzYcvPI">nn.RNN</a></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3"><span class="c5">bidirectional=True, no explicit activation, no stacking</span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.NStepBiRNNTanh.html%23chainer.links.NStepBiRNNTanh&amp;sa=D&amp;source=editors&amp;ust=1615451627783000&amp;usg=AOvVaw0cpPVZiqM5v0AVOzJd9NKw">L.NStepBiRNNTanh</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.html%23torch.nn.RNN&amp;sa=D&amp;source=editors&amp;ust=1615451627784000&amp;usg=AOvVaw1O0v__4sVfl7ZIJlaqTsnj">nn.RNN</a></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3"><span class="c5">bidirectional=True, no explicit activation, no stacking</span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.NStepGRU.html%23chainer.links.NStepGRU&amp;sa=D&amp;source=editors&amp;ust=1615451627785000&amp;usg=AOvVaw2Fw4h9rCMLLKcquWoc1jjo">L.NStepGRU</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.html%23torch.torch.nn.GRU&amp;sa=D&amp;source=editors&amp;ust=1615451627786000&amp;usg=AOvVaw0C6IQbm5JuBpgqyNCDwtVw">nn.GRU</a></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.NStepLSTM.html%23chainer.links.NStepLSTM&amp;sa=D&amp;source=editors&amp;ust=1615451627787000&amp;usg=AOvVaw3spz3l1TsroJMkIzNx3W1-">L.NStepLSTM</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.html%23torch.nn.LSTM&amp;sa=D&amp;source=editors&amp;ust=1615451627787000&amp;usg=AOvVaw0XAjhPVeZYkIQVwJkutgSp">nn.LSTM</a></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.NStepRNNReLU.html%23chainer.links.NStepRNNReLU&amp;sa=D&amp;source=editors&amp;ust=1615451627788000&amp;usg=AOvVaw1gO2ZnFOV1NAZ36OdCNIfK">L.NStepRNNReLU</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.html%23torch.nn.RNN&amp;sa=D&amp;source=editors&amp;ust=1615451627789000&amp;usg=AOvVaw0b1t1Huy8Uaj1eiEgLQ1dJ">nn.RNN</a></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.NStepRNNTanh.html%23chainer.links.NStepRNNTanh&amp;sa=D&amp;source=editors&amp;ust=1615451627790000&amp;usg=AOvVaw3TGLeifLjcYpcD9q-bWHox">L.NStepRNNTanh</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.html%23torch.nn.RNN&amp;sa=D&amp;source=editors&amp;ust=1615451627791000&amp;usg=AOvVaw0xjSzLtoTa47sAHLVlF4G2">nn.RNN</a></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.Parameter.html%23chainer.links.Parameter&amp;sa=D&amp;source=editors&amp;ust=1615451627792000&amp;usg=AOvVaw1Cuvdand9Bo5V73yNKC9c1">L.Parameter</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3"><span class="c5">You could use torch.nn.modules.ParameterList with 1 element</span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.Scale.html%23chainer.links.Scale&amp;sa=D&amp;source=editors&amp;ust=1615451627794000&amp;usg=AOvVaw3njoWCeYvH8bqC1oaco56o">L.Scale</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3"><span class="c5">N/A</span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.StatefulGRU.html%23chainer.links.StatefulGRU&amp;sa=D&amp;source=editors&amp;ust=1615451627796000&amp;usg=AOvVaw0U25IL7bP1MndDipd6RUHq">L.StatefulGRU</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.html%23torch.nn.GRU&amp;sa=D&amp;source=editors&amp;ust=1615451627796000&amp;usg=AOvVaw0JkX0Ubd0vLK_wdWSpjYzx">nn.GRU</a></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.StatelessGRU.html%23chainer.links.StatelessGRU&amp;sa=D&amp;source=editors&amp;ust=1615451627798000&amp;usg=AOvVaw1u2OTRnx2B-OIyljR7G6oA">L.StatelessGRU</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3"><span class="c5">N/A</span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.StatefulMGU.html%23chainer.links.StatefulMGU&amp;sa=D&amp;source=editors&amp;ust=1615451627800000&amp;usg=AOvVaw0NrbtJCvVCOvHf-U9jRoV9">L.StatefulMGU</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3"><span class="c7">See </span><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://github.com/jpeg729/pytorch_bits&amp;sa=D&amp;source=editors&amp;ust=1615451627801000&amp;usg=AOvVaw2-ACuy7fPXWHawnT8kotvS">https://github.com/jpeg729/pytorch_bits</a></span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.StatelessMGU.html%23chainer.links.StatelessMGU&amp;sa=D&amp;source=editors&amp;ust=1615451627802000&amp;usg=AOvVaw1im7wXUQJdBk7tMBXUtq3r">L.StatelessMGU</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3"><span class="c5">N/A</span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.StatefulPeepholeLSTM.html%23chainer.links.StatefulPeepholeLSTM&amp;sa=D&amp;source=editors&amp;ust=1615451627804000&amp;usg=AOvVaw0ckk6GTDQfhuDbn5qZlWUU">L.StatefulPeepholeLSTM</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3"><span class="c7">See </span><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://github.com/pytorch/pytorch/issues/630&amp;sa=D&amp;source=editors&amp;ust=1615451627806000&amp;usg=AOvVaw1IHBhK-y3Uy4bRETpyTweD">https://github.com/pytorch/pytorch/issues/630</a></span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.StatefulZoneoutLSTM.html%23chainer.links.StatefulZoneoutLSTM&amp;sa=D&amp;source=editors&amp;ust=1615451627807000&amp;usg=AOvVaw2Iop4dKwtKvJpU7Ju59wol">L.StatefulZoneoutLSTM</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3"><span class="c7">N/A: </span><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://github.com/pytorch/pytorch/pull/4838&amp;sa=D&amp;source=editors&amp;ust=1615451627808000&amp;usg=AOvVaw3V1eWXARoxIY3OfXl9fxgA">https://github.com/pytorch/pytorch/pull/4838</a></span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.StatelessLSTM.html%23chainer.links.StatelessLSTM&amp;sa=D&amp;source=editors&amp;ust=1615451627809000&amp;usg=AOvVaw3_SqrC_ANA_CeX6eH6KaDh">L.StatelessLSTM</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td></tr><tr class="c34"><td class="c30" colspan="3" rowspan="1"><p class="c3"><span class="c32 c83 c7 c36">Activation/loss/normalization functions with parameters</span></p></td></tr><tr class="c93"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.BatchNormalization.html%23chainer.links.BatchNormalization&amp;sa=D&amp;source=editors&amp;ust=1615451627811000&amp;usg=AOvVaw17uC94KYJk5Ksq7yzfHcea">L.BatchNormalization</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3"><span class="c11 c70 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.html%23batchnorm1d&amp;sa=D&amp;source=editors&amp;ust=1615451627812000&amp;usg=AOvVaw0FLN0VsEM7iH8-CRj93ege">nn.BatchNorm1d</a></span></p><p class="c3"><span class="c11 c7 c70"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.html%23batchnorm1d&amp;sa=D&amp;source=editors&amp;ust=1615451627813000&amp;usg=AOvVaw29IdTkS07SyQq-Xh7IyFPE">nn.BatchNorm2d</a></span></p><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.html%23batchnorm1d&amp;sa=D&amp;source=editors&amp;ust=1615451627813000&amp;usg=AOvVaw29IdTkS07SyQq-Xh7IyFPE">nn.BatchNorm3d</a></span></p></td><td class="c112" colspan="1" rowspan="1"><p class="c3"><span class="c5">The argument `momentum` in the PyTorch implementation seems to be equivalent to `1 - decay` in the Chainer&#39;s link.</span></p><p class="c3"><span class="c5">The default value for the argument `eps` (1e-5) is different from Chainer&#39;s default value (2e-5).</span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.BatchRenormalization.html%23chainer.links.BatchRenormalization&amp;sa=D&amp;source=editors&amp;ust=1615451627815000&amp;usg=AOvVaw1bvxrQqL3vZDfaKPXiBbSE">L.BatchRenormalization</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3"><span class="c5">N/A</span></p></td></tr><tr class="c81"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.DecorrelatedBatchNormalization.html%23chainer.links.DecorrelatedBatchNormalization&amp;sa=D&amp;source=editors&amp;ust=1615451627817000&amp;usg=AOvVaw2FjL8N5JkkRR29wlsS9H6B">L.DecorrelatedBatchNormalization</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3"><span class="c7">Not available. A reference implementation (not that well implemented?) </span><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://github.com/huangleiBuaa/IterNorm-pytorch/blob/master/extension/normailzation/dbn.py&amp;sa=D&amp;source=editors&amp;ust=1615451627818000&amp;usg=AOvVaw3Ic7KoVx8i4vMcjaCVm7CJ">https://github.com/huangleiBuaa/IterNorm-pytorch/blob/master/extension/normailzation/dbn.py</a></span><span class="c7">. Otherwise look at the Torch lua official implementation </span><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://github.com/princeton-vl/DecorrelatedBN&amp;sa=D&amp;source=editors&amp;ust=1615451627818000&amp;usg=AOvVaw2mVRUjKG3AhN5rxVZvcJ_I">https://github.com/princeton-vl/DecorrelatedBN</a></span><span class="c5">.</span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.GroupNormalization.html%23chainer.links.GroupNormalization&amp;sa=D&amp;source=editors&amp;ust=1615451627819000&amp;usg=AOvVaw1ayrh4Y8dMvQtyKcJ6l2kB">L.GroupNormalization</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.html%23torch.nn.GroupNorm&amp;sa=D&amp;source=editors&amp;ust=1615451627820000&amp;usg=AOvVaw0Zh_b_WLNraaJSh8ebAHyN">nn.GroupNorm</a></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3"><span class="c5">affine=True</span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.LayerNormalization.html%23chainer.links.LayerNormalization&amp;sa=D&amp;source=editors&amp;ust=1615451627821000&amp;usg=AOvVaw3rTi8yzzk1Qnl_D0nwWidv">L.LayerNormalization</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.html%23torch.nn.LayerNorm&amp;sa=D&amp;source=editors&amp;ust=1615451627821000&amp;usg=AOvVaw2Z8DdMER5CtyxDA0PTrcry">nn.LayerNorm</a></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3"><span class="c5">elementwise_affine=True</span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.BinaryHierarchicalSoftmax.html%23chainer.links.BinaryHierarchicalSoftmax&amp;sa=D&amp;source=editors&amp;ust=1615451627822000&amp;usg=AOvVaw3BTEJKTl0Rp7UrZTd16UIX">L.BinaryHierarchicalSoftmax</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.BlackOut.html%23chainer.links.BlackOut&amp;sa=D&amp;source=editors&amp;ust=1615451627824000&amp;usg=AOvVaw2ZSrRTRVZMUCAVMaTpaABc">L.BlackOut</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3"><span class="c5">N/A</span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.CRF1d.html%23chainer.links.CRF1d&amp;sa=D&amp;source=editors&amp;ust=1615451627826000&amp;usg=AOvVaw1mTK6t5-qo9OeVFonU3r7J">L.CRF1d</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.SimplifiedDropconnect.html%23chainer.links.SimplifiedDropconnect&amp;sa=D&amp;source=editors&amp;ust=1615451627828000&amp;usg=AOvVaw39SBkXa-nVPpXv2OUW002W">L.SimplifiedDropconnect</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3"><span class="c5">N/A</span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.PReLU.html%23chainer.links.PReLU&amp;sa=D&amp;source=editors&amp;ust=1615451627830000&amp;usg=AOvVaw0O1G5CMIDhGkXz7cOp_2hq">L.PReLU</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.html%23torch.nn.PReLU&amp;sa=D&amp;source=editors&amp;ust=1615451627831000&amp;usg=AOvVaw3Revpmp4S5vmaBEZIALebt">nn.PReLU</a></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.Swish.html%23chainer.links.Swish&amp;sa=D&amp;source=editors&amp;ust=1615451627832000&amp;usg=AOvVaw1450TaAUMzo6M9fyuPuyC-">L.Swish</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3"><span class="c7">See </span><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://blog.ceshine.net/post/pytorch-memory-swish/&amp;sa=D&amp;source=editors&amp;ust=1615451627834000&amp;usg=AOvVaw1uAsW4AGmSNTNTQEcDaJyb">https://blog.ceshine.net/post/pytorch-memory-swish/</a></span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.Maxout.html%23chainer.links.Maxout&amp;sa=D&amp;source=editors&amp;ust=1615451627835000&amp;usg=AOvVaw1CssQ8O4VoM-k_tfDg8bnf">L.Maxout</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.NegativeSampling.html%23chainer.links.NegativeSampling&amp;sa=D&amp;source=editors&amp;ust=1615451627836000&amp;usg=AOvVaw3SE6N9nQ_qUfZYS14LoyJI">L.NegativeSampling</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c30" colspan="3" rowspan="1"><p class="c3"><span class="c32 c7 c36 c83">Machine learning models</span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.Classifier.html%23chainer.links.Classifier&amp;sa=D&amp;source=editors&amp;ust=1615451627840000&amp;usg=AOvVaw2MDSYj_I8Z8gGc5CND_vOW">L.Classifier</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3"><span class="c5">N/A</span></p></td></tr><tr class="c12"><td class="c30" colspan="3" rowspan="1"><p class="c3"><span class="c32 c83 c7 c36">Pre-trained models</span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.VGG16Layers.html%23chainer.links.VGG16Layers&amp;sa=D&amp;source=editors&amp;ust=1615451627843000&amp;usg=AOvVaw3Hc2b0LCAMViYYjXiUYCPL">L.VGG16Layers</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torchvision/models.html%23id2&amp;sa=D&amp;source=editors&amp;ust=1615451627844000&amp;usg=AOvVaw0qBJ1SJAwR_EFRWwxfhIMU">torchvision.models.vgg*</a></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3"><span class="c5">Superset of Chainer&#39;s VGG variations in torchvision.</span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.VGG19Layers.html%23chainer.links.VGG19Layers&amp;sa=D&amp;source=editors&amp;ust=1615451627846000&amp;usg=AOvVaw0mBwckznBlzuS8ElXDCdgX">L.VGG19Layers</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torchvision/models.html%23torchvision.models.vgg19&amp;sa=D&amp;source=editors&amp;ust=1615451627847000&amp;usg=AOvVaw38mOIpB3tCzeNlbCFwG1vG">torchvision.models.vgg19*</a></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3"><span class="c5">ditto</span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.model.vision.vgg.prepare.html%23chainer.links.model.vision.vgg.prepare&amp;sa=D&amp;source=editors&amp;ust=1615451627849000&amp;usg=AOvVaw3iZl2mrQcoHjEixVVYUI6f">L.model.vision.vgg.prepare</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3"><span class="c5">N/A</span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.GoogLeNet.html%23chainer.links.GoogLeNet&amp;sa=D&amp;source=editors&amp;ust=1615451627850000&amp;usg=AOvVaw2SVUqS7CIFiyyvbnQimyhj">L.GoogLeNet</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torchvision/models.html%23torchvision.models.googlenet&amp;sa=D&amp;source=editors&amp;ust=1615451627851000&amp;usg=AOvVaw0b_oUWT60Dm4TjJx57HCNI">torchvision.models.googlenet</a></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.model.vision.googlenet.prepare.html%23chainer.links.model.vision.googlenet.prepare&amp;sa=D&amp;source=editors&amp;ust=1615451627853000&amp;usg=AOvVaw35DKHK8KTRziHWTshn9Jrv">L.model.vision.googlenet.prepare</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3"><span class="c5">transform_input=True in torchvision.models.googlenet</span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.model.vision.resnet.ResNetLayers.html%23chainer.links.model.vision.resnet.ResNetLayers&amp;sa=D&amp;source=editors&amp;ust=1615451627856000&amp;usg=AOvVaw0rczHBd-DaGs0iss8DF3Hb">L.model.vision.resnet.ResNetLayers</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.ResNet50Layers.html%23chainer.links.ResNet50Layers&amp;sa=D&amp;source=editors&amp;ust=1615451627857000&amp;usg=AOvVaw09H1_wjAVtZZUtEadZhcqm">L.ResNet50Layers</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torchvision/models.html%23torchvision.models.resnet50&amp;sa=D&amp;source=editors&amp;ust=1615451627858000&amp;usg=AOvVaw2x-DoWZUThPGhIF9brG89Q">torchvision.models.resnet101</a></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3"><span class="c5">torchvision only, pretrained=True</span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.ResNet101Layers.html%23chainer.links.ResNet101Layers&amp;sa=D&amp;source=editors&amp;ust=1615451627859000&amp;usg=AOvVaw3v7hGtQDaMUp75Sn7ZVc2L">L.ResNet101Layers</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torchvision/models.html%23torchvision.models.resnet101&amp;sa=D&amp;source=editors&amp;ust=1615451627860000&amp;usg=AOvVaw07A5IPNCxe9LujteeKVz3n">torchvision.models.resnet101</a></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3"><span class="c5">torchvision only, pretrained=True</span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.ResNet152Layers.html%23chainer.links.ResNet152Layers&amp;sa=D&amp;source=editors&amp;ust=1615451627861000&amp;usg=AOvVaw1Nrx8qEeBwpFSfggk8zfJW">L.ResNet152Layers</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torchvision/models.html%23torchvision.models.resnet152&amp;sa=D&amp;source=editors&amp;ust=1615451627862000&amp;usg=AOvVaw2C75xJwljXUQx2_xdfnMan">torchvision.models.resnet152</a></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3"><span class="c5">torchvision only, pretrained=True</span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.model.vision.resnet.prepare.html%23chainer.links.model.vision.resnet.prepare&amp;sa=D&amp;source=editors&amp;ust=1615451627863000&amp;usg=AOvVaw3yHs39jkjdKlc_AaqSXdqf">L.model.vision.resnet.prepare</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3"><span class="c5">N/A</span></p></td></tr><tr class="c12"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.TheanoFunction.html%23chainer.links.TheanoFunction&amp;sa=D&amp;source=editors&amp;ust=1615451627865000&amp;usg=AOvVaw0gzuHNsNXY9Jozk2qCof1p">L.TheanoFunction</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3"><span class="c5">N/A</span></p></td></tr><tr class="c34"><td class="c41" colspan="1" rowspan="1"><p class="c3"><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/generated/chainer.links.caffe.CaffeFunction.html%23chainer.links.caffe.CaffeFunction&amp;sa=D&amp;source=editors&amp;ust=1615451627867000&amp;usg=AOvVaw1dok3-GMWy7SumGNCyFNE9">L.caffe.CaffeFunction</a></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c3 c17"><span class="c5"></span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c3"><span class="c7">See </span><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://github.com/marvis/pytorch-caffe&amp;sa=D&amp;source=editors&amp;ust=1615451627868000&amp;usg=AOvVaw0tKu3MfAepsSdlYGokP6-R">https://github.com/marvis/pytorch-caffe</a></span><span class="c7">&nbsp;or </span><span class="c11 c7"><a class="c9" href="https://www.google.com/url?q=https://github.com/Microsoft/MMdnn&amp;sa=D&amp;source=editors&amp;ust=1615451627868000&amp;usg=AOvVaw0-W63LdJzBeT1Ycp0v2DBP">https://github.com/Microsoft/MMdnn</a></span></p></td></tr></tbody></table><p class="c2 c17"><span class="c0"></span></p><h2 class="c39 c25" id="h.hb9kqa7i3enr"><span class="c10">Configuration</span></h2><p class="c2"><span>Here is the mapping of configurations in Chainer (</span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://docs.chainer.org/en/latest/reference/configuration.html&amp;sa=D&amp;source=editors&amp;ust=1615451627869000&amp;usg=AOvVaw0XRAOc-1mrcI0Bccizvj01">chainer.config.*</a></span><span class="c0">) and PyTorch:</span></p><p class="c2 c17"><span class="c0"></span></p><a id="t.f3301a8a5c40dafd6342027539056b8cf48e653c"></a><a id="t.21"></a><table class="c88"><tbody><tr class="c28"><td class="c50 c85" colspan="1" rowspan="1"><p class="c20"><span class="c19">Chainer</span></p></td><td class="c50 c79" colspan="1" rowspan="1"><p class="c20"><span class="c19">PyTorch</span></p></td><td class="c50 c77" colspan="1" rowspan="1"><p class="c20"><span class="c19">Notes</span></p></td></tr><tr class="c28"><td class="c65" colspan="1" rowspan="1"><p class="c18"><span class="c26 c23">autotune</span></p></td><td class="c51" colspan="1" rowspan="1"><p class="c18"><span class="c23 c26">torch.backends.cudnn.benchmark</span></p></td><td class="c74" colspan="1" rowspan="1"><p class="c18"><span class="c0">Not thread-local.</span></p></td></tr><tr class="c28"><td class="c65" colspan="1" rowspan="1"><p class="c18"><span class="c26 c23">cudnn_deterministic</span></p></td><td class="c51" colspan="1" rowspan="1"><p class="c18"><span class="c26 c23">torch.backends.cudnn.deterministic</span></p></td><td class="c74" colspan="1" rowspan="1"><p class="c18"><span class="c0">Not thread-local.</span></p></td></tr><tr class="c28"><td class="c65" colspan="1" rowspan="1"><p class="c18"><span class="c26 c23">cudnn_fast_batch_normalization</span></p></td><td class="c51" colspan="1" rowspan="1"><p class="c18"><span class="c0">N/A</span></p></td><td class="c74" colspan="1" rowspan="1"><p class="c18"><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://github.com/pytorch/pytorch/blob/v1.2.0/aten/src/ATen/native/cudnn/BatchNorm.cpp%23L90-L94&amp;sa=D&amp;source=editors&amp;ust=1615451627873000&amp;usg=AOvVaw2cPcqR2PjV77MpjijFG0Yr">Intentionally unsupported</a></span><span class="c0">&nbsp;as the precision is low in some models.</span></p></td></tr><tr class="c28"><td class="c65" colspan="1" rowspan="1"><p class="c18"><span class="c26 c23">debug</span></p></td><td class="c51" colspan="1" rowspan="1"><p class="c18"><span class="c0">N/A</span></p></td><td class="c74" colspan="1" rowspan="1"><p class="c18"><span>Use </span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/autograd.html%23torch.autograd.detect_anomaly&amp;sa=D&amp;source=editors&amp;ust=1615451627874000&amp;usg=AOvVaw2urpUHVm3065JwvJIIC2RR">torch.autograd.detect_anomaly()</a></span><span class="c0">&nbsp;context-manager to check NaN during backward, display the corresponding forward stack trace when error occurred in backward.</span></p></td></tr><tr class="c28"><td class="c65" colspan="1" rowspan="1"><p class="c18"><span class="c26 c23">dtype</span></p></td><td class="c51" colspan="1" rowspan="1"><p class="c18"><span class="c11 c23"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/torch.html%23torch.set_default_dtype&amp;sa=D&amp;source=editors&amp;ust=1615451627875000&amp;usg=AOvVaw3bCTEVTDW-MI8iW4R-G8RV">torch.set_default_dtype(dtype)</a></span></p></td><td class="c74" colspan="1" rowspan="1"><p class="c18"><span class="c0">Mixed precision support is done via Apex. Not thread-local.</span></p></td></tr><tr class="c28"><td class="c65" colspan="1" rowspan="1"><p class="c18"><span class="c26 c23">enable_backprop</span></p></td><td class="c51" colspan="1" rowspan="1"><p class="c18"><span class="c26 c23">torch.no_grad()</span></p><p class="c18"><span class="c26 c23">torch.enable_grad()</span></p></td><td class="c74" colspan="1" rowspan="1"><p class="c18"><span>You can use them as context-manager or decorator. See also </span><span class="c11"><a class="c9" href="#h.mc69ie7hbl6j">Backprop modes</a></span><span class="c0">.</span></p></td></tr><tr class="c28"><td class="c65" colspan="1" rowspan="1"><p class="c18"><span class="c26 c23">is_recomputing</span></p></td><td class="c51" colspan="1" rowspan="1"><p class="c18"><span class="c0">N/A</span></p></td><td class="c74" colspan="1" rowspan="1"><p class="c18"><span>See </span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/checkpoint.html&amp;sa=D&amp;source=editors&amp;ust=1615451627877000&amp;usg=AOvVaw1pbBnVyuZEgn4NKURbZfhy">torch.utils.checkpoint.checkpoint</a></span><span class="c0">&nbsp;for F.forget equivalent (it also supports RNG).</span></p></td></tr><tr class="c28"><td class="c65" colspan="1" rowspan="1"><p class="c18"><span class="c26 c23">keep_graph_on_report</span></p></td><td class="c51" colspan="1" rowspan="1"><p class="c18"><span class="c0">N/A</span></p></td><td class="c74" colspan="1" rowspan="1"><p class="c18 c17"><span class="c0"></span></p></td></tr><tr class="c28"><td class="c65" colspan="1" rowspan="1"><p class="c18"><span class="c26 c23">lazy_grad_sum</span></p></td><td class="c51" colspan="1" rowspan="1"><p class="c18"><span class="c0">N/A</span></p></td><td class="c74" colspan="1" rowspan="1"><p class="c18 c17"><span class="c0"></span></p></td></tr><tr class="c28"><td class="c65" colspan="1" rowspan="1"><p class="c18"><span class="c26 c23">train</span></p></td><td class="c51" colspan="1" rowspan="1"><p class="c18"><span class="c0">N/A</span></p></td><td class="c74" colspan="1" rowspan="1"><p class="c18"><span>The mode is configured per Module (using Module.train() and Module.eval()). See also </span><span class="c11"><a class="c9" href="#h.bka3yqtw2rpy">Train/Test modes</a></span><span class="c0">.</span></p></td></tr><tr class="c28"><td class="c65" colspan="1" rowspan="1"><p class="c18"><span class="c26 c23">type_check</span></p></td><td class="c51" colspan="1" rowspan="1"><p class="c18"><span class="c0">N/A</span></p></td><td class="c74" colspan="1" rowspan="1"><p class="c18 c17"><span class="c0"></span></p></td></tr><tr class="c28"><td class="c65" colspan="1" rowspan="1"><p class="c18"><span class="c26 c23">use_cudnn</span></p></td><td class="c51" colspan="1" rowspan="1"><p class="c18"><span class="c26 c23">torch.backends.cudnn.enabled</span></p></td><td class="c74" colspan="1" rowspan="1"><p class="c18"><span class="c0">Enabled by default. Not thread-local.</span></p></td></tr><tr class="c28"><td class="c65" colspan="1" rowspan="1"><p class="c18"><span class="c26 c23">use_cudnn_tensor_core</span></p></td><td class="c51" colspan="1" rowspan="1"><p class="c18"><span class="c0">N/A</span></p></td><td class="c74" colspan="1" rowspan="1"><p class="c18"><span class="c0">Tensor Cores cannot be disabled.</span></p></td></tr><tr class="c28"><td class="c65" colspan="1" rowspan="1"><p class="c18"><span class="c26 c23">use_ideep</span></p></td><td class="c51" colspan="1" rowspan="1"><p class="c18"><span class="c0">N/A</span></p></td><td class="c74" colspan="1" rowspan="1"><p class="c18"><span class="c0">PyTorch itself supports MKL-DNN. You can check availability using torch.backends.mkldnn.is_available().</span></p></td></tr><tr class="c115"><td class="c65" colspan="1" rowspan="1"><p class="c18"><span class="c26 c23">use_static_graph</span></p></td><td class="c51" colspan="1" rowspan="1"><p class="c18"><span class="c0">N/A</span></p></td><td class="c74" colspan="1" rowspan="1"><p class="c18 c17"><span class="c0"></span></p></td></tr><tr class="c28"><td class="c65" colspan="1" rowspan="1"><p class="c18"><span class="c26 c23">warn_nondeterministic</span></p></td><td class="c51" colspan="1" rowspan="1"><p class="c18"><span class="c0">N/A</span></p></td><td class="c74" colspan="1" rowspan="1"><p class="c18 c17"><span class="c0"></span></p></td></tr></tbody></table><p class="c2 c17"><span class="c0"></span></p><p class="c2"><span>See </span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/notes/randomness.html&amp;sa=D&amp;source=editors&amp;ust=1615451627887000&amp;usg=AOvVaw3BBadp6tVMZGE-YMs-j_D6">Reproducibility</a></span><span>&nbsp;for the reproducibility (including steps to fix seeds).</span></p><h2 class="c39 c25" id="h.rxt9pteccajv"><span class="c10">Hooks</span></h2><h3 class="c46 c25" id="h.gsgtoxixjcm4"><span class="c8">Function Hooks</span></h3><p class="c2"><span class="c0">There is no equivalent feature in PyTorch.</span></p><p class="c2"><span class="c0">Replacements for Chainer built-in hooks:</span></p><ul class="c64 lst-kix_owgllipbp8j5-0 start"><li class="c2 c42 li-bullet-0"><span>CUDAProfileHook: </span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/master/autograd.html%23torch.autograd.profiler.profile&amp;sa=D&amp;source=editors&amp;ust=1615451627888000&amp;usg=AOvVaw06IN-RkaMIpECwJcYdacAl">torch.autograd.profiler.profile</a></span><span>&nbsp;+ </span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/master/autograd.html%23torch.autograd.profiler.emit_nvtx&amp;sa=D&amp;source=editors&amp;ust=1615451627888000&amp;usg=AOvVaw3HVP9JmsWzBdMO-E4t8B3U">torch.autograd.profiler.emit_nvtx</a></span></li><li class="c2 c42 li-bullet-0"><span>CupyMemoryProfileHook: N/A (</span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/notes/cuda.html%23memory-management&amp;sa=D&amp;source=editors&amp;ust=1615451627889000&amp;usg=AOvVaw3le_LdlTWvTRSgKNuWoN_0">allocator status can be retrieved</a></span><span class="c0">)</span></li><li class="c2 c42 li-bullet-0"><span class="c0">PrintHook: N/A</span></li><li class="c2 c42 li-bullet-0"><span>TimerHook: </span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/master/autograd.html%23torch.autograd.profiler.profile&amp;sa=D&amp;source=editors&amp;ust=1615451627889000&amp;usg=AOvVaw1Q9kzenZgn0eK0NWt8HBpC">torch.autograd.profiler.profile</a></span></li></ul><h3 class="c46 c25" id="h.rvlhszhkp1bw"><span class="c8">Link Hooks</span></h3><p class="c2"><span>You can register </span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html%23forward-and-backward-function-hooks&amp;sa=D&amp;source=editors&amp;ust=1615451627890000&amp;usg=AOvVaw0NskEkmG_PE9FHQYuozkFm">Module Hooks</a></span><span class="c0">&nbsp;per module. There&#39;s no way to inject a hook for every Module called under the specific scope.</span></p><p class="c2"><span class="c0">Replacements for Chainer built-in hooks:</span></p><ul class="c64 lst-kix_9reixuath9e3-0 start"><li class="c2 c42 li-bullet-0"><span>SpectralNormalization: </span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.html%23torch.nn.utils.spectral_norm&amp;sa=D&amp;source=editors&amp;ust=1615451627891000&amp;usg=AOvVaw0k_l3J-FO6O9Air4Sv9XqZ">torch.nn.utils.spectral_norm</a></span><span>&nbsp;/ </span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.html%23torch.nn.utils.remove_spectral_norm&amp;sa=D&amp;source=editors&amp;ust=1615451627891000&amp;usg=AOvVaw3Ag32m_pxPDGKkSRTGgsoZ">torch.nn.utils.remove_spectral_norm</a></span></li><li class="c2 c42 li-bullet-0"><span class="c0">TimerHook: N/A</span></li></ul><h3 class="c46 c25" id="h.5a204an0cllk"><span class="c8">Optimizer Hooks</span></h3><p class="c2"><span>There is no direct equivalent in PyTorch, but you can </span><span>register backward hooks</span><span>&nbsp;per </span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/autograd.html%23torch.Tensor.register_hook&amp;sa=D&amp;source=editors&amp;ust=1615451627892000&amp;usg=AOvVaw1-9fc0xFGyIvHvd-6Bs746">Tensor</a></span><span>&nbsp;/ </span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.html%23torch.nn.Module.register_backward_hook&amp;sa=D&amp;source=editors&amp;ust=1615451627892000&amp;usg=AOvVaw21ywDQRaEZuGOoQTQQtWP3">Module</a></span><span>&nbsp;to modify gradients.</span></p><p class="c2"><span class="c0">Replacements for Chainer built-in hooks:</span></p><ul class="c64 lst-kix_jbicnccaji3d-0 start"><li class="c2 c42 li-bullet-0"><span>WeightDecay: specify as </span><span class="c23">weight_decay</span><span>&nbsp;argument to each Optimizer (e.g., </span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/optim.html%23torch.optim.Adam&amp;sa=D&amp;source=editors&amp;ust=1615451627893000&amp;usg=AOvVaw279QjLFJ1XbZe4KjENVccL">Adam</a></span><span class="c0">)</span></li><li class="c2 c42 li-bullet-0"><span class="c0">Lasso: N/A</span></li><li class="c2 c42 li-bullet-0"><span>GradientClipping: </span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.html%23torch.nn.utils.clip_grad_norm_&amp;sa=D&amp;source=editors&amp;ust=1615451627893000&amp;usg=AOvVaw2mD9L0jrEkiIgPTBUsGVNP">torch.nn.utils.clip_grad_norm_</a></span></li><li class="c2 c42 li-bullet-0"><span>GradientHardClipping: </span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.html%23torch.nn.utils.clip_grad_value_&amp;sa=D&amp;source=editors&amp;ust=1615451627894000&amp;usg=AOvVaw28syNvG3mxL4jCdc8hhcfO">torch.nn.utils.clip_grad_value_</a></span></li><li class="c2 c42 li-bullet-0"><span class="c0">GradientNoise: N/A</span></li><li class="c2 c42 li-bullet-0"><span>GradientLARS: N/A (see </span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://github.com/pytorch/pytorch/issues/18414&amp;sa=D&amp;source=editors&amp;ust=1615451627894000&amp;usg=AOvVaw2M6QZf_E5wIiFRbmLE7zMF">pytorch #18414</a></span><span>&nbsp;and </span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://github.com/noahgolmant/pytorch-lars&amp;sa=D&amp;source=editors&amp;ust=1615451627894000&amp;usg=AOvVaw2dvFIIsY7kE0ZBnwaauY1I">pytorch-lars</a></span><span>)</span></li></ul><h2 class="c39 c25" id="h.79n4spowwnun"><span class="c10">Training PyTorch model using Chainer</span></h2><p class="c2"><span>To quickly try a PyTorch model in a training script using Chainer, </span><span class="c11 c13"><a class="c9" href="#h.jgakn54a8peo">cpm.TorchModule</a></span><span class="c0">&nbsp;is the tool to use. Assuming you have a training script using Chainer, you have to try the following steps:</span></p><p class="c2 c17"><span class="c0"></span></p><ul class="c64 lst-kix_38adif7m6rbx-0 start"><li class="c2 c42 li-bullet-0"><span>Replace the model to train with </span><span class="c13">cpm.TorchModule(module_you_want_to_use)</span><span>. Use </span><span class="c13">to_gpu</span><span class="c0">&nbsp;to transfer the variables to a GPU device.</span></li><li class="c2 c42 li-bullet-0"><span class="c0">Rewrite the loss computation and backprop call with PyTorch.</span></li></ul><ul class="c64 lst-kix_38adif7m6rbx-1 start"><li class="c2 c53 li-bullet-0"><span>If you are using StandardUpdater, make its subclass and override </span><span class="c13">update_core</span><span class="c0">. Write loss calculation and backprop call in PyTorch.</span></li></ul><ul class="c64 lst-kix_38adif7m6rbx-2 start"><li class="c2 c99 li-bullet-0"><span class="c78">NOTE</span><span>: Once you compute the gradient in PyTorch, it is automatically reflected to Chainer parameters, so it is valid to just call </span><span class="c13">optimizer.update()</span><span class="c0">&nbsp;after that.</span></li></ul><ul class="c64 lst-kix_38adif7m6rbx-1"><li class="c2 c53 li-bullet-0"><span>If you are using a custom training loop, rewrite the gradient computation part in PyTorch. See the above NOTE, too.</span></li></ul><h2 class="c39 c25" id="h.33jrfcvcf9zh"><span class="c10">Distributed training </span></h2><p class="c2"><span>As of writing, there are two major ways to run distributed deep learning applications:</span><span class="c13">&nbsp;torch.distributed</span><span>&nbsp;and Horovod. We recommend</span><span class="c13">&nbsp;torch.distributed</span><span class="c0">&nbsp;as a first option because of the following reasons.</span></p><p class="c2 c17"><span class="c0"></span></p><ol class="c64 lst-kix_8dw3ycrgt362-0 start" start="1"><li class="c2 c42 li-bullet-0"><span class="c13">torch.distributed</span><span class="c0">&nbsp;is a part of standard modules of PyTorch.</span></li><li class="c2 c42 li-bullet-0"><span class="c0">It supports some advanced features that Horovod doesn&rsquo;t, such as multi-node batch normalization (e.g. inter-process batch normalization)</span></li></ol><p class="c2 c17"><span class="c0"></span></p><p class="c2"><span class="c0">In this document, we describe both approaches to migrate ChainerMN programs to PyTorch.</span></p><h3 class="c46 c25 c89" id="h.91x1ehh487en"><span class="c8"></span></h3><h3 class="c46 c25" id="h.iyhfmfndl13j"><span class="c8">Pytorch model using torch.distributed&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></h3><p class="c2"><span class="c0">Torch.distributed is the standard module for distributed deep learning of PyTorch. </span></p><p class="c2 c17"><span class="c0"></span></p><p class="c2"><span class="c0">Torch.distributed supports three backends: &ldquo;nccl&rdquo;, &ldquo;mpi&rdquo; and &ldquo;gloo&rdquo;. For users who are migrating from Chainer and ChainerMN and have been using NCCL with MPI, using &ldquo;nccl&rdquo; backend is the most straightforward way. In this section, we assume that you use NCCL and MPI to run your distributed deep learning programs. In particular we assume Open MPI as the MPI implementation used here because it is the recommended option in ChainerMN, but other MPI implementations are mentioned as well.</span></p><h4 class="c46 c25" id="h.moliknyk6ru3"><span class="c38">Invocation</span></h4><p class="c2"><span>In ChainerMN, process invocation is totally coordinated by the MPI runtime. However, in PyTorch and torch.distributed, you may need a few more steps to invoke distributed deep learning processes. The simplest initialization method might be </span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/distributed.html%23environment-variable-initialization&amp;sa=D&amp;source=editors&amp;ust=1615451627899000&amp;usg=AOvVaw1dn2iwL82bUed_1wElLWV_">environment variable initialization</a></span><span class="c0">. </span></p><p class="c2 c17"><span class="c0"></span></p><p class="c2"><span>The following environmental variables are necessary (whatever system you use to invoke your script, including MPI).</span><span>&nbsp;Other variables, </span><span class="c13">WORLD_SIZE</span><span>&nbsp;and</span><span class="c13">&nbsp;RANK,</span><span>&nbsp;are set from inside the following snippet.</span></p><p class="c2 c17"><span class="c0"></span></p><p class="c2"><span class="c13">MASTER_ADDR : </span><span class="c0">Address of the computing node where the rank 0 process runs.</span></p><p class="c2"><span class="c13">MASTER_PORT : </span><span class="c0">A free port of the MASTER_ADDR machine. The port will be used by &nbsp;the rank 0 process.</span></p><p class="c2 c17"><span class="c0"></span></p><p class="c2 c17"><span class="c0"></span></p><p class="c2"><span>Note that process invocation is highly system-dependent issue. PyTorch supports other options such as </span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/distributed.html%23tcp-initialization&amp;sa=D&amp;source=editors&amp;ust=1615451627900000&amp;usg=AOvVaw2mFxt_djyZ4jUk_5LPDb5W">TCP initialization</a></span><span>&nbsp;and </span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/distributed.html%23shared-file-system-initialization&amp;sa=D&amp;source=editors&amp;ust=1615451627900000&amp;usg=AOvVaw3FbbZxscl6DhzFusQGfEnE">shared file-system initialization</a></span><span class="c0">. Please refer to the official documents for more details.</span></p><p class="c2 c17"><span class="c0"></span></p><h4 class="c46 c25" id="h.7r5y5xoqrywb"><span class="c38">Initialization</span></h4><p class="c2"><span class="c0">The following code snippets shows how to initialize torch.distributed module.</span></p><p class="c2 c17"><span class="c1"></span></p><a id="t.bca77ae00997d02300bdcc0347eb376ffd57872b"></a><a id="t.22"></a><table class="c55"><tbody><tr class="c28"><td class="c40" colspan="1" rowspan="1"><p class="c2"><span class="c1"># setup env for torch.distributed</span></p><p class="c2"><span class="c1">comm_world_size = int(os.environ[&quot;OMPI_COMM_WORLD_SIZE&quot;])</span></p><p class="c2"><span class="c1">comm_rank = int(os.environ[&quot;OMPI_COMM_WORLD_RANK&quot;])</span></p><p class="c2"><span class="c1">comm_local_rank = int(os.environ[&#39;OMPI_COMM_WORLD_LOCAL_RANK&#39;])</span></p><p class="c2 c17"><span class="c1"></span></p><p class="c2"><span class="c1">os.environ[&quot;WORLD_SIZE&quot;] = str(comm_world_size)</span></p><p class="c2"><span class="c1">os.environ[&quot;RANK&quot;] = str(comm_rank)</span></p><p class="c2"><span class="c1">torch.cuda.set_device(comm_local_rank)</span></p><p class="c2"><span class="c1">torch.distributed.init_process_group(backend=&#39;nccl&#39;, init_method=&#39;env://&#39;)</span></p></td></tr></tbody></table><p class="c2 c17"><span class="c0"></span></p><p class="c2"><span class="c0">Environmental variables set by MPI runtime are here, instead of communicator.intra_rank in ChainerMN because torch.distributed does not provide corresponding rank information. If you use MVAPICH2, use MV2_COMM_WORLD_SIZE, MV2_COMM_WORLD_RANK, MV2_COMM_WORLD_LOCAL_RANK respectively. </span></p><p class="c2 c17"><span class="c0"></span></p><h4 class="c46 c25" id="h.nzh1eoy3bny9"><span class="c38">Dataset scattering</span></h4><p class="c2"><span>Each node can get a slice of a globally shared dataset using a </span><span class="c13">DistributedSampler</span><span>.</span></p><a id="t.89ef2d99828a1b22383a2ae56fba3bdc817aa41c"></a><a id="t.23"></a><table class="c55"><tbody><tr class="c28"><td class="c40" colspan="1" rowspan="1"><p class="c21"><span class="c35 c13">sampler = torch.utils.data.distributed.DistributedSampler(dataset, </span></p><p class="c21"><span class="c35 c13">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; num_replicas=comm_world_size,</span></p><p class="c21"><span class="c35 c13">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; rank=comm_rank)</span></p><p class="c21 c17"><span class="c35 c13"></span></p><p class="c21"><span class="c35 c13">loader_kwargs = {&#39;num_workers&#39;: 1, &#39;pin_memory&#39;: True} &nbsp;# Assuming we use GPUs</span></p><p class="c21"><span class="c35 c13">loader = torch.utils.data.DataLoader(train_dataset,</span></p><p class="c21"><span class="c35 c13">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;batch_size=args.batch_size, </span></p><p class="c21"><span class="c35 c13">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;sampler=sampler, **loader_kwargs)</span></p></td></tr></tbody></table><p class="c21 c17"><span class="c0"></span></p><p class="c2"><span>This will make every worker to only load a slice of the dataset, this sampler can be normally fed to the </span><span class="c13">DataLoader</span><span class="c0">.</span></p><p class="c2 c17"><span class="c0"></span></p><p class="c2"><span>Also, you need to call</span><span class="c13">&nbsp;DistributedSampler.set_epoch()</span><span class="c0">&nbsp;to adjust epoch numbers. &nbsp;Thus typical training loop looks like:</span></p><p class="c2 c17"><span class="c35 c13"></span></p><a id="t.bbd36d8e629516ffbb814279727dc47778519869"></a><a id="t.24"></a><table class="c55"><tbody><tr class="c28"><td class="c40" colspan="1" rowspan="1"><p class="c21"><span class="c35 c13">for epoch in range(1, args.epochs + 1):</span></p><p class="c21"><span class="c35 c13">&nbsp; &nbsp; train_sampler.set_epoch(epoch)</span></p><p class="c21"><span class="c35 c13">&nbsp; &nbsp; train(args, model, device, train_loader, optimizer, epoch)</span></p><p class="c21"><span class="c35 c13">&nbsp; &nbsp; test(args, model, device, test_loader, len(test_dataset))</span></p><p class="c21"><span class="c35 c13">&nbsp; &nbsp; scheduler.step()</span></p></td></tr></tbody></table><p class="c21 c17"><span class="c0"></span></p><p class="c2 c17"><span class="c1"></span></p><h4 class="c46 c25" id="h.kyzvorr2x2ij"><span class="c38">Data transfer to devices</span></h4><p class="c2"><span>We need to specify the device to which the data is transferred using</span><span class="c13">&nbsp;comm_local_rank</span><span class="c0">. </span></p><p class="c2 c17"><span class="c35 c13"></span></p><a id="t.febd87097d6acdc6ac335c2829150af17406f2f5"></a><a id="t.25"></a><table class="c55"><tbody><tr class="c28"><td class="c40" colspan="1" rowspan="1"><p class="c21"><span class="c35 c13">class MyNN(nn.module):</span></p><p class="c21"><span class="c13 c35">&nbsp; &nbsp; ...</span></p><p class="c21 c17"><span class="c35 c13"></span></p><p class="c21"><span class="c35 c13">device = torch.device(&quot;cuda:{}&quot;.format(comm_local_rank) if use_cuda else &quot;cpu&quot;)</span></p><p class="c21"><span class="c35 c13">model = MyNN().to(device)</span></p><p class="c21"><span class="c35 c13">model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[comm_local_rank])</span></p></td></tr></tbody></table><p class="c2 c17"><span class="c0"></span></p><h4 class="c46 c25" id="h.s0hd5ycxwpdh"><span class="c38">Optimizer wrapping</span></h4><p class="c2"><span>In contrast to </span><span class="c11"><a class="c9" href="#h.z7g1nmbbb3uh">Horovod</a></span><span class="c0">, We can use the same optimizer as in non-distributed execution. </span></p><p class="c2 c17"><span class="c0"></span></p><h4 class="c46 c25" id="h.d1nulkakooh"><span class="c38">Initial values broadcast</span></h4><p class="c2"><span>Parameter values are synchronized (i.e. initial broadcast and allreduce in every iteration) automatically by</span><span class="c13">&nbsp;DistributedDataParallel</span><span class="c0">&nbsp;class and thus no further modification is necessary.</span></p><p class="c2 c17"><span class="c0"></span></p><h4 class="c46 c25" id="h.x5dny5mqq9px"><span class="c38">Metrics average and reductions</span></h4><h4 class="c46 c25" id="h.5t9pfndndqnr"><span class="c66"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/distributed.html%23multi-gpu-collective-functions&amp;sa=D&amp;source=editors&amp;ust=1615451627907000&amp;usg=AOvVaw1-8raOsQQezJFUiPPisQH-">https://pytorch.org/docs/stable/distributed.html#multi-gpu-collective-functions</a></span></h4><p class="c2 c17"><span class="c0"></span></p><h4 class="c46 c25" id="h.3lb7ncfjheoo"><span class="c38">Synchronization</span></h4><p class="c2"><span class="c0">To avoid potential data races other kinds of bugs, you may need to use torch.distributed.barrier() to synchronize processes before or after data loading, and finishing the application. </span></p><p class="c2 c17"><span class="c0"></span></p><p class="c2 c17"><span class="c0"></span></p><h3 class="c46 c25 c89" id="h.fm07a8ui8sm1"><span class="c8"></span></h3><h3 class="c46 c25" id="h.dis62849w9cz"><span class="c8">PyTorch model using Horovod</span></h3><p class="c2"><span>PyTorch can use </span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://github.com/horovod/horovod&amp;sa=D&amp;source=editors&amp;ust=1615451627908000&amp;usg=AOvVaw3uIb-_gEYLTNhCJMwayqnJ">Horovod</a></span><span>&nbsp;to do Data Parallel training in a similar way to ChainerMN.<br>Data is distributed across the nodes and the optimizer is wrapped in with </span><span>Horovod</span><span class="c0">&nbsp;to automatically average the gradients of several MPI processes.</span></p><h4 class="c46 c25 c86" id="h.o3x9ygfly4q"><span class="c38"></span></h4><h4 class="c46 c25" id="h.tje6mmfdx3pf"><span class="c38">Horovod initialization</span></h4><p class="c2"><span>The following snippet shows how to import horovod and retrieve the current worker id and the total number of workers.</span></p><a id="t.1f187a1cc2a0f3f05314bb5efd0eca754b829707"></a><a id="t.26"></a><table class="c55"><tbody><tr class="c28"><td class="c40" colspan="1" rowspan="1"><p class="c2"><span class="c1">import horovod.torch as hvd</span></p><p class="c2"><span class="c1">hvd.init()</span></p><p class="c2"><span class="c1">print(&lsquo;My rank is {} of {} workers&lsquo;.format(hvd.rank(), hvd.size()))</span></p></td></tr></tbody></table><p class="c2 c17"><span class="c0"></span></p><p class="c2"><span class="c13">hvd.local_rank() </span><span>is used to get the rank inside a single node, this is useful to assign GPUs, similar to ChainerMN&rsquo;s</span><span class="c13">&nbsp;intra_rank()</span><span>.</span></p><a id="t.2947184f677b00b78c938df5ebaf8a3de12018f5"></a><a id="t.27"></a><table class="c55"><tbody><tr class="c105"><td class="c40" colspan="1" rowspan="1"><p class="c2"><span class="c35 c13 c73">torch.cuda.set_device(hvd.local_rank())</span></p></td></tr></tbody></table><p class="c2 c17"><span class="c0"></span></p><h4 class="c46 c25" id="h.37g288q2db5i"><span class="c38">Dataset scattering</span></h4><p class="c2"><span>Each node can get a slice of a globally shared dataset using a </span><span class="c13">DistributedSampler</span><span>.</span></p><a id="t.49bd94ec55cd43d4b62152a8a3740c05dbfb8bf1"></a><a id="t.28"></a><table class="c55"><tbody><tr class="c28"><td class="c40" colspan="1" rowspan="1"><p class="c21"><span class="c13 c54">torch.utils.data.distributed.DistributedSampler(dataset, </span><span class="c13">num_replicas=</span><span class="c35 c13">hvd.size(), </span></p><p class="c21"><span class="c13">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; rank=</span><span class="c35 c13">hvd.rank())</span></p></td></tr></tbody></table><p class="c21 c17"><span class="c0"></span></p><p class="c2"><span>This will make every worker to only load a slice of the dataset, this sampler can be normally fed to the </span><span class="c13">DataLoader</span><span class="c0"><br></span></p><h4 class="c46 c25" id="h.z7g1nmbbb3uh"><span class="c38">Optimizer wrapping</span></h4><p class="c2"><span>The optimizer is wrapped in a </span><span class="c13">hvd.DistributedOptimizer</span><span>&nbsp;object with the following configuration parameters.<br><br></span><span class="c13">compression </span><span>: value in</span><span class="c1">&nbsp;{hvd.Compression.fp16, hvd.Compression.none}<br></span></p><p class="c2"><span>compression is used to reduce the size of the allreduce operations performed by the optimizer.<br><br></span><span class="c13">backward_passes_per_step : int </span><span class="c0">default value usually 1</span></p><p class="c2 c17"><span class="c1"></span></p><p class="c2"><span>Number of batches that are performed locally before performing the gradients exchange.</span></p><a id="t.e7c44ee3b69544418ca97691b43afb5b46f1e908"></a><a id="t.29"></a><table class="c55"><tbody><tr class="c28"><td class="c40" colspan="1" rowspan="1"><p class="c2"><span class="c1">optimizer = hvd.DistributedOptimizer(</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; optimizer, named_parameters=model.named_parameters(),</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; compression=compression,</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; backward_passes_per_step=args.batches_per_allreduce)</span></p></td></tr></tbody></table><p class="c2"><span><br>From the documentation:<br><br></span><span class="c44">DistributedOptimizer exposes the </span><span class="c58 c63">synchronize()</span><span class="c44">&nbsp;method, which forces allreduce operations to finish before continuing the execution. It&rsquo;s useful in conjunction with gradient clipping, or other operations that modify gradients in place before </span><span class="c63 c58">step()</span><span class="c44">is executed. Make sure to use </span><span class="c63 c58">optimizer.skip_synchronize()</span><span class="c44">&nbsp;if you&rsquo;re calling </span><span class="c63 c58">synchronize()</span><span class="c44">&nbsp;in your code.</span></p><p class="c2 c17"><span class="c0"></span></p><h4 class="c46 c25" id="h.gq621f9b16zf"><span class="c38">Initial values broadcast</span></h4><p class="c2"><span>Before starting the training loop, initial model parameters and the optimizer state must be broadcasted to all the workers:</span></p><a id="t.d9c5fa6d0a478715e92abcf59d1d7d36e3acde5b"></a><a id="t.30"></a><table class="c55"><tbody><tr class="c28"><td class="c40" colspan="1" rowspan="1"><p class="c2"><span class="c1">hvd.broadcast_parameters(model.state_dict(), root_rank=0)</span></p><p class="c2"><span class="c1">hvd.broadcast_optimizer_state(optimizer, root_rank=0)</span></p></td></tr></tbody></table><h4 class="c46 c25 c86" id="h.7hiogyr5jasg"><span class="c38"></span></h4><h4 class="c46 c25" id="h.9t9k4lfat6t2"><span class="c38">Metrics average and reductions</span></h4><p class="c2"><span class="c0">When computing the loss and other metrics such as accuracy, the values of multiple workers can be explicitly exchanged to compute averages:</span></p><a id="t.7b00fd4181f942caf888937294ff90e6381ac1d0"></a><a id="t.31"></a><table class="c55"><tbody><tr class="c28"><td class="c40" colspan="1" rowspan="1"><p class="c18"><span class="c13 c73">self.sum += hvd.allreduce(val.detach().cpu(), name=metric_name)</span></p></td></tr></tbody></table><p class="c2 c17"><span class="c0"></span></p><p class="c2"><span class="c0">Horovod has support to exchange data using other MPI collectives:</span></p><ul class="c64 lst-kix_rsr7bmzen4a7-0 start"><li class="c2 c42 li-bullet-0"><span class="c1">horovod.torch.allgather</span></li><li class="c2 c42 li-bullet-0"><span class="c1">horovod.torch.broadcast</span></li></ul><p class="c2"><span>There are _async versions of the three functions that can be queried using </span><span class="c13">poll()</span><span>&nbsp;on the returned handler or </span><span class="c13">synchronize()</span><span class="c0">&nbsp;to wait till completion.</span></p><p class="c2 c17"><span class="c0"></span></p><h4 class="c46 c25" id="h.bfpbumwh7xzs"><span class="c38">Horovod code structure</span></h4><a id="t.e2ebc92c8c2223cbc3ddccd433f6d952be2ae85c"></a><a id="t.32"></a><table class="c55"><tbody><tr class="c28"><td class="c40" colspan="1" rowspan="1"><p class="c18"><span class="c29 c13">import torch</span></p><p class="c18"><span class="c29 c13">import horovod.torch as hvd</span></p><p class="c18"><span class="c29 c13">&hellip;</span></p><p class="c18"><span class="c29 c13">&hellip;</span></p><p class="c18"><span class="c29 c13">def main():</span></p><p class="c18"><span class="c13 c58">&nbsp; &nbsp; </span><span class="c61 c13 c58 c67"># Initialize horovod</span></p><p class="c2"><span class="c29 c13">&nbsp; &nbsp; hvd.init()</span></p><p class="c2"><span class="c13 c58">&nbsp; &nbsp; </span><span class="c29 c13 c73">torch.cuda.set_device(hvd.local_rank())</span></p><p class="c2"><span class="c13 c58 c73">&nbsp; &nbsp; </span><span class="c61 c13 c58 c73 c67"># Read the dataset and create the iterators</span></p><p class="c2"><span class="c29 c13 c73">&nbsp; &nbsp; dataset = datasets.ImageFolder(&hellip;)</span></p><p class="c2"><span class="c13 c58 c73">&nbsp; &nbsp; train_sampler = </span><span class="c29 c13">torch.utils.data.distributed.DistributedSampler(dataset, num_replicas=hvd.size(), </span></p><p class="c21"><span class="c29 c13">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; rank=hvd.rank())</span></p><p class="c21"><span class="c13 c58">&nbsp; &nbsp; loader = torch.utils.data.DataLoader(dataset, sampler=train_sampler, &hellip;)<br> &nbsp; &nbsp;</span><span class="c61 c13 c58 c67"># Set up the model, checkpoints, &hellip;</span></p><p class="c21"><span class="c29 c13">&nbsp; &nbsp; &hellip;</span></p><p class="c21"><span class="c13 c58">&nbsp; &nbsp; </span><span class="c61 c13 c58 c67"># Create the optimizer</span></p><p class="c21"><span class="c29 c13">&nbsp; &nbsp; optimizer = optim.SGD(model.parameters(), &hellip;)</span></p><p class="c21"><span class="c29 c13">&nbsp; &nbsp; optimizer = hvd.DistributedOptimizer(</span></p><p class="c21"><span class="c29 c13">&nbsp; &nbsp; &nbsp; &nbsp; optimizer, named_parameters=model.named_parameters(),</span></p><p class="c21"><span class="c13 c29">&nbsp; &nbsp; &nbsp; &nbsp; compression= hvd.Compression.none,</span></p><p class="c21"><span class="c29 c13">&nbsp; &nbsp; &nbsp; &nbsp; backward_passes_per_step=args.batches_per_allreduce)<br></span></p><p class="c21"><span class="c13 c58">&nbsp; &nbsp; </span><span class="c61 c13 c58 c67"># Broadcast initial state</span></p><p class="c21"><span class="c61 c13 c58 c54">&nbsp; &nbsp; broadcast parameters &amp; optimizer state.</span></p><p class="c21"><span class="c61 c13 c58 c54">&nbsp; &nbsp; hvd.broadcast_parameters(model.state_dict(), root_rank=0)</span></p><p class="c21"><span class="c61 c13 c58 c54">&nbsp; &nbsp; hvd.broadcast_optimizer_state(optimizer, root_rank=0)</span></p><p class="c21"><span class="c13 c58 c54">&nbsp; &nbsp; </span><span class="c61 c13 c58 c67"># Start training</span></p><p class="c21"><span class="c13 c58 c54 c61">&nbsp; &nbsp; for epoch in range(epochs):</span></p><p class="c21"><span class="c61 c13 c58 c54">&nbsp; &nbsp; &nbsp; &nbsp; train_sampler.set_epoch(epoch)</span></p><p class="c21"><span class="c61 c13 c58 c54">&nbsp; &nbsp; &nbsp; &nbsp; &hellip; </span></p></td></tr></tbody></table><p class="c2 c17"><span class="c1"></span></p><h4 class="c46 c25" id="h.apxtaciyccdc"><span class="c38">Obtaining Horovod traces to measure performance</span></h4><p class="c2"><span>Communication traces showing Horovod communications can be obtained by setting the </span><span class="c13">HOROVOD_TIMELINE</span><span class="c0">&nbsp;environment variable.</span></p><a id="t.64ecae9ad815cf624f1b9ed5b6a24476d88e0a59"></a><a id="t.33"></a><table class="c55"><tbody><tr class="c28"><td class="c40" colspan="1" rowspan="1"><p class="c18"><span class="c1">mpirun -bind-to-none -np 8 -x HOROVOD_TIMELINE=timeline.json ...</span></p></td></tr></tbody></table><p class="c2 c17"><span class="c0"></span></p><p class="c2"><span class="c0">The resultant trace can be visualized in Chrome by using the browser built-in chrome://tracing feature.</span></p><p class="c2 c17"><span class="c0"></span></p><h4 class="c46 c25" id="h.9jn0jqinrgx3"><span class="c38">Tuning Horovod performance</span></h4><p class="c2"><span class="c0">Horovod has several knobs to improve its performance</span></p><p class="c2 c17"><span class="c0"></span></p><ul class="c64 lst-kix_9i19e8txouig-0 start"><li class="c2 c42 li-bullet-0"><span>TensorFusion improves network utilization when there are many operations to be done on small tensors. Instead of eagerly starting multiple small reductions, horovod combines the small tensors on a buffer and then batch operations in order to perform a more efficient communication/computation overlap. </span><span class="c13 c54 c58">HOROVOD_FUSION_THRESHOLD</span><span class="c56 c58 c54">&nbsp;</span><span class="c58 c54">controls the size of the buffer in bytes (default is 64MB). </span><span class="c13 c58 c54">HOROVOD_CYCLE_TIME </span><span class="c22 c54 c75">specifies the amount of time in ms to wait for tensors to be batched before reducing them.</span></li><li class="c2 c42 li-bullet-0"><span class="c58 c54">Hierarchical Reduce: </span><span class="c13 c58 c54">HOROVOD_HIERARCHICAL_ALLREDUCE</span><span class="c22 c54 c75">&nbsp;does a hybrid approach where allreduce in-node is done by NCCL, and allreduce across nodes is done by MPI.</span></li><li class="c2 c42 li-bullet-0"><span class="c13 c58 c54">HOROVOD_AUTOTUNE </span><span class="c58 c54">can be used for horovod to perform a Bayesian optimization on all the configurable parameters at the first epochs of training. More info </span><span class="c11 c58"><a class="c9" href="https://www.google.com/url?q=https://github.com/horovod/horovod/blob/83eaa163b395ae8866a404f94facf82cc8127642/docs/autotune.rst&amp;sa=D&amp;source=editors&amp;ust=1615451627923000&amp;usg=AOvVaw1wRthkWe_2bdaGLUVnkLAj">here</a></span></li></ul><h4 class="c46 c25 c86" id="h.81xyyty2r9qq"><span class="c38"></span></h4><h4 class="c46 c25" id="h.wuax6jpon8sy"><span class="c38">Using Horovod with apex</span></h4><p class="c2"><span>Horovod launches all-reduce in parallel with backward computation, and </span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://github.com/NVIDIA/apex&amp;sa=D&amp;source=editors&amp;ust=1615451627924000&amp;usg=AOvVaw3Sj9MNYSYXRk-SPh8vYpoA">apex</a></span><span class="c0">&nbsp;unscales gradient after backward computation.</span></p><p class="c2"><span class="c0">To avoid race conditions, we have to wait for all-reduce completion before unscaling:</span></p><a id="t.6f2636f4210fb90c92d9a363f1d87ad8d31bcaab"></a><a id="t.34"></a><table class="c55"><tbody><tr class="c28"><td class="c40" colspan="1" rowspan="1"><p class="c18"><span class="c1">from apex import amp</span></p><p class="c18"><span class="c1">...</span></p><p class="c18"><span class="c1">with amp.scale_loss(loss, optimizer) as scaled_loss:</span></p><p class="c18"><span class="c1">&nbsp; &nbsp; scaled_loss.backward()</span></p><p class="c18"><span class="c1">&nbsp; &nbsp; optimizer.synchronize() &nbsp;# Wait for all-reduce completion</span></p><p class="c18"><span class="c1">with optimizer.skip_synchronize():</span></p><p class="c18"><span class="c1">&nbsp; &nbsp; optimizer.step()</span></p></td></tr></tbody></table><p class="c2 c17"><span class="c0"></span></p><p class="c2"><span>Also, </span><span class="c13">backward_passes_per_step </span><span>should be 1 when using Horovod and apex. The current implementation of Horovoda and apex do not work as expected when </span><span class="c13">backward_passes_per_step</span><span class="c0">&nbsp;is not 1.</span></p><p class="c2 c17"><span class="c0"></span></p><h4 class="c46 c25" id="h.pdah33gpi4li"><span class="c38">Multi-Node Batch Normalization in Horovod</span></h4><p class="c2"><span>Horovod does not yet officially support MNBN (</span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://github.com/horovod/horovod/issues/1384&amp;sa=D&amp;source=editors&amp;ust=1615451627928000&amp;usg=AOvVaw2mGpu9Lvkx-SfeblImY8Jc">https://github.com/horovod/horovod/issues/1384</a></span><span>), but there exists an unofficial implementation: </span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://github.com/atranitell/Synchronized-BatchNorm-PyTorch-Horovod/blob/master/sync_bn.py&amp;sa=D&amp;source=editors&amp;ust=1615451627928000&amp;usg=AOvVaw2JZNWxghGVdG9u7k16S3q5">https://github.com/atranitell/Synchronized-BatchNorm-PyTorch-Horovod/blob/master/sync_bn.py</a></span><span>. </span><span>Apex also has an implementation: </span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://nvidia.github.io/apex/parallel.html%23apex.parallel.SyncBatchNorm&amp;sa=D&amp;source=editors&amp;ust=1615451627928000&amp;usg=AOvVaw1cgRr-gFJRbvJZO_NZyl4S">https://nvidia.github.io/apex/parallel.html#apex.parallel.SyncBatchNorm</a></span></p><p class="c2 c17"><span class="c0"></span></p><h4 class="c46 c25" id="h.eqj7dzgbr9y7"><span class="c38">Gathering arbitrary objects using Horovod and mpi4py</span></h4><p class="c2"><span>Horovod supports simultaneous usage with mpi4py (</span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://github.com/horovod/horovod%23mpi4py&amp;sa=D&amp;source=editors&amp;ust=1615451627929000&amp;usg=AOvVaw08IEDKd__8TAahKCyWG9e1">https://github.com/horovod/horovod#mpi4py</a></span><span>). You can directly work with mpi4py to e.g. rewrite ChainerMN&#39;s </span><span class="c23">comm.gather_obj</span><span>:</span></p><a id="t.18c04d1a8953c91d68759f73f1d8c923e9d7fc66"></a><a id="t.35"></a><table class="c55"><tbody><tr class="c28"><td class="c40" colspan="1" rowspan="1"><p class="c18"><span class="c43 c56 c7">import horovod.torch as hvd</span></p><p class="c18 c17"><span class="c43 c56 c7"></span></p><p class="c18"><span class="c43 c56 c7"># Initialize Horovod</span></p><p class="c18"><span class="c43 c56 c7">hvd.init()</span></p><p class="c18 c17"><span class="c43 c56 c7"></span></p><p class="c18"><span class="c43 c56 c7"># Verify that MPI multi-threading is supported.</span></p><p class="c18"><span class="c43 c56 c7">assert hvd.mpi_threads_supported()</span></p><p class="c18 c17"><span class="c43 c56 c7"></span></p><p class="c18"><span class="c43 c56 c7">from mpi4py import MPI<br>mpi_comm = MPI.COMM_WORLD</span></p><p class="c18"><span class="c43 c56 c7">assert hvd.size() == mpi_comm.Get_size()</span></p><p class="c18"><span class="c54 c7 c56">mpi_comm.gather(obj, root=0) &nbsp;# This is equal to ChainerMN&rsquo;s comm.gather_obj</span></p></td></tr></tbody></table><p class="c2 c17"><span class="c0"></span></p><h4 class="c46 c25" id="h.y8eom3uzrv56"><span class="c38">Alternatives to Horovod</span></h4><p class="c2"><span class="c0">Horovod is introduced here because it greatly resembles ChainerMN and can be used in our computing infrastructure right away. Alternatives are:<br></span></p><ul class="c64 lst-kix_dnm4ke3nhj1e-0 start"><li class="c2 c42 li-bullet-0"><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/tutorials/intermediate/dist_tuto.html&amp;sa=D&amp;source=editors&amp;ust=1615451627932000&amp;usg=AOvVaw0wDXEhvw-JWGEK3dSwbFFa">Pytorch distributed API</a></span></li><li class="c2 c42 li-bullet-0"><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://williamfalcon.github.io/pytorch-lightning/Trainer/Distributed%2520training/%23Multi-node&amp;sa=D&amp;source=editors&amp;ust=1615451627933000&amp;usg=AOvVaw3nQNwf44rYizkm4_9IttGS">Lightning</a></span></li></ul><p class="c2 c17"><span class="c0"></span></p><h3 class="c46 c25" id="h.qfxp9de828j4"><span class="c8">Chainer model using Horovod</span></h3><p class="c2"><span>To train chainer models in distributed environments using Horovod, the chainer link should be wrapped using </span><span class="c11 c13"><a class="c9" href="#h.46et3pwf65s">cpm.LinkAsTorchModel</a></span><span class="c13">. </span><span class="c0">The use of a PyTorch optimizer is required.</span></p><a id="t.a248f011d1b3a5976ee0a1b79acb09d221b6c0f7"></a><a id="t.36"></a><table class="c55"><tbody><tr class="c28"><td class="c40" colspan="1" rowspan="1"><p class="c18"><span class="c1">model = ChainerModel()</span></p><p class="c18"><span class="c1">model.to_device(ch_device)</span></p><p class="c18 c17"><span class="c1"></span></p><p class="c18"><span class="c61 c13 c67 c69"># Initialize parameters before converting to `ChainerParameter`s.</span></p><p class="c18"><span class="c1">model(ch_device.xp.zeros((1, 784)).astype(&#39;f&#39;))</span></p><p class="c18 c17"><span class="c1"></span></p><p class="c18"><span class="c61 c13 c69 c67"># Convert parameters to `ChainerParameter`s to share memory with PyTorch.</span></p><p class="c18"><span class="c1">torched_model = cpm.LinkAsTorchModel(model)</span></p><p class="c18 c17"><span class="c1"></span></p><p class="c18"><span class="c1">optimizer = optim.SGD(torched_model.parameters(), lr=args.lr)</span></p><p class="c18 c17"><span class="c1"></span></p><p class="c18"><span class="c1">optimizer = hvd.DistributedOptimizer(</span></p><p class="c18"><span class="c1">&nbsp; &nbsp; optimizer, named_parameters=torched_model.named_parameters())</span></p><p class="c18"><span class="c1">hvd.broadcast_parameters(torched_model.state_dict(), root_rank=0)</span></p><p class="c18"><span class="c13">hvd.broadcast_optimizer_state(optimizer, root_rank=0)</span></p></td></tr></tbody></table><p class="c2 c17"><span class="c0"></span></p><h3 class="c46 c25" id="h.vlwpxcx5nw9c"><span>Py</span><span class="c8">Torch model using ChainerMN</span></h3><p class="c2"><span class="c0">Using the cpm tool it is also possible to train a PyTorch model using ChainerMN.</span></p><p class="c2"><span class="c0">The current support is limited only to data parallel training.</span></p><a id="t.0eeb743abfa107ec882dd18dcbb467ec22d093d1"></a><a id="t.37"></a><table class="c55"><tbody><tr class="c28"><td class="c40" colspan="1" rowspan="1"><p class="c18"><span class="c13 c58 c73">from</span><span class="c13 c58 c73">&nbsp;chainer_pytorch_migration </span><span class="c13 c58 c73">import</span><span class="c29 c13 c73">&nbsp;chainermn</span></p><p class="c18 c17"><span class="c43 c23 c7"></span></p><p class="c18"><span class="c43 c23 c7">&hellip;</span></p><p class="c18"><span class="c23 c54 c7 c73">comm </span><span class="c23 c7 c73 c110">=</span><span class="c23 c54 c7 c73">&nbsp;chainermn.create_communicator(</span><span class="c23 c7 c73 c91">&#39;pure_nccl&#39;</span><span class="c43 c23 c7">)<br></span></p><p class="c18"><span class="c43 c23 c7"># Set up standard ResNet-50 model.</span></p><p class="c18"><span class="c43 c23 c7">model = models.resnet50()</span></p><p class="c18"><span class="c43 c23 c7">model.cuda()</span></p><p class="c18"><span class="c43 c23 c7">w_model = links.TorchModule(model)</span></p><p class="c18"><span class="c43 c23 c7">w_model.to_gpu(device)</span></p><p class="c18 c17"><span class="c43 c23 c7"></span></p><p class="c18 c17"><span class="c43 c23 c7"></span></p><p class="c18"><span class="c43 c23 c7">optimizer = optim.SGD(model.parameters(), lr=lr)</span></p><p class="c18 c17"><span class="c43 c23 c7"></span></p><p class="c18"><span class="c43 c23 c7">optimizer = chainermn.create_multi_node_optimizer(optimizer, comm)</span></p><p class="c18"><span class="c23 c7 c43">optimizer.setup(w_model)</span></p></td></tr></tbody></table><p class="c2 c17"><span class="c0"></span></p><h2 class="c39 c25" id="h.2jl4lfh90jqb"><span class="c10">Porting code that edits the computational graph</span></h2><h3 class="c46 c25" id="h.gbqjtjbzzzno"><span class="c8">Unchaining nodes </span></h3><p class="c2"><span class="c0">Explains differences of how variables can be unchained from the computational graph.</span></p><ul class="c64 lst-kix_5s83gilt2o7g-0 start"><li class="c2 c42 li-bullet-0"><span>Chainer: </span><span class="c13">Variable.{data,array}</span><span>&nbsp;or </span><span class="c13">Variable.unchain()</span><span class="c0">. </span></li><li class="c2 c42 li-bullet-0"><span>PyTorch: </span><span class="c11 c13"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/autograd.html%23torch.Tensor.detach&amp;sa=D&amp;source=editors&amp;ust=1615451627941000&amp;usg=AOvVaw05_2E3iFGsuaarmKWaGCob">Tensor.detach()</a></span><span>. This method returns a new </span><span class="c13">Tensor</span><span>&nbsp;unchained from the computational graph with </span><span class="c13">requires_grad</span><span>&nbsp;set to </span><span class="c13">False</span><span>. This is not an in-place operation in contrast to </span><span class="c13">Variable.unchain()</span><span>&nbsp;and does not incur any side effects (although the new </span><span class="c13">Tensor</span><span>&nbsp;will share the same memory). An in-place variant is however provided as well </span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/autograd.html%23torch.Tensor.detach_&amp;sa=D&amp;source=editors&amp;ust=1615451627941000&amp;usg=AOvVaw1kQeUEES7CGwXoWsGom-KU">Tensor.detach_()</a></span><span class="c0">.</span></li><li class="c2 c42 li-bullet-0"><span>PyTorch: </span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://github.com/pytorch/pytorch.github.io/pull/31&amp;sa=D&amp;source=editors&amp;ust=1615451627942000&amp;usg=AOvVaw0EIJeekd6OX04VXa23gx7j">Tensor.data is discouraged</a></span><span class="c0">&nbsp;and it seems like it might even get deprecated in the future (based on comments in forums and on GitHub).</span></li><li class="c2 c42 li-bullet-0"><span>Chainer&rsquo;s </span><span class="c13">Variable.unchain_backward()</span><span>&nbsp;counterpart in PyTorch is not available?</span></li></ul><h3 class="c46 c25" id="h.mc69ie7hbl6j"><span class="c8">Backprop modes</span></h3><p class="c2"><span class="c0">Explains differences of how backprop modes are switched.</span></p><ul class="c64 lst-kix_g3mw20emt8ab-0 start"><li class="c2 c42 li-bullet-0"><span>Chainer: </span><span class="c13">no_backprop_mode</span><span>, </span><span class="c13">force_backprop_mode </span><span class="c0">correspond to the following in PyTorch.</span></li><li class="c2 c42 li-bullet-0"><span>PyTorch: </span><span class="c11 c13"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/autograd.html%23torch.autograd.no_grad&amp;sa=D&amp;source=editors&amp;ust=1615451627942000&amp;usg=AOvVaw0XToPfmyLupuC-xtaWR8q8">torch.autograd.no_grad()</a></span><span>, </span><span class="c11 c13"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/autograd.html%23torch.autograd.enable_grad&amp;sa=D&amp;source=editors&amp;ust=1615451627943000&amp;usg=AOvVaw1ftKE2-6Dtw3BKuGiy1GiW">torch.autograd.enable_grad()</a></span><span class="c0">. </span></li><li class="c2 c42 li-bullet-0"><span class="c13">no_grad()</span><span>&nbsp;can also be used to allow writing data directly to an already allocated </span><span class="c13">Tensor</span><span class="c0">&nbsp;using [...].</span></li><li class="c2 c42 li-bullet-0"><span>In Chainer, this is also a configuration (</span><span class="c13">configuration.config.enable_backprop</span><span>), it is however not in PyTorch.</span></li><li class="c2 c42 li-bullet-0"><span class="c0">Both Chainer and PyTorch default to backprop mode being enabled.</span></li></ul><h3 class="c46 c25" id="h.bka3yqtw2rpy"><span class="c8">Train/Test modes</span></h3><p class="c2"><span class="c0">Explains differences of how train/test modes are switched. </span></p><ul class="c64 lst-kix_3sh9r2c1w16g-0 start"><li class="c2 c42 li-bullet-0"><span>Chainer: This mode is controlled via a configuration (</span><span class="c13">configuration.config.train</span><span class="c0">).</span></li><li class="c2 c42 li-bullet-0"><span>PyTorch: This mode is a module state and should be changed using </span><span class="c11 c13"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.html%23torch.nn.Module.train&amp;sa=D&amp;source=editors&amp;ust=1615451627944000&amp;usg=AOvVaw1LtL0CCtZGBEHSHWobAyOG">torch.nn.Module.train(mode)</a></span><span>, </span><span class="c11 c13"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.html%23torch.nn.Module.eval&amp;sa=D&amp;source=editors&amp;ust=1615451627944000&amp;usg=AOvVaw1zTZQi3CLT-abAXb4xfMZp">torch.nn.Module.eval()</a></span><span>. This also means that you must pass the &ldquo;train&rdquo; argument to functions calls such as </span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.functional.html%23torch.nn.functional.dropout&amp;sa=D&amp;source=editors&amp;ust=1615451627944000&amp;usg=AOvVaw3WbrBYZ7GIbYZ3YU-g-QHz">torch.nn.functional.dropout</a></span><span>&nbsp;otherwise use the </span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.html%23torch.nn.Dropout&amp;sa=D&amp;source=editors&amp;ust=1615451627945000&amp;usg=AOvVaw17jwofkuPJvc55crtYQATG">torch.nn.Dropout module</a></span><span>&nbsp;which is stateful.</span></li><li class="c2 c42 li-bullet-0"><span class="c0">In both Chainer and PyTorch, train/test mode affects the behavior of certain functions/links/modules such as dropout and batch normalization. </span></li><li class="c2 c42 li-bullet-0"><span>Both Chainer and PyTorch default to train mode.</span></li></ul><h1 class="c25 c90 c92" id="h.r8th8e47f02c"><span class="c61 c66 c36 c100">Ecosystem</span></h1><p class="c2"><span class="c0">This section introduces some of the larger repositories under the PyTorch GitHub organization. It also refers to the official list of other ecosystem-libraries acknowledged by PyTorch.</span></p><h3 class="c46 c25" id="h.d863gr6235hf"><span class="c8">PyTorch</span></h3><p class="c2"><span class="c36">Summary:</span><span>&nbsp;</span><span class="c54 c73 c106">Tensors and Dynamic neural networks in Python with strong GPU acceleration</span></p><p class="c2"><span class="c36">GitHub:</span><span>&nbsp;</span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://github.com/pytorch/pytorch&amp;sa=D&amp;source=editors&amp;ust=1615451627946000&amp;usg=AOvVaw1N84rS5abvJ9unX9uP1l0q">https://github.com/pytorch/pytorch</a></span></p><ul class="c64 lst-kix_47504ojh0ba1-0 start"><li class="c2 c42 li-bullet-0"><span class="c36">Installation:</span><span>&nbsp;See </span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/get-started/locally/&amp;sa=D&amp;source=editors&amp;ust=1615451627947000&amp;usg=AOvVaw1jylhhkvDPQTRV61oSaifX">Start Locally</a></span><span>&nbsp;for the instructions. Note that not all variants are hosted on PyPI; as of PyTorch 1.2.0, </span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://pypi.org/project/torch/&amp;sa=D&amp;source=editors&amp;ust=1615451627947000&amp;usg=AOvVaw2kcchqHTSKn_mL4iFvR0FG">torch</a></span><span>/</span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://pypi.org/project/torchvision/&amp;sa=D&amp;source=editors&amp;ust=1615451627948000&amp;usg=AOvVaw2I4ZICcAJqxe8B5UeYB_O_">torchvision</a></span><span class="c0">&nbsp;packages hosted on PyPI are for CUDA 10.0.</span></li><li class="c2 c42 li-bullet-0"><span class="c36">Nightly builds</span><span>: Nightly build is provided so you can try the pre-built binary with new merged feature next day. Each nightly build is tagged per each day so you can download nightly version locked pip wheel and prebuilt </span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/cppdocs/installing.html&amp;sa=D&amp;source=editors&amp;ust=1615451627948000&amp;usg=AOvVaw0FKXEEGlfCFhw_ey3G96O5">libtorch</a></span><span>&nbsp;of specific build date. For pip wheel refer </span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/get-started/locally/&amp;sa=D&amp;source=editors&amp;ust=1615451627949000&amp;usg=AOvVaw15w-tJ_hoQngIMQSALLnrp">Start Locally</a></span><span>&nbsp;(</span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html&amp;sa=D&amp;source=editors&amp;ust=1615451627949000&amp;usg=AOvVaw2ZGzlzLkxMf9EWGHlo5LR6">CPU page</a></span><span>, </span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://download.pytorch.org/whl/nightly/cu92/torch_nightly.html&amp;sa=D&amp;source=editors&amp;ust=1615451627949000&amp;usg=AOvVaw215n1Bdg3nJdAXlBIKxaqK">CUDA 9.2 page</a></span><span>, </span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://download.pytorch.org/whl/nightly/cu100/torch_nightly.html&amp;sa=D&amp;source=editors&amp;ust=1615451627950000&amp;usg=AOvVaw3EbatcskbEBurZcnV_30Kp">CUDA 10.0 page</a></span><span>) to get optimal version.</span></li></ul><h3 class="c46 c25" id="h.w69092dw2rtl"><span class="c8">Ignite</span></h3><p class="c2"><span class="c36">Summary:</span><span class="c0">&nbsp;High level utilities such as training loop abstraction.</span></p><p class="c2"><span class="c36">GitHub:</span><span>&nbsp;</span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://github.com/pytorch/ignite&amp;sa=D&amp;source=editors&amp;ust=1615451627951000&amp;usg=AOvVaw0ma7G-2NOlYTWursU8itAP">https://github.com/pytorch/ignite</a></span></p><h3 class="c46 c25" id="h.b2r58wgcvv09"><span class="c8">torchvision</span></h3><p class="c2"><span class="c36">Summary:</span><span class="c0">&nbsp;PyTorch for CV.</span></p><p class="c2"><span class="c36">GitHub: </span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://github.com/pytorch/vision&amp;sa=D&amp;source=editors&amp;ust=1615451627951000&amp;usg=AOvVaw09g9tcd7_kpyHQMkdSpLq3">https://github.com/pytorch/vision</a></span></p><p class="c2"><span>Recommended by the official installation guide to install along with </span><span class="c13">pytorch</span><span class="c0">.</span></p><p class="c2"><span class="c0">Provides domain-agnostic (not limited to CV) data augmentation functionality.</span></p><p class="c2"><span>Provides loaders for video data. Slow due to ffmpeg but this might be improved in the future?</span></p><h3 class="c46 c25" id="h.ctcyjos25n6z"><span class="c8">torchtext</span></h3><p class="c2"><span class="c36">Summary:</span><span class="c0">&nbsp;PyTorch for NLP.</span></p><p class="c2"><span class="c36">GitHub:</span><span>&nbsp;</span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://github.com/pytorch/text&amp;sa=D&amp;source=editors&amp;ust=1615451627953000&amp;usg=AOvVaw2bW5_02G0ZkeRjF7tivdYT">https://github.com/pytorch/text</a></span></p><h3 class="c46 c25" id="h.l9nxjlcurusl"><span class="c8">torchaudio </span></h3><p class="c2"><span class="c36">Summary:</span><span>&nbsp;PyTorch for audio data.</span></p><p class="c2"><span class="c36">GitHub:</span><span>&nbsp;</span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://github.com/pytorch/audio&amp;sa=D&amp;source=editors&amp;ust=1615451627954000&amp;usg=AOvVaw1p-5I-Snkb9NcickxkA2fb">https://github.com/pytorch/audio</a></span></p><h3 class="c46 c25" id="h.p90hj6w1ucn0"><span class="c8">Fairseq</span></h3><p class="c2"><span class="c36">Summary:</span><span class="c0">&nbsp;Seq2seq models.</span></p><p class="c2"><span class="c36">GitHub:</span><span>&nbsp;</span><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://github.com/pytorch/fairseq&amp;sa=D&amp;source=editors&amp;ust=1615451627955000&amp;usg=AOvVaw3AJGrN3I8J2sbb9s7lctSM">https://github.com/pytorch/fairseq</a></span></p><p class="c2"><span>Seq2seq models such as translation. Includes the Transformer and BERT-like models.</span></p><h3 class="c46 c25" id="h.eaofa6s7sf0k"><span class="c8">Other</span></h3><p class="c2"><span class="c0">There is an official list of libraries included in the PyTorch ecosystem (besides the domain specific libraries above), including e.g. Ignite.</span></p><p class="c2"><span class="c11"><a class="c9" href="https://www.google.com/url?q=https://pytorch.org/ecosystem&amp;sa=D&amp;source=editors&amp;ust=1615451627956000&amp;usg=AOvVaw23BcIDM47B9gOlOvaa-hxa">https://pytorch.org/ecosystem</a></span></p><div class="c49"><p class="c18"><a href="#cmnt_ref1" id="cmnt1">[a]</a><span class="c0">We can do it with `expand` https://pytorch.org/docs/stable/tensors.html#torch.Tensor.expand</span></p></div></body></html>